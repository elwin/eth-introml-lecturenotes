\chapter{Linear Regression}
\todo{Fix this chapter}


\section{Linear Regression}
The model assumption is that $y \approx f(x)$ where $f(x)$ is
linear (affine) in $x$,
i.e. $f(\vec{x}) = \vec{w}^T \vec{x} + w_0$.
We generally use a homogeneous representation:
\begin{equation*}
\vec{w}^T \vec{x} + w_0 = \Tilde{\vec{w}}^T \Tilde{\vec{x}}
\end{equation*}
where $\Tilde{\vec{w}} = [w_1, \dotsc, w_d, w_0]^T$ and
$\Tilde{\vec{x}} = [x_1, \dotsc, x_d, 1]^T$.
For equality, noise denoted by a random variable $\epsilon$
has to be accounted for, i.e.
$y = \Tilde{\vec{w}}^T \Tilde{\vec{x}} + \epsilon$.
For a complete dataset $\vec{X}$ with labels $\vec{y}$,
this results in
\begin{equation*}
\vec{y} = \vec{X} \vec{w} + \vec{\epsilon}
\end{equation*}

The data set is $D = \{(\vec{x}_1, y_1), \dotsc, (\vec{x}_n, y_n)\}$
with $\vec{x}_i \in \R^d$ and $y_i \in \R$.

A residual is $r_i = y_i - \vec{w}^T \vec{x}_i$ and the
vector of residuals is given as
$\vec{r} = \vec{y} - \vec{X}\vec{w}$.

\subsection{Least-Squares Linear Regression}
For least-squares linear regression, the empirical cost is
\begin{equation*}
\hat{R}(\vec{w}) = \norm{\vec{r}}_2^2
= \sum_{i = 1}^n{r_i^2}
= \sum_{i = 1}^n{(y_i - \vec{w}^T \vec{x}_i)^2}
\end{equation*}
and we try to find $\vec{w}^*$ with
\begin{equation*}
\vec{w}^* = \arg\min_{\vec{w}}{
	\sum_{i = 1}^n{(y_i - \vec{w}^T \vec{x}_i)^2}
}
\end{equation*}

This can be rewritten as
\begin{equation*}
\hat{R}(\vec{w})
= \norm{\vec{y} - \vec{X}\vec{w}}_2^2
= \vec{y}^T \vec{y} - 2 \vec{y}^T \vec{X} \vec{w} + \vec{w}^T \vec{X}^T \vec{X} \vec{w}
\end{equation*}

The problem can be solved in closed form as
\begin{equation*}
\vec{w}^* = (\vec{X}^T \vec{X})^{-1} \vec{X}^T \vec{y}
\end{equation*}
where $\vec{X}$ is a matrix whose rows are the samples and
$\vec{y}$ is a column vector containing the corresponding labels.

The objective function is convex, thus we can also use gradient
descent to find the global optimum.
With step size $\frac{1}{2}$,
gradient descent converges linearly.
For an $\epsilon$, we can find $x_t$ so that
$f(x_t) - \min_x(f(x)) \leq \epsilon$ in time
$\mathcal{O}(\log \frac{1}{\epsilon})$.

The gradient in the 1-dimensional case is given as
\begin{equation*}
\nabla \hat{R}(w)
= -2 \sum_{i = 1}^n{r_i \cdot x_i}
\end{equation*}
and in the $d$-dimensional case as
\begin{equation*}
\nabla \hat{R}(\vec{w})
= -2 \sum_{i = 1}^n{r_i \cdot \vec{x}_i^T}
= 2 \vec{w}^T \vec{X}^T \vec{X} - 2 \vec{y}^T \vec{X}
\end{equation*}.
Note that $\hat{R}(\vec{w}) \in \R^{1 \times d}$.

The closed-form solution takes $\mathcal{O}(n^3)$ time.
The gradient descent solution takes
$\mathcal{O}(n \cdot d \log \frac{1}{\epsilon})$ time.

Thus, gradient descent might converge faster.
Furthermore, an optimal solution might not be required,
and many problems do not admit a closed-form solution.

Instead of the squared error, other loss functions might be useful:
\begin{equation*}
\hat{R}(\vec{w}) = \sum_{i = 1}^n{l_p(y_i - \vec{w}^T \vec{x}_i)}
\end{equation*}
where $l_p = |r|^p$.

$l_p$ loss is convex for $p \geq 1$.
The $l_1$ loss puts less emphasis on large outliers.
For a large $p$, the results are restricted to a region.
For $p < 1$, the function becomes non-convex but is even more
robust towards outliers.

The family of $L_p$ norms is defined as
\begin{equation*}
\norm{\vec{w}}_p =
\sqrt[p]{\sum_{i=1}^d{|w_i|^p}}
\end{equation*}
and their partial derivative
\begin{equation*}
\frac{\partial \norm{\vec{w}}_p^p}{\partial w_j}
= p |w_j|^{p-1} \cdot sign(w_j)
\end{equation*}


\subsection{Feature Transformations}
We can fit non-linear functions via linear regression
by using non-linear basis functions of the data:
\begin{equation*}
f(\vec{x}) = \sum_{i=1}^D{w_i \phi_i(\vec{x})}
\end{equation*}
with $\phi : \R^d \to \R^D$.
$\phi$ is called a feature map.

The model is still linear,
but on the feature map,
not on the original features.

To fit polynomial functions of $d$-dimensions,
$\phi(\vec{x})$ is the vector of all monomials of degree
up to $k$ in variables $x_1, \dotsc, x_d$.
Monomials are polynomials with only one term and coefficient $1$.

For a periodic function
\begin{equation*}
y(t) = a_0 +
\sum_{n=1}^\infty{(a_n \cos{nt} + b_n \sin{nt})} + \epsilon
\end{equation*}
we can use a feature map
\begin{equation*}
\phi(t) = [1, \cos{t}, \sin{t}, \cos{2t}, \sin{2t}, \dotsc]
\end{equation*}


\subsection{Ridge Regression}
Least squares regression is not strongly convex,
prone to overfitting and may be ill-behaved if
some features are correlated.
The optimisation problem can be made strongly convex by adding
a regularisation term $\lambda > 0$:
\begin{equation*}
\min_{\vec{w}}{\frac{1}{n}
	\sum_{i=1}^n{(y_i - \vec{w}^T \vec{x}_i)^2
		+ \lambda \norm{\vec{w}}_2^2
}}
\end{equation*}
This results in the new empirical risk
\begin{equation*}
\hat{R}_{ridge}(\vec{w})
= \norm{\vec{y} - \vec{X}\vec{w}}_2^2
+ \lambda \norm{\vec{w}}_2^2
\end{equation*}

The scale of $\vec{x}$ now matters!
If the features are scaled, it changes the scale of $\lambda$.
Thus, the data has to be standardised.

The closed-form solution is
\begin{equation*}
\hat{\vec{w}} =
(\vec{X}^T \vec{X} + n \lambda \vec{I})^{-1} \vec{X}^T \vec{y}
\end{equation*}

The gradient is
TODO: Is this really correct? (Derivative/gradient)
\begin{equation*}
\nabla_{\vec{w}} \hat{R}_{ridge}(\vec{w})
= \nabla_{\vec{w}} \hat{R}(\vec{w})
+ \lambda \nabla_{\vec{w}} \norm{\vec{w}}_2^2
= 2 \vec{w}^T \vec{X}^T \vec{X} - 2 \vec{y}^T \vec{X} + 2 \vec{w}^T
\end{equation*}
resulting in an update step
\begin{equation*}
\vec{w}_{t+1} =
(1 - 2\lambda \eta_t)\vec{w}_t - \eta_t \nabla \hat{R}(\vec{w})
\end{equation*}

Ridge regression is also called $L_2$-regularisation
or weight decay (as it decays weights to zero).
If $\lambda \to \infty$ then $w \to 0$.


\subsection{Lasso Regression}
Lasso regression is $L_1$ regularised
least squares linear regression.

The objective then becomes
\begin{equation*}
\min_{\vec{w}}{\frac{1}{n}
	\sum_{i=1}^n{(y_i - \vec{w}^T \vec{x}_i)^2
		+ \lambda \norm{\vec{w}}_1
}}
\end{equation*}

The penalty, in theory, encourages coefficients to be
exactly zero.
This results in some form in automatic feature selection.

While in ridge regression, weights decay relatively smoothly,
in lasso regression, some weights may heavily increase
with increasing $\lambda$.
This makes intuitive sense because one weight may suddenly
"perform the work" that multiple weights did before.


\subsection{Kernelised Linear Regression}
In kernelised linear regression,
regularisation is already built-in.

Let $\vec{K}$ be the Gram matrix and $\vec{k}_i$ the
$i$-th column of the Gram matrix.

The kernelised objective for the squared loss is
\begin{equation*}
\left(\sum_{j=1}^n{\alpha_j k(\vec{x}_j, \vec{x}_i)} - y_i\right)^2
= (\vec{\alpha}^T \vec{k}_i - y_i)^2
\end{equation*}
and for the $L_2$ regulariser
\begin{equation*}
\sum_{i=1}^n{\sum_{j=1}^n{
		\alpha_i \alpha_j k(\vec{x}_i, \vec{x}_j)
}}
= \vec{\alpha}^T \vec{K} \vec{\alpha}
\end{equation*}

The kernelised optimisation objective is then
\begin{equation*}
\hat{\vec{\alpha}} = \arg\min_{\vec{\alpha}}{
	\frac{1}{n}
	\underbrace{
		\sum_{i=1}^n{
			(\vec{\alpha}^T \vec{k}_i - y_i)^2
		}
	}_{\norm{\vec{\alpha}^T \vec{K} - \vec{y}}_2^2}
	+ \lambda \vec{\alpha}^T \vec{K} \vec{\alpha}
}
\end{equation*}

The closed-form solution (with regulariser) is given as
\begin{equation*}
\hat{\vec{\alpha}} = (
\vec{K} + \lambda \vec{I}
)^{-1} \vec{y}
\end{equation*}

For a data point $\vec{x}$, the prediction is
\begin{equation*}
\hat{y} = \sum_{i=1}^n{\hat{\alpha}_i k(\vec{x}_i, \vec{x})}
\end{equation*}


\subsubsection{Loss Functions}
TODO: Functions from tutorial 3
TODO: Tutorial 3, 3.1


\section{Gradient Descent}
Let $\vec{w}_0 \in \R^d$ be an arbitrary
starting point,
$\eta_t$ the learning rate at time $t$ and
$\hat{R}(\vec{w})$ be a differentiable function
which should be minimised.
For $t = 1, 2, \dotsc$ gradient descent calculates
\begin{equation*}
\vec{w}_{t + 1} = \vec{w}_t - \eta_t
\nabla \hat{R}(\vec{w}_t)
\end{equation*}

Under mild assumptions, if the step size is sufficiently
small, gradient descent converges to a stationary
point.
Thus, for convex objectives, it finds the optimal solution (if one exists).

If the step size is chosen too small, convergence may
take very long.
If the step size is too large, the objective might
oscillate or even diverge.
The step size can also be chosen adaptively.

The line search heuristic works as follows:
Let $g_t = \nabla \hat{R}(\vec{w}_t)$.
Pick $\eta_t \in \arg\min_{\eta \in (0, \infty)}(
\hat{R}(\vec{w}_t - \eta \cdot g_t)
)$,
i.e. the step size which minimises the objective
w.r.t. the current gradient.

The bold driver heuristic works as follows:
\begin{itemize}
	\item If the function decreases, increase the step size:
	\begin{equation*}
	\hat{R}(\underbrace{\vec{w}_t - \eta_t g_t}_{\vec{w}_{t+1}}) < \hat{R}(\vec{w}_t)
	\Rightarrow \eta_{t + 1} = \eta_t \cdot c_{acc}
	\end{equation*}
	\item If the function increases, decrease the step size:
	\begin{equation*}
	\hat{R}(\vec{w}_t - \eta_t g_t) > \hat{R}(\vec{w}_t)
	\Rightarrow \eta_{t + 1} = \eta_t \cdot c_{dec}
	\end{equation*}
\end{itemize}

Stochastic gradient descent uses only a single
sample per iteration.
The sample is picked uniform at random
(with replacement) from the dataset,
used to compute the gradient and to update the
weights.

Stochastic gradient descent is guaranteed to
converge under mild assumptions if
$\sum_t{\eta_t} = \infty$ and
$\sum_t{\eta_t^2} < \infty$.

Using only a single sample at a time is computationally inefficient
and might have large variance in the gradient estimate.
It might thus lead to slow convergence.

Using mini-batch SGD reduces variance.
It uses mini-batches and averages the gradients with respect to
multiple randomly selected points.


\section{Standardisation}
Standardisation ensures that each feature has
zero mean and unit variance.

Let $\Tilde{x}_{i,j}$ be the $j$-th feature of
the $i$-th data point. Then
\begin{align*}
\Tilde{x}_{i,j} &= \frac{x_{i,j} - \hat{\mu}_j}{\hat{\sigma}_j} \\
\hat{\mu}_j &= \frac{1}{n} \sum_{i=1}^n{x_{i,j}} \\
\hat{\sigma}_j^2 &= \frac{1}{n} \sum_{i=1}^n{(x_{i,j} - \hat{\mu}_j)^2}
\end{align*}

