\chapter{Generative Modelling}
\todo{Fix this chapter}

\section{Generative Modelling}
\emph{Discriminative models} aim to estimate
the conditional distribution
\begin{equation*}
P(y \mid \vec{x})
\end{equation*}
\emph{Generative models} aim to estimate
the joint distribution
\begin{equation*}
P(y, \vec{x})
\end{equation*}

Because discriminative models do not attempt
to model $P(\vec{x})$,
they are unable to detect outliers
and can thus become overconfident.

A conditional distribution can always be
derived from a joint one, but not vice versa:
\begin{equation*}
P(y \mid \vec{x})
= \frac{P(\vec{x}, y)}{P(\vec{x})}
= \frac{P(\vec{x}, y)}{
	\int{P(\vec{x}, y) dy}
}
\end{equation*}

The typical approach for generative
modelling is as follows:
\begin{enumerate}
	\item Estimate prior on labels: $P(y)$
	\item Estimate conditional distribution
	$P(\vec{x} \mid y)$ for each class $y$
	\item Obtain posterior (predictive) distribution
	using Bayes' rule:
	\begin{equation*}
	P(y \mid \vec{x}) =
	\frac{1}{Z} 
	\underbrace{P(y) P(\vec{x} \mid y)}_{P(\vec{x}, y)}
	\end{equation*}
\end{enumerate}
where $Z = P(\vec{x})$ is a normalisation
constant,
i.e. $Z = \sum_y{P(y) P(\vec{x} \mid y)}$.
If only $P(y \mid \vec{x})$ is desired,
the normalisation constant can be omitted.

Thus, generative modelling attempts to
infer the process according to which
examples are generated.

\subsection{Comparison}
In generative modelling, we have a
probabilistic model of each class.
The decision boundary is where one
model becomes more likely.
It allows natural usage of unlabelled data.
Discriminative modelling focuses
on the decision boundary.
It is more powerful (in terms of prediction)
if a lot of examples are available.
It also cannot use unlabelled data.

Generative models are always better if
the model is well-specified.
Otherwise, generative modelling is
better in case of small amount of data,
discriminative modelling in
case of large amount of data.


\subsection{Conjugate Distributions}
A pair of prior distribution and likelihood function
is called \emph{conjugate} if
the posterior distribution remains in
the same family as the prior.

Conjugate priors thus allow for regularisation
with almost no additional computational cost.

\subsubsection{List of Conjugate Distributions}
TODO: List of conjugate priors, slides 22 p. 36 and tutorial.

\subsubsection{Dirichlet Prior, Multinomial Likelihood}
The \emph{Dirichlet} prior is conjugate with
respect to the \emph{Multinomial} likelihood.

Let $\vec{X} = (X_1, \dotsc, X_n)$ and
$X_i \sim Categorical(\vec{\theta})$ iid.
$\vec{\theta} = (\theta_1, \dotsc, \theta_m)$
is the vector of the probabilities for
individual values.
Furthermore, let $N_j$ denote the
the number of times $j$ occurs in $\vec{X}$
and $\vec{N} = (N_1, \dotsc, N_m)$.

Then, $\vec{N} \sim Multi(\vec{\theta}, n)$
is from a \emph{multinomial} distribution.

The probability mass function is
\begin{equation*}
P(\vec{N} \mid n, \vec{\theta}) =
\underbrace{\frac{n!}{\prod_{j=1}^m{N_j!}}}_\text{normalisation constant}
\prod_{j=1}^m{\theta_j^{N_j}}
\end{equation*}

For $Multi(\vec{\theta}, 1)$ is a categorical
distribution,
$Multi((\theta, 1 - \theta), n)$ equivalent to a binomial distribution.
The multinomial distribution is thus a
generalisation of the binomial distribution.

The \emph{Dirichlet distribution} has probability density function
\begin{equation*}
P(\vec{\theta} \mid \vec{\alpha}) =
\underbrace{\frac{\Gamma(\sum_{j=1}^m{\alpha_j})}{\prod_{j=1}^m{\Gamma(\alpha_j)}}}_\text{normalisation constant}
\prod_{j=1}^m{\theta_j^{\alpha_j - 1}}
\end{equation*}
with $\alpha_j > 0$.
We write $\vec{\theta} \sim Dir(\vec{\alpha})$.

If we have a Dirichlet prior $P(\vec{\theta})$
and a multinomial likelihood $P(\vec{X} \mid \vec{\theta})$,
the resulting posterior $P(\vec{\theta} \mid \vec{X}) \propto P(\vec{X} \mid \vec{\theta}) P(\vec{\theta})$
is
\begin{equation*}
P(\vec{\theta} \mid \vec{X}) = Dir(\vec{N} + \vec{\alpha})
\end{equation*}
Thus, $\vec{\alpha}$ act as pseudo-counts,
similar to the beta prior and binomial likelihood.

The Dirichlet prior is also conjugate with respect
to the Categorical likelihood.

The MLE of the multinomial likelihood is
\begin{equation*}
\vec{\theta}_j^* = \frac{N_j}{n}
\end{equation*}
and the MAP estimate with a Dirichlet prior is
\begin{equation*}
\vec{\theta}_j^* = \frac{N_j + \alpha_j - 1}{n + \sum_{j' = 1}^m{(\alpha_{j'} - 1)}}
\end{equation*}


\section{Naive Bayes Model}
Naive Bayes is a generative model.

The class labels
$y \in \mathcal{Y} = \{1, \dotsc, c\}$
are modelled as being generated
from a categorical variable:
\begin{equation*}
P(Y = y) = p_y, p_y \geq 0, \sum_{y=1}^c{p_y} = 1
\end{equation*}

The features are modelled conditionally
independent given Y:
\begin{equation*}
P(X_1, \dotsc, X_d \mid Y)
= \prod_{i=1}^d{P(X_i \mid Y)}
\end{equation*}
i.e. given a class label,
each feature is generated independently
of all other features.

This is a very strong modelling assumption
which in practice often does not hold
(but still works well).

The feature distributions $P(X_i \mid Y)$
still need to be defined.
They lead to different Naive Bayes classifiers.


\subsection{Decision Rules}
Let $\hat{P}(y)$ and
$\hat{P}(\vec{x} \mid y)$
be the estimated distributions.

To predict the label $y$ for a new
example $\vec{x}$,
Bayes' theorem can be used
\begin{equation*}
P(y \mid \vec{x}) =
\frac{1}{Z} P(y) P(\vec{x} \mid y)
\end{equation*}
with
\begin{equation*}
Z = \sum_{y=1}^c{
	P(y) P(\vec{x} \mid y)
}
\end{equation*}

The label $\hat{y}$ can then be
predicted using Bayesian decision theory:
\begin{equation*}
\hat{y} =
\arg\max_y{\hat{P}(y \mid \vec{x})} =
\arg\max_y{\hat{P}(y) \prod_{i=1}^d{\hat{P}(x_i \mid y)}}
\end{equation*}


\subsection{Gaussian Naive Bayes Classifier}
In a GNB, the features are modelled
as \emph{conditionally independent Gaussians}:
\begin{equation*}
P(x_i \mid y)
= \mathcal{N}(x_i \mid \mu_{y,i}, \sigma^2_{y, i})
\end{equation*}
Note that $\mu$ and $\sigma^2$
depend on the class and feature,
i.e. there are $c \cdot d$
versions of each parameter.

\subsubsection{MLE for Two Classes}
Assume there are two classes,
i.e. $\mathcal{Y} = \{+1, -1\}$.
Let $D$ be the data set.

We have $P(Y = +1) = p$ and
$P(Y = -1) = (1 - p)$.
The MLE for $p$ is
\begin{equation*}
\arg\max_p{P(D \mid p)}
= \frac{n_+}{n_+ + n_-}
\end{equation*}
where $n_+$ is the number of positive
samples, $n_-$ the number of negative
samples.

The conditional distribution for the
data is $P(x_i \mid y) = \mathcal{N}(x_i; \mu_{y, i}, \sigma^2_{y, i})$.
Then we have to do the usual MLE for
Gaussian distributions.
TODO: Result.

\subsubsection{Summary}
Let
$D = \{(\vec{x}_1, y_1), \dotsc, (\vec{x}_n, y_n)\}$
be the data set
with $y_i \in \{1, \dotsc, c\}$.

The MLE for the class prior is
\begin{equation*}
\hat{p}_y = \hat{P}(Y = y)
= \frac{Count(Y = y)}{n}
\end{equation*}

The MLE for the feature distribution
$\hat{P}(x_i \mid y) = \mathcal{N}(x_i; \hat{\mu}_{y, i}, \sigma^2_{y,i})$ is
\begin{align*}
\hat{\mu}_{y,i} &=
\frac{1}{Count(Y = y)}
\sum_{j:y_j = y}{x_{j,i}} \\
\hat{\sigma}^2_{y, i} &=
\frac{1}{Count(Y = y)}
\sum_{j:y_j = y}{(
	x_{j,i} - \hat{\mu}_{y,i}
	)^2}
\end{align*}
i.e. the empirical mean and variance
for all examples with class $y$.

Prediction given $\vec{x}$ is
\begin{equation*}
\hat{y} = \arg\max_y{
	\hat{P}(y \mid \vec{x})
}
= \arg\max_y{
	\hat{P}(y)
	\prod_{i=1}^d{
		\hat{P}(x_i \mid y)
	}
}
\end{equation*}

\subsubsection{Binary Classification}
In the binary case ($c = 2$),
the above prediction is equivalent to
\begin{equation*}
\hat{y} = sign\left( \log{
	\frac{
		\overbrace{P(Y = +1 \mid \vec{x})}^p
	}
	{P(Y = -1 \mid \vec{x})}
}\right)
\end{equation*}

The \emph{discriminant function} is defined as
\begin{equation*}
f(\vec{x}) = \log{\frac
	{P(Y = +1 \mid \vec{x})}
	{P(Y = -1 \mid \vec{x})}
}
\end{equation*}

Now two further assumptions are introduced.
Assumption 1 is that
$P(Y = +1) = P(Y = -1) = 0.5$,
i.e. both classes have equal probability.
Assumption 2 is that
$\forall y \in \mathcal{Y} : \sigma^2_{y,i} = \sigma^2_i$,
i.e. the variance and the class are independent.

TODO: Derivation?
Then,
\begin{equation*}
f(\vec{x}) = \vec{w}^T \vec{x} + w_0
\end{equation*}
i.e. GNB produces a linear classifier with
\begin{align*}
w_0 &= \log{\frac{\hat{p}_+}{1-\hat{p}_+}}
+ \sum_{i=1}^d{
	\frac{\hat{\mu}^2_{-,i} - \hat{\mu}^2_{+, i}}{2\hat{\sigma}^2_i}
} \\
w_i &= \frac
{\hat{\mu}_{+,i} - \hat{\mu}_{-,i}}
{\hat{\sigma}^2_i}
\end{align*}

If the GNB and other assumptions hold
(independent features, conditioned on
class labels, coming from Gaussian, assumptions 1 and 2)
then the resulting decision boundary
is equivalent to that from logistic regression:
Let $p(x) := P(Y = +1 \mid \vec{x})$.
\begin{align*}
&\Rightarrow f(\vec{x}) = \log{
	\frac{p(\vec{x})}{1 - p(\vec{x})}
} \\
&\Rightarrow \exp{(f(\vec{x}))}
= \frac{p(\vec{x})}{1 - p(\vec{x})} \\
&\Rightarrow p(\vec{x}) =
\frac{\exp{(f(\vec{x}))}}
{1 + \exp{(f(\vec{x}))}}
= \frac{1}{1 + \exp{(-f(\vec{x}))}}
= \sigma(f(\vec{x}))
\end{align*}


\subsection{Poisson Naive Bayes Classifier}
Let $\vec{x}_i \in \mathbb{N}^d, y \in \{0, 1\}$ be the
i.i.d. observations.
The Poisson Naive Bayes Classifier assumes
$Y \sim Bernoulli(p)$ and
$P(\vec{x}_i \mid y_i) = \prod_{j=1}^d{P(x_{i,j} \mid y_i)}$
with $P(x_{i,j} \mid y_i) = Poiss(x_{i, j}; \lambda_{y_i, j})$.

Let $n_0 := |\{i : y_i = 0\}|$ and
$n_1 := |\{i : y_i = 1\}|$.

The MLE for the labels is given as
$p_0 = \frac{n_0}{n}$
and $p_1 = \frac{n_1}{n} = 1 - p_0$.

The MLE for the conditional distribution
with $y \in \{0,1\}$ is given as
\begin{equation*}
\lambda_{y,j} = \frac{
	\sum_{i: y_i = y}^n{x_{i,j}}
}{n_y}
\end{equation*}
i.e. the mean of all samples of class $y$.

It can be shown that the Poisson Naive Bayes Classifier
produces a linear decision boundary.


\subsection{Categorical Naive Bayes}
Features may of course also take discrete values.
Thus, it might not make sense to model
them as Gaussians but e.g. as
Bernoulli, Categorical or Multinomial.

In the categorical case,
the class labels are still assumed to
be generated from a categorical variable, i.e.
\begin{equation*}
P(Y = y) = p_y
\end{equation*}
with $y \in \mathcal{Y} = \{1, \dotsc, c\}$.

The features are modelled as (conditionally)
independent categorical random variables:
\begin{equation*}
P(X_i = c \mid Y = y) = \theta^{(i)}_{c \mid y}
\end{equation*}
where $\theta^{(i)}_{c \mid y}$ is the
probability that the $i$-th feature
takes value $c$ for class $y$.

\subsubsection{Estimation}
The MLE for the class distribution,
$\hat{P}(Y = y) = \hat{p}_y$,
is again given as
\begin{equation*}
\hat{p}_y = \frac{Count(Y = y)}{n}
\end{equation*}

The MLE for the distribution of feature $i$,
$\hat{P}(X_i = c \mid Y = y) = \theta^{(i)}_{c \mid y}$,
is given as
\begin{equation*}
\theta^{(i)}_{c \mid y} =
\frac{Count(X_i = c, Y = y)}{Count(Y = y)}
\end{equation*}

\subsubsection{Prediction}
Prediction for a new point $\vec{x}$ is
done as usual as
\begin{equation*}
\hat{y} = \arg\max_y{\hat{P}(y \mid \vec{x})}
= \arg\max_y{
	\hat{P}(y) \prod_{i=1}^d{\hat{P}(x_i \mid y)}
}
\end{equation*}

\subsubsection{Dropping the Naive Assumption}
Removing the naive assumption for
categorical features is much harder.
In general,
it requires exponentially many parameters (in $d$).
We have to specify
$P(X_1 = x_1, \dotsc, X_d = x_d | Y = y)$ for each $x_1, \dotsc, x_d$,
exponentially many numbers.
This is computationally infeasible and very easy to overfit.

However, there are models which replace the naive
assumption with some weaker ones.


\subsection{Mixing Distributions}
Due to the naive assumption,
each feature is modelled to be from a
different conditional distribution.

Those distributions do not have to be
of the same type,
and even discrete and continuous distributions
can be mixed.
Fitting (MLE) and prediction stays the same.


\subsection{Regularisation}
MLE is prone to overfitting as it maximises the likelihood
for exactly the given realisation.
MLE has nice asymptotic properties
but for finite data, it is prone to overfitting.
There are two ways to combat overfitting in
generative modelling.

One way is to restrict the model class,
e.g. assumptions on the covariance structure.
This results in fewer parameters.

\subsubsection{Parameter Priors}
In generative modelling,
it is very natural to think about priors
for regularisation.
This poses a second form of regularisation.

Priors are always applied to the distribution of $P(Y)$.
We want to put a prior distribution $P(\theta)$
and then compute the posterior distribution
$P(\theta \mid y_1, \dotsc, y_n)$.

TODO: Not 100 percent sure this makes sense.

\subsubsection{Example: Beta Prior}
Assume we have $c = 2$ and thus
$P(Y = +1) = \theta$.

The \emph{Beta-Distribution} is defined as
\begin{equation*}
Beta(\theta; \alpha_+, \alpha_-) =
\frac{1}{B(\alpha_+, \alpha_-)}
\theta^{\alpha_+ - 1}
(1 - \theta)^{\alpha_- - 1}
\end{equation*}

$B(\alpha_+, \alpha_-)$ is
a normalisation constant to
ensure the integral becomes 1.
If $\alpha_+$ is smaller than $\alpha_-$,
the distribution is more peaked and the
peak is more to the right.
If both are less than 1,
the resulting distribution is bimodal.
If $\alpha_+$ and $\alpha_-$ go to infinity,
the distribution is going to converge to a point mass
on $\frac{\alpha_+}{\alpha_-}$.

The interesting property of the Beta-distribution
is that its posterior
is also a Beta-distribution,
i.e. it is \emph{conjugate} with the Binomial likelihood:
\begin{equation*}
P(\theta \mid y_1, \dotsc, y_n; \alpha_+, \alpha_-)
= P(\theta \mid n_+, n_-, \alpha_+, \alpha_-)
= Beta(\theta; \alpha_+ + n_+, \alpha_- + n_-)
\end{equation*}

The MAP estimate is then given as
\begin{equation*}
\hat{\theta} = \arg\max_{\theta}{P(\theta \mid y_1, \dotsc, y_n; \alpha_+, \alpha_-)}
= \frac{\alpha_+ + n_+ - 1}{\alpha_+ + n_+ + \alpha_- + n_- - 2}
\end{equation*}
Thus the $\alpha_+, \alpha_-$ act as pseudo-counts.


\subsection{Issues}
GNB models assume that features are generated
independently given a class label.
However, if there is (conditional)
correlation between features, given class labels,
then this assumption is violated!

For example,
assume $P(Y = +1) = P(Y = -1) = 0.5$ and
further features
$x_2, \dotsc, x_d = x_1$,
i.e. are duplicates.
A GNB classifier which only uses $x_1$
has
\begin{equation*}
f_1(x) = \log{
	\frac{P(Y = +1 \mid x_1 = x)}
	{P(Y = -1 \mid x_1 = x)}
}
\end{equation*}
where a classifier which uses all $d$
features has
\begin{equation*}
f_2(\vec{x}) = \log{
	\frac
	{\prod_{i=1}^d{P(X_i = x_i \mid Y = +1)}}
	{\prod_{i=1}^d{P(X_i = x_i \mid Y = -1)}}
}
= d \cdot f_1(x_1)
\end{equation*}
Thus the prediction gets blown-up,
i.e. become overconfident
(very close to 0 or 1),
even though it should theoretically not change.

This is fine if only the most likely class
matters, but bad if probabilities should
be used for decision making.
Logistic regression works better in that case
because we do not look at $P(\vec{x})$ and thus
do not have to explain correlation.
Generally, logistic regression is more
robust and better in terms of classification
performance, but cannot provide information
about $P(\vec{x})$.

A way to overcome this is to
drop the naive assumption,
i.e. use full Gaussian Bayes Classifiers.


\section{Gaussian Bayes Classifiers}
Gaussian Bayes Classifiers (GBC) are
generalisations of GNB classifiers.

The class labels are modelled as generated
from a categorical variable:
\begin{equation*}
P(Y = y) = p_y
\end{equation*}
with $y \in \mathcal{Y} = \{1, \dotsc, c\}$.

The features $\vec{x}$ are modelled as
generated by multivariate Gaussians:
\begin{equation*}
P(\vec{x} \mid y)
= \mathcal{N}(\vec{x}; \vec{\mu}_y, \vec{\Sigma}_y)
\end{equation*}
with $\vec{\mu}_y$ being the mean and
$\vec{\Sigma}_y$ the covariance matrix.


\subsection{Estimation}
Let $D = \{(\vec{x}_1, y_1), \dotsc, (\vec{x}_n, y_n)\}$ be the data set.

The MLE for the class label distribution
$\hat{P}(Y = y) = \hat{p}_y$ is given as
\begin{equation*}
\hat{p}_y = \frac{Count(Y = y)}{n}
\end{equation*}

The MLE for the feature distribution
$\hat{P}(\vec{x} \mid y) = \mathcal{N}(\vec{x}; \vec{\mu}_y, \vec{\Sigma}_y)$
is given as
\begin{align*}
\hat{\vec{\mu}}_y
&= \frac{1}{Count(Y = y)}
\sum_{i : y_i = y}{\vec{x}_i} \\
\hat{\vec{\Sigma}}_y
&= \frac{1}{Count(Y = y)}
\sum_{i : y_i = y}{
	(\vec{x}_i - \hat{\vec{\mu}}_y)
	(\vec{x}_i - \hat{\vec{\mu}}_y)^T
}
\end{align*}
i.e. the empirical (class) mean and the
empirical (class) covariance.

Note that now, for the (co)variance,
$d^2$ instead of $d$ parameters need
to be estimated.


\subsection{Discriminant Function}
Given $P(Y = +1) = p$ and
$P(\vec{x} \mid y) = \mathcal{N}(\vec{x}; \vec{\mu}_y, \vec{\Sigma}_y)$.

We want
\begin{equation*}
f(\vec{x}) = \frac{
	P(Y = +1 \mid \vec{x})
}{
	P(Y = -1 \mid \vec{x})
}
\end{equation*}

For GBC, this is given by
\begin{equation*}
f(\vec{x}) =
\log{\frac{p}{1-p}} +
\frac{1}{2} \left[
\log{\frac{|\hat{\vec{\Sigma}}_-|}{|\hat{\vec{\Sigma}}_+|}}
+
\left(
(\vec{x}-\hat{\vec{\mu}}_-)^T
\hat{\vec{\Sigma}}_-^{-1}
(\vec{x}-\hat{\vec{\mu}}_-)
\right)
-
\left(
(\vec{x}-\hat{\vec{\mu}}_+)^T
\hat{\vec{\Sigma}}_+^{-1}
(\vec{x}-\hat{\vec{\mu}}_+)
\right)
\right]
\end{equation*}
with $|\cdot|$ denoting the (matrix)
discriminant.


\subsection{Variations}
If $\vec{\Sigma}$ is a diagonal matrix,
this corresponds to a
\emph{Gaussian Naive Bayes} classifier.
The diagonal elements are the
$\sigma^2_{y, i}$.

For $c = 2$, $P(Y = +1) = P(Y = -1) = 0.5$,
$\vec{\Sigma}$ a diagonal matrix (naive),
and equal covariances (not depending on class)
$\hat{\vec{\Sigma}}_- = \hat{\vec{\Sigma}}_+ = \hat{\vec{\Sigma}}$,
this is of the same form as \emph{Logistic Regression}.
If the modelling assumptions are met,
the predictions are the same.

For $c = 2$, $P(Y = +1) = P(Y = -1) = 0.5$
and equal covariances (not depending on class)
$\hat{\vec{\Sigma}}_- = \hat{\vec{\Sigma}}_+ = \hat{\vec{\Sigma}}$,
this corresponds to
\emph{Fisher's Linear Discriminant Analysis}.

TODO: More here?

TODO: Big Picture, slides 22, p. 22

\subsubsection{Comparison to GNB Classifiers}
The conditional independence assumption of
GNB models can lead to overconfidence
where general GBC capture correlations
among features and thus avoids overconfidence.
However, GNB predictions can still be useful.

Also, GNB models are cheaper to fit:
Fitting a naive model has parameters
in $\mathcal{O}(c \cdot d)$
where a full GBC has parameters
in $\mathcal{O}(c \cdot d^2)$.


\section{Fisher's Linear Discriminant Analysis}
Suppose we use a GBC.
Further assume
\begin{itemize}
	\item only two classes, $c = 2$.
	\item equal class probabilities,
	$p = P(Y = +1) = P(Y = -1) = 0.5$.
	\item equal class covariances,
	$\hat{\vec{\Sigma}}_- = \hat{\vec{\Sigma}}_+ = \hat{\vec{\Sigma}}$.
\end{itemize}

Then, the discriminant function greatly
simplifies to
\begin{equation*}
f(\vec{x}) = 
\vec{x}^T \hat{\vec{\Sigma}}^{-1}
(\hat{\vec{\mu}}_+ - \hat{\vec{\mu}}_-)
+ \frac{1}{2}
\left(
\hat{\vec{\mu}}_-^T \hat{\vec{\Sigma}}^{-1} \hat{\vec{\mu}}_-
-
\hat{\vec{\mu}}_+^T \hat{\vec{\Sigma}}^{-1} \hat{\vec{\mu}}_+
\right)
\end{equation*}
TODO: Derivation?

Under these assumptions, we then predict
\begin{equation*}
\hat{y} = sign(f(\vec{x}))
= sign(\vec{w}^T \vec{x} + w_0)
\end{equation*}
with
\begin{align*}
\vec{w} &= \hat{\vec{\Sigma}}^{-1}
(\hat{\vec{\mu}}_+ - \hat{\vec{\mu}}_-) \\
w_0 &= \frac{1}{2}
\left(
\hat{\vec{\mu}}_-^T \hat{\vec{\Sigma}}^{-1} \hat{\vec{\mu}}_-
-
\hat{\vec{\mu}}_+^T \hat{\vec{\Sigma}}^{-1} \hat{\vec{\mu}}_+
\right)
\end{align*}
Thus the resulting classifier is linear.

This linear classifier is called
\emph{Fisher's Linear Discriminant Analysis}
(LDA).


\subsection{Comparison}
\subsubsection{Comparison to Logistic Regression}
Fisher's LDA uses the discriminant function
\begin{equation*}
f(\vec{x}) = \log{
	\frac{P(Y = +1) \mid \vec{x})}{P(Y = -1 \mid \vec{x})}
}
\end{equation*}
which is derived
$f(\vec{x}) = \vec{w}^T \vec{x} + w_0$.
It can be rearranged to
\begin{equation*}
P(Y = +1 \mid \vec{x}) = \frac{1}{1 + \exp{(-f(\vec{x})}}
\end{equation*}
and by using the derived form
\begin{equation*}
P(Y = +1 \mid \vec{x}) = \sigma(\vec{w}^T \vec{x} + w_0)
\end{equation*}

This is the same form
as logistic regression,
assuming homogeneous coordinates
are used.

If the model assumptions are met,
LDA will make the same prediction
as logistic regression.

Fisher's LDA is a generative model,
i.e. models $P(\vec{x}, y)$.
Logistic regression is a discriminative
model, i.e. models $P(y \mid \vec{x})$.

LDA can thus be used to detect
outliers with $P(\vec{x}) < \tau$.
It also assumes normality of $\vec{X}$,
which is a very strong assumption.
Logistic regression makes no assumptions
on $\vec{X}$ but can also not be
used to detect outliers.
Generally, LDA is not very robust
against violations of its assumptions
where logistic regression,
making less assumptions, is more robust.


\subsubsection{Comparison to PCA}
LDA can be viewed as a projection into
a 1-dimensional subspace
which maximises the ratio of
between-class and within-class variance.

In contrast, PCA maximises the
variance of the resulting
1-dimensional projection;
it does not know about classes.


\subsection{Quadratic Discriminant Analysis}
There is no reason to assume that the
variances are equal for different classes.

In the resulting, more general case,
the discriminant is
\begin{equation*}
f(\vec{x}) =
\log{\frac{p}{1-p}} +
\frac{1}{2} \left[
\log{\frac{|\hat{\vec{\Sigma}}_-|}{|\hat{\vec{\Sigma}}_+|}}
+
\left(
(\vec{x}-\hat{\vec{\mu}}_-)^T
\hat{\vec{\Sigma}}_-^{-1}
(\vec{x}-\hat{\vec{\mu}}_-)
\right)
-
\left(
(\vec{x}-\hat{\vec{\mu}}_+)^T
\hat{\vec{\Sigma}}_+^{-1}
(\vec{x}-\hat{\vec{\mu}}_+)
\right)
\right]
\end{equation*}
with $|\cdot|$ denoting the (matrix)
discriminant.
Note that this is precisely the discriminant
of a GBC.
It has a quadratic form.

The prediction is done as
\begin{equation*}
\hat{y} = sign(f(\vec{x}))
\end{equation*}


\section{Gaussian Mixture Models}
\emph{Gaussian Mixture Models} (GMMs) are generative models.
Because generative modelling describes tries to describe all
the data, it can naturally be used in the case of missing data.
Unsupervised learning is an extreme case of missing data
where all the labels are absent.

In GMMs, we assume the data was generated by a mixture of Gaussians.
A \emph{Gaussian mixture} is a convex combination of
$k$ Gaussian distributions:
\begin{equation*}
P(\vec{x} \mid \theta)
= P(\vec{x} \mid \vec{\mu}, \vec{\Sigma}, \vec{w})
= \sum_{i=1}^k{
	w_i \cdot \mathcal{N}(\vec{x} ; \vec{\mu}_i, \vec{\Sigma}_i)
}
\end{equation*}
with $w_i \geq 0$ and $\sum_{i=1}^k{w_i} = 1$,
i.e. defining a categorical distribution.
The $w_i$ are called the \emph{mixture weights}.

Mixture models in general are convex combinations
of $k$ simple (base) distributions.
Mixture models are more expressive than
the base distributions as they allow
for multimodal data representation.
GMMs are thus mixture models where all
base distributions are Gaussians.

The negative log likelihood,
assuming the samples $\vec{x}_1, \dotsc, \vec{x}_n$
are i.i.d., is
\begin{equation*}
(\vec{\mu}^*, \vec{\Sigma}^*, \vec{w}^*)
= \arg\min{
	-\sum_{i=1}^n{\log{
			\sum_{j=1}^k{
				w_j \mathcal{N}(\vec{x}_i \mid \vec{\mu}_j, \vec{\Sigma}_j)
			}
	}}
}
\end{equation*}
This is a non-convex objective.
Because it contains the logarithm of a sum,
no closed-form solution can be derived.
Furthermore, SGD is challenging to apply because
the covariance matrices must remain symmetric positive definite.

Let $z$ be a cluster index and $\vec{x}$ a feature.
The joint distribution
$P(z, \vec{x}) = w_z \mathcal{N}(\vec{x} \mid \vec{\mu}_u, \vec{\Sigma}_z)$
is identical to the generative model used by a GBC.
However, in contrast to a GBC, the cluster index
$z$ is a latent variable, i.e. unobserved.
Thus, fitting a GMM is equivalent to training
a GBC without labels and
a natural approach would be to try to infer the labels.


\subsection{Hard-EM}
Let $\theta^{(t)} = (\vec{w}^{(t)}, \vec{\mu}^{(t)}, \vec{\Sigma}^{(t)})$
denote the parameters at step $t$.

First, (carefully) initialise the parameters
$\theta^{(0)}$.

Then, for $t = 1, 2, \dotsc$
\begin{description}
	\item[E-step]: Predict the most likely class for each data point.
	\begin{equation*}
	z_i^{(t)} = \arg\max_z{
		P(z \mid \vec{x}_i, \theta^{(t-1)})
	}
	= \arg\max_z{
		\underbrace{P(z \mid \theta^{(t-1)})}_{w_z^{(t-1)}}
		\cdot
		\underbrace{P(\vec{x}_i \mid z, \theta^{(t-1)})}_{\mathcal{N}(\vec{x}_i; \vec{\mu}_i^{(t-1)}, \vec{\Sigma}_i^{(t-1)})}
	}
	\end{equation*}
	This gives the "completed" data set $D^{(t)}$.
	\item[M-step]: Compute MLE for the GBC.
	\begin{equation*}
	\theta^{(t)} = \arg\max_\theta{P(D^{(t)} \mid \theta)}
	\end{equation*}
\end{description}

The intuitive interpretation is as follows:
In the E-step, the (missing) class labels are inferred
using the GBC of the previous round.
This gives a completed data set for each time step.
Then, in the M-step, MLE for a GBC is performed,
using the previously inferred class labels.

Hard-EM is conceptually very similar to the k-means algorithm.

\subsubsection{Issues}
The Hard-EM algorithm has various issues.
In particular, each point is forced to
declare allegiance to some cluster,
even though the model is uncertain.
Intuitively, this tries to extract too much
information from a single point.

A GMM with $k$ mixture components can always
represent mixtures with $< k$ components.
By setting some mixture weights to $0$,
or putting multiple distributions directly
on top of each other,
all but one mixture components can be eliminated.

However, suppose a data set is generated by a single
Gaussian distribution.
If, for example, $k=2$, the optimal solution for
the Hard-EM algorithm are two mixtures which each
contain roughly half of the points.

Thus, Hard-EM works poorly in practice if clusters
are overlapping.


\subsection{Soft-EM}
To fix the issues of Hard-EM,
we can allow a data point to be assigned to multiple clusters
at the same time.

Suppose we have models $P(z \mid \theta)$ and 
$P(\vec{x} \mid \theta)$.
Then, for each data point, the posterior distribution over
cluster membership can be calculated,
i.e. inferring distributions over the latent variables $z$.

$\gamma_j(\vec{x})$ is the \emph{responsibility},
denoting the probability that $\vec{x}$ belongs to component $j$,
i.e. $\gamma_j(\vec{x}) = P(Z = j \mid \vec{x}, \theta)$.
This can be calculated as
\begin{equation*}
\gamma_j(\vec{x}) = \frac{
	\overbrace{w_j}^{P(Z = j \mid \theta)}
	\overbrace{P(\vec{x} \mid \vec{\Sigma}_j, \vec{\mu}_j)}^{P(\vec{x} \mid Z = j, \theta)}
}{
	\underbrace{
		\sum_{l=1}^k{
			w_l P(\vec{x} \mid \vec{\Sigma}_l, \vec{\mu}_l)
		}
	}_{P(x \mid \theta)}
}
\end{equation*}

At the MLE $(\vec{\mu}^*, \vec{\Sigma}^*, \vec{w}^*)$,
it must hold that
\begin{align*}
\vec{\mu}^*_j &= \frac{\sum_{i=1}^n{\gamma_j(\vec{x}_i) \vec{x}_i}}{\sum_{i=1}^n{\gamma_j(\vec{x}_i)}} \\
\vec{\Sigma}^*_j &= \frac{
	\sum_{i=1}^n{
		\gamma_j(\vec{x}_i) (\vec{x}_i - \vec{\mu}_j^*) (\vec{x}_i - \vec{\mu}_j^*)^T
	}
}{
	\sum_{i=1}^n{\gamma_j(\vec{x}_i)}
}
\\
w_j^* &= \frac{1}{n} \sum_{i=1}^n{\gamma_j(\vec{x}_i)}
\end{align*}
All parameters are simply weighted averages,
according to the responsibilities.
The issue is that the equations are coupled,
i.e. the $\gamma_j$ depend on
all the model parameters and vice versa.

The \emph{Soft-EM} still allows to optimise them.
For $t = 1, 2, \dotsc$
\begin{description}
	\item[E-step]: Calculate cluster membership weights
	(aka responsibilities)
	for each point $\vec{x}_i$.
	\begin{equation*}
	\gamma_j^{(t)}(\vec{x}_i) = \frac{
		w_j^{(t-1)} P(\vec{x}_i \mid \vec{\Sigma}_j^{(t-1)}, \vec{\mu}_j^{(t-1)})
	}{
		\sum_{l=1}^k{
			w_l^{(t-1)} P(\vec{x}_i \mid \vec{\Sigma}_l^{(t-1)}, \vec{\mu}_l^{(t-1)})
		}
	}
	\end{equation*}
	\item[M-step]: Fit clusters to weighted data points,
	i.e. calculate the closed form ML solution.
	\begin{align*}
	\vec{\mu}^{(t)}_j
	&= \frac{\sum_{i=1}^n{\gamma_j^{(t)}(\vec{x}_i) \vec{x}_i}}{\sum_{i=1}^n{\gamma_j^{(t)}(\vec{x}_i)}} \\
	\vec{\Sigma}^{(t)}_j &= \frac{
		\sum_{i=1}^n{
			\gamma_j^{(t)}(\vec{x}_i) (\vec{x}_i - \vec{\mu}_j^{(t)}) (\vec{x}_i - \vec{\mu}_j^{(t)})^T
		}
	}{
		\sum_{i=1}^n{\gamma_j^{(t)}(\vec{x}_i)}
	}
	\\
	w_j^{(t)} &= \frac{1}{n} \sum_{i=1}^n{\gamma_j^{(t)}(\vec{x}_i)}
	\end{align*}
\end{description}
The \emph{E} stands for \emph{Expected sufficient statistics},
the \emph{M} stands for \emph{Maximum likelihood solution}.


\subsection{Hard- vs Soft-EM}
Soft-EM generally results in higher likelihood values
as it is able to deal with overlapping clusters.
However, Hard-EM might be useful in certain scenarios.

The term "EM" alone typically refers to Soft-EM.


\subsection{Constrained GMMs}
There are special cases of GMMs by constraining the parameters:
\begin{description}
	\item[Spherical] assumes features are independent and
	have the same variance for each component.
	\begin{equation*}
	\vec{\Sigma}_j = \sigma^2_j \cdot \vec{I}_d
	\end{equation*}
	
	\item[Diagonal] assumes features are independent,
	i.e. Gaussian Naive Bayes in an unsupervised scenario.
	\begin{equation*}
	\vec{\Sigma}_j = diag(\sigma_{j,1}^2, \dotsc, \sigma_{j,d}^2)
	\end{equation*}
	
	\item[Tied] assumes all (or some) 
	covariance matrices are equal.
	\begin{equation*}
	\vec{\Sigma}_1 = \dotsb = \vec{\Sigma}_d
	\end{equation*}
	
	\item[Full] imposes no restrictions.
\end{description}

This is done by tying together parameters in the MLE.

Each constraint reduces the number of parameters and thus
acts as a form of regularisation.


\subsection{Comparison to k-means}
Assume uniform weights $w_1 = \dotsb = w_k = \frac{1}{k}$
and identical, spherical covariances
$\vec{\Sigma}_{1:k} = \sigma^2 \cdot \vec{I}_d$.

Then, the Hard-EM algorithm is equivalent to the
k-means algorithm.
TODO: Derivation?

Conversely, the k-means algorithm can be understood
as limiting case of Soft-EM for GMMs.
The assumptions are the same as above,
with additionally the variances tending to zero.
If $\sigma \to 0$, it holds that
\begin{equation*}
\gamma_j(\vec{x}) =
\begin{cases}
1 & \text{if $\vec{\mu}_j$ is (unique) closest to $\vec{x}$} \\
0 & \text{if not (and there are no ties)}
\end{cases}
\end{equation*}

\subsection{Initialisation}
Fitting GMMs is a non-convex problem
and harder than k-means, i.e. NP-hard.
Thus, initialisation is important.

For initialisation, we typically do
\begin{description}
	\item[Weights] equally or uniformly.
	\item[Means] randomly or via k-means++.
	\item[Variances] spherical,
	e.g. according to empirical variance in the data.
\end{description}


\subsection{Model Selection}
For GMMs, the number of components $k$ needs to be selected.
Generally, the same techniques as for k-means can be used.

However, if the data actually stems from a mixture of Gaussians,
cross-validation can be used to select $k$.
We then aim to maximise the log-likelihood on the validation set.
Even if the underlying process is not a mixture of Gaussians,
cross-validation typically works fairly well
and the validation set provides a signal.


\subsection{Degeneracy}
The Soft-EM optimises a log-likelihood,
and thus might overfit.

Suppose the 1-dimensional case with a single data point $x$.
Then, the negative log-likelihood is
\begin{equation*}
-\log{P(x \mid \mu, \sigma)} =
\frac{1}{2}
\underbrace{\log{(2 \pi \sigma^2)}}_\text{$\to -\infty$ if $\sigma \to 0$}
+
\frac{1}{2 \sigma^2}
\underbrace{(x - \mu)^2}_\text{$0$ if $\mu = x$}
\end{equation*}
and the minimum is given by selecting $\mu = x$ and
then letting $\sigma \to 0$.
Then, the loss converges to $-\infty$.

Therefore, the optimal GMM chooses $k=n$ and
puts one Gaussian on each data point with variance
tending to $0$.
This is a case of overfitting and explains poor test set
log-likelihood.

Degeneracy (i.e. variances tending towards 0)
can be avoided by adding a small term $\nu^2$ to the
diagonal of the MLE:
\begin{equation*}
\vec{\Sigma}^{(t)}_j = \frac{
	\sum_{i=1}^n{
		\gamma_j^{(t)}(\vec{x}_i) (\vec{x}_i - \vec{\mu}_j^{(t)}) (\vec{x}_i - \vec{\mu}_j^{(t)})^T
	}
}{
	\sum_{i=1}^n{\gamma_j^{(t)}(\vec{x}_i)}
}
+ \underbrace{\nu^2 \vec{I}}_\text{additional term}
\end{equation*}

From a Bayesian standpoint,
this is equivalent to placing a (conjugate) Wishart prior
on the covariance matrix,
and computing the MAP instead of MLE.

$\nu$ can be chosen using cross-validation.


\subsection{Use Cases}
Mixture models are useful because
they can encode assumptions about shape
(e.g. fit ellipses instead of points),
can be part of more complex models
(e.g. classifiers),
can output the likelihood of $P(\vec{x})$
and are naturally useful for semi-supervised
learning.

\subsubsection{Clustering}
GMMs are naturally useful for clustering.
Either one or multiple Gaussians can be
used to model clusters.

If only one Gaussian is used per cluster,
a normal GMM can be fitted.
Points are then assigned such that
$P(z \mid \vec{x}, \theta)$ is maximised.

However, assuming that each cluster has
a Gaussian shape is a strong assumption.
Thus, we can also model each class as
a collection of clusters,
i.e. each class is described as a
mixture of Gaussians.

TODO: How exactly should this work?

\subsubsection{Gaussian-Mixture Bayes Classifiers}
GMMs can be used in GBCs to estimate
$P(\vec{x} \mid y)$.

Thus, we first estimate the class prior
$P(y)$ as usual.
Then, for each class $y$,
instead of estimating
$P(\vec{x} \mid y)$ as a single Gaussian,
it is estimated as a GMM:
\begin{equation*}
P(\vec{x} \mid y) = \sum_{j = 1}^{k_y}{
	w_j^{(y)}
	\mathcal{N}(\vec{x}; \vec{\mu}_j^{(y)}, \vec{\Sigma}_j^{(y)})
}
\end{equation*}
Estimation is only done on the
$\vec{x}_i$ with $y_i = y$.

Prediction is then done the same way
as it is done with normal GBCs:
\begin{equation*}
P(y \mid \vec{x}) = \frac{1}{Z}
p(y) \sum_{j = 1}^{k_y}{
	w_j^{(y)}
	\mathcal{N}(\vec{x}; \vec{\mu}_j^{(y)}, \vec{\Sigma}_j^{(y)})
}
\end{equation*}

\subsubsection{Density Estimation}
GMMs can also be used for density estimation.
$P(\vec{x})$ is modelled as a Gaussian mixture,
$P(y \mid \vec{x})$ as some other model
(e.g. logistic regression, ANN).

Then, the joint density is given as
\begin{equation*}
P(\vec{x}, y) = P(\vec{x}) \cdot P(y \mid \vec{x})
\end{equation*}

This combines the robustness of
discriminative models with the
ability to detect outliers.

\subsubsection{Anomaly Detection}
Outlier detection is done by setting some
\emph{threshold} $\tau \in (0, 1)$.
Then, $\vec{x}$ is an outlier if
and only if
$P(\vec{x}) < \tau$.

Setting the decision threshold if no
examples of outliers are available
is challenging.
For example,
a possible goal is to have a constant
fraction of the probability mass
be represented as anomaly, e.g. 1\%.
We can think of anomalies as a positive class.
It is thus often called \emph{one-class classification}.

If examples are available,
cross-validation can be used
to select $\tau$.
Varying $\tau$ trades false-positives
against false-negatives.
Thus, precision-recall or ROC curves
can be used as an evaluation criterion.
F1 score and related metrics can
be used to select the number of clusters
and other hyperparameters.

\subsubsection{Semi-Supervised Learning}
Semi-supervised learning can be interpreted
as a missing-data problem where
some but not all labels are missing.
Semi-supervised learning is learning
from a large amount of unlabelled data and
a small amount of labelled data.

GMMs can easily be extended to
semi-supervised learning.

For instances $\vec{x}_i$ with known
label $y_i$, it must hold that
$\gamma_j(\vec{x}_i) = [j = y_i]$.
Thus, the EM algorithm can be modified.
In the E-step, we differentiate between
labelled and unlabelled samples.
For unlabelled samples,
we calculate the original responsibility
$\gamma_j^{(t)}(\vec{x}_i) = P(Z = j \mid \vec{x}_i, \vec{\mu}^{(t - 1)}, \vec{\Sigma}^{(t - 1)}, \vec{w}^{(t - 1)})$.
For labelled samples, we set $\gamma_j(\vec{x}_i) = [j = y_i]$.
The M-step does not change.


\subsection{Theory behind the EM Algorithm}
\subsubsection{Equivalent formulation}
The \emph{complete data log-likelihood} under $\theta$ is
\begin{equation*}
\log{P(\vec{x}_{1:n} \mid \theta)}
\end{equation*}
or alternatively, including latent variables
\begin{equation*}
\log{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)}
\end{equation*}
where $z_i$ describes the responsibilities
(as a random variable)
of data point $\vec{x}_i$.

The \emph{expected complete data log-likelihood}
as a function of $\theta$,
written as $Q(\theta; \Tilde{\theta})$, is
\begin{align*}
Q(\theta; \Tilde{\theta}) &=
\Exp_{z_{1:n}}[
\log{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)} \mid \vec{x}_{1:n}, \Tilde{\theta}
] \\
&= \sum_{z_{1:n}}{
	P(z_{1:n} \mid \vec{x}_{1:n}, \Tilde{\theta}) \log{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)}
}
\end{align*}
$Q(\theta; \Tilde{\theta})$ is the EM objective.

We can show that the EM algorithm does the following:
In the E-step, the expected data log-likelihood
(as a function of $\theta$) $Q(\theta; \theta^{(t-1)})$
is calculated.
In the M-step, it is maximised:
\begin{equation*}
\theta^{(t)} = \arg\max_\theta{Q(\theta; \theta^{(t-1)})}
\end{equation*}

$Q(\theta; \theta^{(t-1)})$ can be simplified:
\begin{align*}
Q(\theta; \theta^{(t-1)}) &=
\Exp_{z_{1:n}}\left[
\log{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)} \mid \vec{x}_{1:n}, \theta^{(t-1)}
\right] \\
&\overset{iid}{=} \Exp_{z_{1:n}}\left[
\sum_{i=1}^n{
	\log{P(\vec{x}_i, z_i \mid \theta)} \mid \vec{x}_{1:n}, \theta^{(t-1)}
}
\right] \\
&= \sum_{i=1}^n{ \Exp_{z_{1:n}}\left[
	\log{P(\vec{x}_i, z_i \mid \theta)} \mid \vec{x}_{1:n}, \theta^{(t-1)}
	\right]
} \\
&= \sum_{i=1}^n{ \Exp_{z_i}\left[
	\log{P(\vec{x}_i, z_i \mid \theta)} \mid \vec{x}_i, \theta^{(t-1)}
	\right]
} \\
&= \sum_{i=1}^n{
	\sum_{j=1}^k{
		\underbrace{P(z_i = j \mid \vec{x}_i, \theta^{(t-1)})}_{\gamma_j(\vec{x}_i)}
		\cdot
		\log{\underbrace{P(\vec{x}_i, z_i = j \mid \theta)}_{w_j \mathcal{N}(\vec{x}_i; \vec{\mu}_j, \vec{\Sigma}_j)}}
	}
}
\end{align*}

Thus, the EM objective function is equivalent to
\begin{equation*}
Q(\theta; \theta^{(t-1)})
= \sum_{i=1}^n{\sum_{z_i=1}^k{
		\gamma_{z_i}(\vec{x}_i)
		\log{P(\vec{x}_i, z_i \mid \theta)}
}}
\end{equation*}

Thus, the E-step is equivalent to computing
the \emph{expected sufficient statistics}
$\gamma_z(\vec{x}_i) = P(z \mid \vec{x}_i, \theta^{(t-1)})$.

In the M-step, we compute
\begin{equation*}
\theta^{(t)} = \arg\max_\theta{
	Q(\theta; \theta^{(t-1)})
}
= \arg\max_\theta{
	\sum_{i=1}^n{
		\sum_{z_i=1}^k{
			\gamma_{z_i}(\vec{x}_i)
			\log{P(\vec{x}_i, z_i \mid \theta)}
		}
	}
}
\end{equation*}
This is equivalent to training a GBC with
weighted data.
This has a closed-form solution
(which is given in the EM algorithm pseudocode).

\subsubsection{Convergence}
Using the alternative formulation,
we can prove that the EM algorithm
monotonically increases the likelihood:
\begin{equation*}
\log{P(\vec{x}_{1:n} \mid \theta^{(t)})}
\geq
\log{P(\vec{x}_{1:n} \mid \theta^{(t-1)})}
\end{equation*}

First, we rewrite
\begin{equation*}
P(\vec{x}_{1:n} \mid \theta)
= P(\vec{x}_{1:n} \mid \theta) \cdot \frac{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)}{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)}
= \frac{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)}{P(z_{1:n} \mid \vec{x}_{1:n}, \theta)}
\end{equation*}

The logarithm and expectation are both
monotonous functions and therefore
do not change monotonicity.
Thus, applying both functions to
$P(\vec{x}_{1:n} \mid \theta)$ gives
\begin{align*}
& \Exp_{z_{1:n}}\left[
\log{P(\vec{x}_{1:n} \mid \theta)}
\mid \vec{x}_{1:n}, \theta^{(t-1)}
\right] \\
&= \Exp_{z_{1:n}}\left[
\log{\frac{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)}{P(z_{1:n} \mid \vec{x}_{1:n}, \theta)}}
\mid \vec{x}_{1:n}, \theta^{(t-1)}
\right] \\
&= \Exp_{z_{1:n}}\left[
\log{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)} - \log{P(z_{1:n} \mid \vec{x}_{1:n}, \theta)}
\mid \vec{x}_{1:n}, \theta^{(t-1)}
\right] \\
&= \underbrace{
	\Exp_{z_{1:n}}\left[
	\log{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)}
	\mid \vec{x}_{1:n}, \theta^{(t-1)}
	\right]
}_{Q(\theta; \theta^{(t-1)})}
-
\underbrace{
	\Exp_{z_{1:n}}\left[
	\log{P(z_{1:n} \mid \vec{x}_{1:n}, \theta)}
	\mid \vec{x}_{1:n}, \theta^{(t-1)}
	\right]
}_{f(\theta)}
\end{align*}

Therefore, for monotonicity
it suffices to show
\begin{align*}
Q(\theta^{(t)}; \theta^{(t-1)}) &\geq Q(\theta^{(t - 1)}; \theta^{(t-1)}) \\
f(\theta^{(t)}) &\leq f(\theta^{(t-1)})
\end{align*}

The first inequality follows directly from
the fact that in the M-step,
we take $\theta^{(t)} = \arg\max_\theta{Q(\theta; \theta^{(t-1)})}$.

It remains to show that $f(\theta^{(t)}) \leq f(\theta^{(t-1)})$,
i.e.
\begin{align*}
& \Exp_{z_{1:n}}\left[
\log{P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t)})}
\mid \vec{x}_{1:n}, \theta^{(t-1)}
\right]
\leq
\Exp_{z_{1:n}}\left[
\log{P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t - 1)})}
\mid \vec{x}_{1:n}, \theta^{(t-1)}
\right] \\
&\Leftrightarrow
\Exp_{z_{1:n}}\left[
\log{P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t - 1)})}
-
\log{P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t)})}
\mid \vec{x}_{1:n}, \theta^{(t-1)}
\right]
\geq 0 \\
&\Leftrightarrow
\Exp_{z_{1:n}}\left[
\log{
	\frac{P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t - 1)})}{P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t)})}
}
\mid \vec{x}_{1:n}, \theta^{(t-1)}
\right]
\geq 0 \\
&\Leftrightarrow
\sum_{z_{1:n}}{
	P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t-1)})
	\log{
		\frac{P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t - 1)})}{P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t)})}
	}
}
\geq 0 \\
&\Leftrightarrow
KL(P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t - 1)}) || P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t)}))
\geq 0
\end{align*}
This holds because the KL-divergence is non-negative.

The \emph{Kullback-Leibler divergence} $KL(p || q)$
for two distributions $p, q$ is defined as
\begin{equation*}
KL(p || q) = \Exp_{\vec{x} \sim p}\left[
\log{\frac{p(\vec{x})}{q(\vec{x})}}
\right]
= \begin{cases}
\sum_{\vec{x}}{p(\vec{x}) \log{\frac{p(\vec{x})}{q(\vec{x})}}}
& \text{discrete} \\
\int{p(\vec{x}) \log{\frac{p(\vec{x})}{q(\vec{x})}} d\vec{x}}
& \text{continuous}
\end{cases}
\end{equation*}

The KL-divergence is non-negative
and in general not symmetric.

\subsubsection{Hard-EM Convergence}
TODO: Hard-EM convergence

\subsubsection{EM Algorithm more generally}
The EM algorithm is much more widely applicable.
It can be used whenever the E and M steps are
tractable,
i.e. it is possible to
\emph{compute} and \emph{maximise}
the complete data likelihood.

The EM algorithm can even be used in
the case of missing features,
not only missing labels.


\subsection{Bernoulli Mixture Models}
TODO: Bernoulli mixture models, tutorial 14


\subsection{Useful Concepts}
\subsubsection{Entropy}
For a discrete probability distribution $p$,
the \emph{entropy} is defined as
\begin{equation*}
\mathcal{H}(p) = \sum_x{-p(x) \log{p(x)}}
\end{equation*}
The more "spread out" a distribution is,
the higher its entropy,
and the more concentrated the lower its entropy.
A uniform distribution has the highest entropy.

\subsubsection{Jensenâ€™s Inequality}
If $f$ is a convex function, \emph{Jensen's inequality} is
\begin{equation*}
f(\Exp[X]) \leq \Exp[f(x)]
\end{equation*}
If $X$ is constant, this is equal.
If $f$ is a concave function,
the inequality is reversed.

\subsubsection{KL Divergence}
The \emph{KL-divergence} measures difference between
two distributions.
Let $p$ and $q$ be discrete probability distributions.
Then, the KL-divergence is defined as
\begin{equation*}
KL(p || q) = \sum_x{p(x) \log{\frac{p(x)}{q(x)}}}
\end{equation*}

The KL-divergence is only defined if
for each $x$ $q(x) = 0 \Rightarrow p(x) = 0$,
and in that case is zero at that point.

It is also generally not symmetric
and becomes zero if $p = q$.

It can be shown that the KL-divergence is
always positive using Jensen's inequality:
\begin{equation*}
KL(p || q) = -\sum_x{p(x) \log{\frac{p(x)}{q(x)}}} \geq - \log{
	\sum_x{p(x) \frac{p(x)}{q(x)}}
} = 0
\end{equation*}


\section{Generative Adversarial Networks}
The previously considered generative models
were very simple and fail to capture high-dimensional,
complex data such as audio or images.
Thus, the goal is to use neural networks
for generative modelling.

Given a sample $\vec{x}_1, \dotsc, \vec{x}_n$,
\emph{implicit generative models} are models
of the form
\begin{equation*}
\vec{X} = G(\vec{Z}; \vec{w})
\end{equation*}
where $\vec{Z}$ is a simple distribution
(e.g. low-dimensional Gaussian),
$G$ is a flexible non-linear mapping
(e.g. neural network)
and $\vec{X}$ is the data-generating process.

The key challenge with implicit generative models
is to compute the data likelihood,
which is generally not possible.
If we start with a simple distribution
and know $P(z)$, what is $P(\vec{x})$?
This not only depends on the prior but
also on the parameters.
The distribution is very complicated,
intractable in general,
and therefore the maximum likelihood
cannot just be calculated as usual.
Thus, surrogate objective functions
are required for training.
This leads to
\emph{Variational Autoencoders} (VAEs)
and \emph{Generative Adversarial Networks} (GANs).

\subsection{Formulation}
The key idea is to optimise the parameters
such that the samples from the model are
hard to distinguish from the real data samples.
Distinguishing is a classification problem,
thus a discriminator network is trained to do so.
This is called \emph{discriminative learning}.

Now, two neural networks are trained simultaneously.
The \emph{generator} network $G : \R^m \to \R^d$
tries to generate realistic examples;
the \emph{discriminator} network
$D : \R^d \to [0, 1]$
tries to distinguish real and generated samples.

The discriminator wants
\begin{equation*}
D(\vec{x}) =
\begin{cases}
\approx 1 & \text{if $x$ is real} \\
\approx 0 & \text{if $x$ is generated}
\end{cases}
\end{equation*}
The generator wants
\begin{equation*}
D(G(\vec{z})) \approx 1 \text{ for samples $\vec{z}$}
\end{equation*}
This results in the following \emph{minimax} game:
\begin{equation*}
\min_G{\max_D{
		\Exp_{\vec{x} \sim \text{Data}}[\log{D(\vec{x})}]
		+
		\Exp_{\vec{z} \sim \vec{Z}}[\log{(1 - D(G(\vec{z}))}]
}}
\end{equation*}
Here, the logarithm was used,
but there are other monotonic
transformations used.


\subsection{Training}
The original objective optimises over
$G$ and $D$ directly.
However, we want to fix the function class
(i.e. neural network architecture)
and optimise over weights:
\begin{equation*}
\min_{\vec{w}_G}{\max_{\vec{w}_D}{
		\underbrace{
			\Exp_{\vec{x} \sim \text{Data}}[\log{D(\vec{x}; \vec{w}_D)}]
			+
			\Exp_{\vec{z} \sim \vec{Z}}[\log{(1 - D(G(\vec{z}; \vec{w}_G); \vec{w}_D)}]
		}_{M(\vec{w}_G, \vec{w}_D)}
}}
\end{equation*}

From the conflicting objective,
optimising
\begin{equation*}
\min_{\vec{w}_G}{\max_{\vec{w}_D}{
		M(\vec{w}_G, \vec{w}_D)
}}
\end{equation*}
requires finding a \emph{saddle point}
rather than a minimum.

If $G$ and $D$ have enough capacity,
the data-generating distribution is
actually a saddle point of
$\min_{\vec{w}_G}{\max_{\vec{w}_D}{M(\vec{w}_G, \vec{w}_D)}}$.
This is because
$\max_{\vec{w}_G}{M(\vec{w}_G, \vec{w}_D)}$ is
up to constants the Jensen-Shannon divergence
$JS(P_\text{Data} || P_G)$.

The \emph{Jensen-Shannon divergence} between
two distributions $p, q$ is defined as
\begin{equation*}
JS(p || q) =
\frac{1}{2} KL(p || \frac{p+q}{2})
+
\frac{1}{2} KL(q || \frac{p+q}{2})
\end{equation*}
and is sort-of a symmetric version
of the KL-divergence.
It holds (under some restrictions) that
$JS(p || q) = 0 \Leftrightarrow p = q$.

Commonly, (mini-batch) SGD is applied to
the generator and discriminator simultaneously:
\begin{align*}
\vec{w}_G^{(t+1)} &= \vec{w}_G^{(t)} - \eta_t \nabla_{\vec{w}_G} M(\vec{w}_G, \vec{w}_D^{(t)}) \\
\vec{w}_D^{(t+1)} &= \vec{w}_D^{(t)} + \eta_t \nabla_{\vec{w}_D} M(\vec{w}_G^{(t)}, \vec{w}_D) \\
\end{align*}
where the different signs indicate the difference
of minimising/maximising.
There are also other variants where the
networks are updated sequentially.


\subsection{Challenges}
GANs are notoriously hard to train,
and many tricks/heuristics exist to
mitigate some problems.

\subsubsection{Oscillations / Divergence}
Because two conflicting objectives are optimised,
it might happen that the weights oscillate
around a local minimum or even diverge.

\subsubsection{Mode Collapse}
It might happen that the generator models
certain modes of the data well
but completely ignores others.

This can happen if the discriminator
cannot detect that only samples from
a certain mode are generated.
It might also happen that the
well-modelled modes switch as
soon as the discriminator has adjusted.

\subsubsection{Data Memorisation}
The GAN objective is similar to minimising
a JS-divergence.
In practice,
if the JS divergence of the sample data
would be minimised,
the optimal generator would just
uniformly sample from the data set.

Thus, the model is prone to memorising
the data set instead of actually learning the distribution.
By going away from the idealised setting,
i.e. making restricting the discriminator capacity,
we get better samples.

\subsubsection{Evaluation}
Evaluating GANs is currently an unresolved issue.
Because the likelihood is intractable,
it cannot be calculated on a holdout set.

There exist various heuristics,
but no domain-independent solutions.
