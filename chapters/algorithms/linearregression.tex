\section{Linear Regression}
The model assumption is that $y \approx f(x)$ where $f(x)$ is
linear (affine) in $x$,
i.e. $f(\vec{x}) = \vec{w}^T \vec{x} + w_0$.
We generally use a homogeneous representation:
\begin{equation*}
    \vec{w}^T \vec{x} + w_0 = \Tilde{\vec{w}}^T \Tilde{\vec{x}}
\end{equation*}
where $\Tilde{\vec{w}} = [w_1, \dotsc, w_d, w_0]^T$ and
$\Tilde{\vec{x}} = [x_1, \dotsc, x_d, 1]^T$.
For equality, noise denoted by a random variable $\epsilon$
has to be accounted for, i.e.
$y = \Tilde{\vec{w}}^T \Tilde{\vec{x}} + \epsilon$.
For a complete dataset $\vec{X}$ with labels $\vec{y}$,
this results in
\begin{equation*}
    \vec{y} = \vec{X} \vec{w} + \vec{\epsilon}
\end{equation*}

The data set is $D = \{(\vec{x}_1, y_1), \dotsc, (\vec{x}_n, y_n)\}$
with $\vec{x}_i \in \R^d$ and $y_i \in \R$.

A residual is $r_i = y_i - \vec{w}^T \vec{x}_i$ and the
vector of residuals is given as
$\vec{r} = \vec{y} - \vec{X}\vec{w}$.

\subsection{Least-Squares Linear Regression}
For least-squares linear regression, the empirical cost is
\begin{equation*}
    \hat{R}(\vec{w}) = \norm{\vec{r}}_2^2
    = \sum_{i = 1}^n{r_i^2}
    = \sum_{i = 1}^n{(y_i - \vec{w}^T \vec{x}_i)^2}
\end{equation*}
and we try to find $\vec{w}^*$ with
\begin{equation*}
    \vec{w}^* = \arg\min_{\vec{w}}{
        \sum_{i = 1}^n{(y_i - \vec{w}^T \vec{x}_i)^2}
    }
\end{equation*}

This can be rewritten as
\begin{equation*}
    \hat{R}(\vec{w})
    = \norm{\vec{y} - \vec{X}\vec{w}}_2^2
    = \vec{y}^T \vec{y} - 2 \vec{y}^T \vec{X} \vec{w} + \vec{w}^T \vec{X}^T \vec{X} \vec{w}
\end{equation*}

The problem can be solved in closed form as
\begin{equation*}
    \vec{w}^* = (\vec{X}^T \vec{X})^{-1} \vec{X}^T \vec{y}
\end{equation*}
where $\vec{X}$ is a matrix whose rows are the samples and
$\vec{y}$ is a column vector containing the corresponding labels.

The objective function is convex, thus we can also use gradient
descent to find the global optimum.
With step size $\frac{1}{2}$,
gradient descent converges linearly.
For an $\epsilon$, we can find $x_t$ so that
$f(x_t) - \min_x(f(x)) \leq \epsilon$ in time
$\mathcal{O}(\log \frac{1}{\epsilon})$.

The gradient in the 1-dimensional case is given as
\begin{equation*}
    \nabla \hat{R}(w)
    = -2 \sum_{i = 1}^n{r_i \cdot x_i}
\end{equation*}
and in the $d$-dimensional case as
\begin{equation*}
    \nabla \hat{R}(\vec{w})
    = -2 \sum_{i = 1}^n{r_i \cdot \vec{x}_i^T}
    = 2 \vec{w}^T \vec{X}^T \vec{X} - 2 \vec{y}^T \vec{X}
\end{equation*}.
Note that $\hat{R}(\vec{w}) \in \R^{1 \times d}$.

The closed-form solution takes $\mathcal{O}(n^3)$ time.
The gradient descent solution takes
$\mathcal{O}(n \cdot d \log \frac{1}{\epsilon})$ time.

Thus, gradient descent might converge faster.
Furthermore, an optimal solution might not be required,
and many problems do not admit a closed-form solution.

Instead of the squared error, other loss functions might be useful:
\begin{equation*}
    \hat{R}(\vec{w}) = \sum_{i = 1}^n{l_p(y_i - \vec{w}^T \vec{x}_i)}
\end{equation*}
where $l_p = |r|^p$.

$l_p$ loss is convex for $p \geq 1$.
The $l_1$ loss puts less emphasis on large outliers.
For a large $p$, the results are restricted to a region.
For $p < 1$, the function becomes non-convex but is even more
robust towards outliers.

The family of $L_p$ norms is defined as
\begin{equation*}
    \norm{\vec{w}}_p =
    \sqrt[p]{\sum_{i=1}^d{|w_i|^p}}
\end{equation*}
and their partial derivative
\begin{equation*}
    \frac{\partial \norm{\vec{w}}_p^p}{\partial w_j}
    = p |w_j|^{p-1} \cdot sign(w_j)
\end{equation*}


\subsection{Feature Transformations}
We can fit non-linear functions via linear regression
by using non-linear basis functions of the data:
\begin{equation*}
    f(\vec{x}) = \sum_{i=1}^D{w_i \phi_i(\vec{x})}
\end{equation*}
with $\phi : \R^d \to \R^D$.
$\phi$ is called a feature map.

The model is still linear,
but on the feature map,
not on the original features.

To fit polynomial functions of $d$-dimensions,
$\phi(\vec{x})$ is the vector of all monomials of degree
up to $k$ in variables $x_1, \dotsc, x_d$.
Monomials are polynomials with only one term and coefficient $1$.

For a periodic function
\begin{equation*}
    y(t) = a_0 +
    \sum_{n=1}^\infty{(a_n \cos{nt} + b_n \sin{nt})} + \epsilon
\end{equation*}
we can use a feature map
\begin{equation*}
    \phi(t) = [1, \cos{t}, \sin{t}, \cos{2t}, \sin{2t}, \dotsc]
\end{equation*}


\subsection{Ridge Regression}
Least squares regression is not strongly convex,
prone to overfitting and may be ill-behaved if
some features are correlated.
The optimisation problem can be made strongly convex by adding
a regularisation term $\lambda > 0$:
\begin{equation*}
    \min_{\vec{w}}{\frac{1}{n}
        \sum_{i=1}^n{(y_i - \vec{w}^T \vec{x}_i)^2
            + \lambda \norm{\vec{w}}_2^2
    }}
\end{equation*}
This results in the new empirical risk
\begin{equation*}
    \hat{R}_{ridge}(\vec{w})
    = \norm{\vec{y} - \vec{X}\vec{w}}_2^2
        + \lambda \norm{\vec{w}}_2^2
\end{equation*}

The scale of $\vec{x}$ now matters!
If the features are scaled, it changes the scale of $\lambda$.
Thus, the data has to be standardised.

The closed-form solution is
\begin{equation*}
    \hat{\vec{w}} =
    (\vec{X}^T \vec{X} + n \lambda \vec{I})^{-1} \vec{X}^T \vec{y}
\end{equation*}

The gradient is
TODO: Is this really correct? (Derivative/gradient)
\begin{equation*}
    \nabla_{\vec{w}} \hat{R}_{ridge}(\vec{w})
    = \nabla_{\vec{w}} \hat{R}(\vec{w})
        + \lambda \nabla_{\vec{w}} \norm{\vec{w}}_2^2
    = 2 \vec{w}^T \vec{X}^T \vec{X} - 2 \vec{y}^T \vec{X} + 2 \vec{w}^T
\end{equation*}
resulting in an update step
\begin{equation*}
    \vec{w}_{t+1} =
    (1 - 2\lambda \eta_t)\vec{w}_t - \eta_t \nabla \hat{R}(\vec{w})
\end{equation*}

Ridge regression is also called $L_2$-regularisation
or weight decay (as it decays weights to zero).
If $\lambda \to \infty$ then $w \to 0$.


\subsection{Lasso Regression}
Lasso regression is $L_1$ regularised
least squares linear regression.

The objective then becomes
\begin{equation*}
    \min_{\vec{w}}{\frac{1}{n}
        \sum_{i=1}^n{(y_i - \vec{w}^T \vec{x}_i)^2
            + \lambda \norm{\vec{w}}_1
    }}
\end{equation*}

The penalty, in theory, encourages coefficients to be
exactly zero.
This results in some form in automatic feature selection.

While in ridge regression, weights decay relatively smoothly,
in lasso regression, some weights may heavily increase
with increasing $\lambda$.
This makes intuitive sense because one weight may suddenly
"perform the work" that multiple weights did before.


\subsection{Kernelised Linear Regression}
In kernelised linear regression,
regularisation is already built-in.

Let $\vec{K}$ be the Gram matrix and $\vec{k}_i$ the
$i$-th column of the Gram matrix.

The kernelised objective for the squared loss is
\begin{equation*}
    \left(\sum_{j=1}^n{\alpha_j k(\vec{x}_j, \vec{x}_i)} - y_i\right)^2
    = (\vec{\alpha}^T \vec{k}_i - y_i)^2
\end{equation*}
and for the $L_2$ regulariser
\begin{equation*}
    \sum_{i=1}^n{\sum_{j=1}^n{
        \alpha_i \alpha_j k(\vec{x}_i, \vec{x}_j)
    }}
    = \vec{\alpha}^T \vec{K} \vec{\alpha}
\end{equation*}

The kernelised optimisation objective is then
\begin{equation*}
    \hat{\vec{\alpha}} = \arg\min_{\vec{\alpha}}{
        \frac{1}{n}
        \underbrace{
        \sum_{i=1}^n{
            (\vec{\alpha}^T \vec{k}_i - y_i)^2
        }
        }_{\norm{\vec{\alpha}^T \vec{K} - \vec{y}}_2^2}
        + \lambda \vec{\alpha}^T \vec{K} \vec{\alpha}
    }
\end{equation*}

The closed-form solution (with regulariser) is given as
\begin{equation*}
    \hat{\vec{\alpha}} = (
        \vec{K} + \lambda \vec{I}
    )^{-1} \vec{y}
\end{equation*}

For a data point $\vec{x}$, the prediction is
\begin{equation*}
    \hat{y} = \sum_{i=1}^n{\hat{\alpha}_i k(\vec{x}_i, \vec{x})}
\end{equation*}


\subsubsection{Loss Functions}
TODO: Functions from tutorial 3
TODO: Tutorial 3, 3.1
