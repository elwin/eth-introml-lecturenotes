\section{Perceptron}
The general idea is to fit a model
$h: \mathbb{R}^d \to \{+1, -1\}$ where $+1$ denotes the positive
class, $-1$ the negative class:
$h(\vec{x}) = sign(\vec{w}^T \vec{x})$.

In linear classification, the assumption is that
the classification boundary is a hyperplane
going through the origin.

The $\ell_{0/1}$ loss is defined as follows
\begin{equation*}
    \ell_{0/1}(\vec{w}; \vec{x}_i, y_i) =
    \begin{cases}
        1 & \text{if $y_i \neq sign(\vec{w}^T \vec{x}_i)$} \\
        0 & \text{otherwise}
    \end{cases}
\end{equation*}

The goal is to minimise the $\ell_{0/1}$ loss.
However, it is not convex and not differentiable.
Thus, a surrogate loss is needed.

The perceptron loss is a convex surrogate for the
$\ell_{0/1}$ loss, defined as
\begin{equation*}
    \ell_p(\vec{w}; \vec{x}, y) =
    \max(0, -y \vec{w}^T \vec{x})
\end{equation*}
and is a surrogate for the $\ell_{0/1}$ loss.
It is $0$ if the sign of the prediction and $y$ match,
and linear in the misclassification otherwise.

The gradient of the perceptron loss for a single sample is
\begin{equation*}
    \nabla_{\vec{w}} \ell_p(\vec{w}; \vec{x}_i, y_i) =
    \begin{cases}
        0 & \text{if $\vec{w}^T \vec{x}_i y_i \geq 0$} \\
        -y_i \vec{x}_i & \text{if $\vec{w}^T \vec{x}_i y_i < 0$}
    \end{cases}
\end{equation*}

Thus, the gradient of the empirical risk of the perceptron loss is
\begin{equation*}
    \nabla_{\vec{w}} \hat{R}(\vec{w})
    = - \frac{1}{n} \sum_{i : (\vec{x}_i, y_i) \text{incorrect}}{
        y_i \vec{x}_i
    }
\end{equation*}

The perceptron algorithm performs stochastic gradient descent
(1 sample at a time) on the perceptron loss function
$\ell_p$ with learning rate $1$.
In practice, a different learning rate is used
because the data is not usually linearly separable.

\begin{theorem}
    If the data is linearly separable, the perceptron will obtain
    a linear separator.
\end{theorem}


\subsection{Kernelised Perceptron}
The reformulation of the perceptron in terms of inner
products is
\begin{equation*}
    \hat{\vec{\alpha}} = \arg\min_{\vec{\alpha}}{
        \frac{1}{n}
        \sum_{i=1}^n{\max\{
            0, -\sum_{j=1}^n{\alpha_j y_i y_j \vec{x}_i^T \vec{x}_j}
        \}}
    }
\end{equation*}
and the kernelised objective
\begin{equation*}
    \hat{\vec{\alpha}} = \arg\min_{\vec{\alpha}}{
        \frac{1}{n}
        \sum_{i=1}^n{\max\{
            0, -\sum_{j=1}^n{\alpha_j y_i y_j k(\vec{x}_i, \vec{x}_j)}
        \}}
    }
\end{equation*}

Prediction for sample $\vec{x}$ is done as
\begin{equation*}
    \hat{y} = sign\left(
        \sum_{j=1}^n{\alpha_j y_j k(\vec{x}_j, \vec{x})}
    \right)
\end{equation*}

Training is very similar to the original formulation.
If the wrong class for sample $\vec{x}_i$ is predicted,
the update is
\begin{equation*}
    \alpha_{t+1, i} \gets \alpha_{t, i} + \eta_t
\end{equation*}
