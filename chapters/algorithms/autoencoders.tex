\section{Autoencoders}
The key idea is to learn the identity function
$\vec{x} \approx f(\vec{x}; \theta)$.
We can also think of it as learning a
compression and decompression function, both in one.
$f(\vec{x};\theta) = f_2(f_1(\vec{x};\theta_1);\theta_2)$ where
$f_2$ is the decoder, $f_1$ is the encoder.
Both functions are learned jointly.

\emph{Neural network autoencoders} are ANNs where
there is one output unit for each of the $d$ inputs
and the number of hidden units $k$ is typically smaller
than the number of inputs.

An example objective function is the sum of squared errors:
\begin{equation*}
    \arg\min_{\vec{W}}{
        \sum_{i=1}^n{
            \norm{\vec{x}_i - f(\vec{x}_i; \vec{W})}_2^2
        }
    }
\end{equation*}

The objective can be optimised using SGD.
However, initialisation is important and challenging.

If all non-linearities of an autoencoder
are the identity function,
then fitting an autoencoder is equivalent to PCA.
