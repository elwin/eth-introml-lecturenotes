\section{Gaussian Mixture Models}
\emph{Gaussian Mixture Models} (GMMs) are generative models.
Because generative modelling describes tries to describe all
the data, it can naturally be used in the case of missing data.
Unsupervised learning is an extreme case of missing data
where all the labels are absent.

In GMMs, we assume the data was generated by a mixture of Gaussians.
A \emph{Gaussian mixture} is a convex combination of
$k$ Gaussian distributions:
\begin{equation*}
    P(\vec{x} \mid \theta)
    = P(\vec{x} \mid \vec{\mu}, \vec{\Sigma}, \vec{w})
    = \sum_{i=1}^k{
        w_i \cdot \mathcal{N}(\vec{x} ; \vec{\mu}_i, \vec{\Sigma}_i)
    }
\end{equation*}
with $w_i \geq 0$ and $\sum_{i=1}^k{w_i} = 1$,
i.e. defining a categorical distribution.
The $w_i$ are called the \emph{mixture weights}.

Mixture models in general are convex combinations
of $k$ simple (base) distributions.
Mixture models are more expressive than
the base distributions as they allow
for multimodal data representation.
GMMs are thus mixture models where all
base distributions are Gaussians.

The negative log likelihood,
assuming the samples $\vec{x}_1, \dotsc, \vec{x}_n$
are i.i.d., is
\begin{equation*}
    (\vec{\mu}^*, \vec{\Sigma}^*, \vec{w}^*)
    = \arg\min{
        -\sum_{i=1}^n{\log{
            \sum_{j=1}^k{
            w_j \mathcal{N}(\vec{x}_i \mid \vec{\mu}_j, \vec{\Sigma}_j)
            }
        }}
    }
\end{equation*}
This is a non-convex objective.
Because it contains the logarithm of a sum,
no closed-form solution can be derived.
Furthermore, SGD is challenging to apply because
the covariance matrices must remain symmetric positive definite.

Let $z$ be a cluster index and $\vec{x}$ a feature.
The joint distribution
$P(z, \vec{x}) = w_z \mathcal{N}(\vec{x} \mid \vec{\mu}_u, \vec{\Sigma}_z)$
is identical to the generative model used by a GBC.
However, in contrast to a GBC, the cluster index
$z$ is a latent variable, i.e. unobserved.
Thus, fitting a GMM is equivalent to training
a GBC without labels and
a natural approach would be to try to infer the labels.


\subsection{Hard-EM}
Let $\theta^{(t)} = (\vec{w}^{(t)}, \vec{\mu}^{(t)}, \vec{\Sigma}^{(t)})$
denote the parameters at step $t$.

First, (carefully) initialise the parameters
$\theta^{(0)}$.

Then, for $t = 1, 2, \dotsc$
\begin{description}
    \item[E-step]: Predict the most likely class for each data point.
    \begin{equation*}
        z_i^{(t)} = \arg\max_z{
            P(z \mid \vec{x}_i, \theta^{(t-1)})
        }
        = \arg\max_z{
            \underbrace{P(z \mid \theta^{(t-1)})}_{w_z^{(t-1)}}
            \cdot
            \underbrace{P(\vec{x}_i \mid z, \theta^{(t-1)})}_{\mathcal{N}(\vec{x}_i; \vec{\mu}_i^{(t-1)}, \vec{\Sigma}_i^{(t-1)})}
        }
    \end{equation*}
    This gives the "completed" data set $D^{(t)}$.
    \item[M-step]: Compute MLE for the GBC.
    \begin{equation*}
        \theta^{(t)} = \arg\max_\theta{P(D^{(t)} \mid \theta)}
    \end{equation*}
\end{description}

The intuitive interpretation is as follows:
In the E-step, the (missing) class labels are inferred
using the GBC of the previous round.
This gives a completed data set for each time step.
Then, in the M-step, MLE for a GBC is performed,
using the previously inferred class labels.

Hard-EM is conceptually very similar to the k-means algorithm.

\subsubsection{Issues}
The Hard-EM algorithm has various issues.
In particular, each point is forced to
declare allegiance to some cluster,
even though the model is uncertain.
Intuitively, this tries to extract too much
information from a single point.

A GMM with $k$ mixture components can always
represent mixtures with $< k$ components.
By setting some mixture weights to $0$,
or putting multiple distributions directly
on top of each other,
all but one mixture components can be eliminated.

However, suppose a data set is generated by a single
Gaussian distribution.
If, for example, $k=2$, the optimal solution for
the Hard-EM algorithm are two mixtures which each
contain roughly half of the points.

Thus, Hard-EM works poorly in practice if clusters
are overlapping.


\subsection{Soft-EM}
To fix the issues of Hard-EM,
we can allow a data point to be assigned to multiple clusters
at the same time.

Suppose we have models $P(z \mid \theta)$ and 
$P(\vec{x} \mid \theta)$.
Then, for each data point, the posterior distribution over
cluster membership can be calculated,
i.e. inferring distributions over the latent variables $z$.

$\gamma_j(\vec{x})$ is the \emph{responsibility},
denoting the probability that $\vec{x}$ belongs to component $j$,
i.e. $\gamma_j(\vec{x}) = P(Z = j \mid \vec{x}, \theta)$.
This can be calculated as
\begin{equation*}
    \gamma_j(\vec{x}) = \frac{
        \overbrace{w_j}^{P(Z = j \mid \theta)}
        \overbrace{P(\vec{x} \mid \vec{\Sigma}_j, \vec{\mu}_j)}^{P(\vec{x} \mid Z = j, \theta)}
    }{
        \underbrace{
        \sum_{l=1}^k{
            w_l P(\vec{x} \mid \vec{\Sigma}_l, \vec{\mu}_l)
        }
        }_{P(x \mid \theta)}
    }
\end{equation*}

At the MLE $(\vec{\mu}^*, \vec{\Sigma}^*, \vec{w}^*)$,
it must hold that
\begin{align*}
    \vec{\mu}^*_j &= \frac{\sum_{i=1}^n{\gamma_j(\vec{x}_i) \vec{x}_i}}{\sum_{i=1}^n{\gamma_j(\vec{x}_i)}} \\
    \vec{\Sigma}^*_j &= \frac{
    \sum_{i=1}^n{
    \gamma_j(\vec{x}_i) (\vec{x}_i - \vec{\mu}_j^*) (\vec{x}_i - \vec{\mu}_j^*)^T
    }
    }{
        \sum_{i=1}^n{\gamma_j(\vec{x}_i)}
    }
    \\
    w_j^* &= \frac{1}{n} \sum_{i=1}^n{\gamma_j(\vec{x}_i)}
\end{align*}
All parameters are simply weighted averages,
according to the responsibilities.
The issue is that the equations are coupled,
i.e. the $\gamma_j$ depend on
all the model parameters and vice versa.

The \emph{Soft-EM} still allows to optimise them.
For $t = 1, 2, \dotsc$
\begin{description}
    \item[E-step]: Calculate cluster membership weights
    (aka responsibilities)
    for each point $\vec{x}_i$.
    \begin{equation*}
        \gamma_j^{(t)}(\vec{x}_i) = \frac{
            w_j^{(t-1)} P(\vec{x}_i \mid \vec{\Sigma}_j^{(t-1)}, \vec{\mu}_j^{(t-1)})
        }{
            \sum_{l=1}^k{
                w_l^{(t-1)} P(\vec{x}_i \mid \vec{\Sigma}_l^{(t-1)}, \vec{\mu}_l^{(t-1)})
            }
        }
    \end{equation*}
    \item[M-step]: Fit clusters to weighted data points,
    i.e. calculate the closed form ML solution.
    \begin{align*}
        \vec{\mu}^{(t)}_j
        &= \frac{\sum_{i=1}^n{\gamma_j^{(t)}(\vec{x}_i) \vec{x}_i}}{\sum_{i=1}^n{\gamma_j^{(t)}(\vec{x}_i)}} \\
        \vec{\Sigma}^{(t)}_j &= \frac{
        \sum_{i=1}^n{
        \gamma_j^{(t)}(\vec{x}_i) (\vec{x}_i - \vec{\mu}_j^{(t)}) (\vec{x}_i - \vec{\mu}_j^{(t)})^T
        }
        }{
            \sum_{i=1}^n{\gamma_j^{(t)}(\vec{x}_i)}
        }
        \\
        w_j^{(t)} &= \frac{1}{n} \sum_{i=1}^n{\gamma_j^{(t)}(\vec{x}_i)}
    \end{align*}
\end{description}
The \emph{E} stands for \emph{Expected sufficient statistics},
the \emph{M} stands for \emph{Maximum likelihood solution}.


\subsection{Hard- vs Soft-EM}
Soft-EM generally results in higher likelihood values
as it is able to deal with overlapping clusters.
However, Hard-EM might be useful in certain scenarios.

The term "EM" alone typically refers to Soft-EM.


\subsection{Constrained GMMs}
There are special cases of GMMs by constraining the parameters:
\begin{description}
    \item[Spherical] assumes features are independent and
    have the same variance for each component.
    \begin{equation*}
        \vec{\Sigma}_j = \sigma^2_j \cdot \vec{I}_d
    \end{equation*}
    
    \item[Diagonal] assumes features are independent,
    i.e. Gaussian Naive Bayes in an unsupervised scenario.
    \begin{equation*}
        \vec{\Sigma}_j = diag(\sigma_{j,1}^2, \dotsc, \sigma_{j,d}^2)
    \end{equation*}
    
    \item[Tied] assumes all (or some) 
    covariance matrices are equal.
    \begin{equation*}
        \vec{\Sigma}_1 = \dotsb = \vec{\Sigma}_d
    \end{equation*}
    
    \item[Full] imposes no restrictions.
\end{description}

This is done by tying together parameters in the MLE.

Each constraint reduces the number of parameters and thus
acts as a form of regularisation.


\subsection{Comparison to k-means}
Assume uniform weights $w_1 = \dotsb = w_k = \frac{1}{k}$
and identical, spherical covariances
$\vec{\Sigma}_{1:k} = \sigma^2 \cdot \vec{I}_d$.

Then, the Hard-EM algorithm is equivalent to the
k-means algorithm.
TODO: Derivation?

Conversely, the k-means algorithm can be understood
as limiting case of Soft-EM for GMMs.
The assumptions are the same as above,
with additionally the variances tending to zero.
If $\sigma \to 0$, it holds that
\begin{equation*}
    \gamma_j(\vec{x}) =
    \begin{cases}
        1 & \text{if $\vec{\mu}_j$ is (unique) closest to $\vec{x}$} \\
        0 & \text{if not (and there are no ties)}
    \end{cases}
\end{equation*}

\subsection{Initialisation}
Fitting GMMs is a non-convex problem
and harder than k-means, i.e. NP-hard.
Thus, initialisation is important.

For initialisation, we typically do
\begin{description}
    \item[Weights] equally or uniformly.
    \item[Means] randomly or via k-means++.
    \item[Variances] spherical,
    e.g. according to empirical variance in the data.
\end{description}


\subsection{Model Selection}
For GMMs, the number of components $k$ needs to be selected.
Generally, the same techniques as for k-means can be used.

However, if the data actually stems from a mixture of Gaussians,
cross-validation can be used to select $k$.
We then aim to maximise the log-likelihood on the validation set.
Even if the underlying process is not a mixture of Gaussians,
cross-validation typically works fairly well
and the validation set provides a signal.


\subsection{Degeneracy}
The Soft-EM optimises a log-likelihood,
and thus might overfit.

Suppose the 1-dimensional case with a single data point $x$.
Then, the negative log-likelihood is
\begin{equation*}
    -\log{P(x \mid \mu, \sigma)} =
    \frac{1}{2}
    \underbrace{\log{(2 \pi \sigma^2)}}_\text{$\to -\infty$ if $\sigma \to 0$}
    +
    \frac{1}{2 \sigma^2}
    \underbrace{(x - \mu)^2}_\text{$0$ if $\mu = x$}
\end{equation*}
and the minimum is given by selecting $\mu = x$ and
then letting $\sigma \to 0$.
Then, the loss converges to $-\infty$.

Therefore, the optimal GMM chooses $k=n$ and
puts one Gaussian on each data point with variance
tending to $0$.
This is a case of overfitting and explains poor test set
log-likelihood.

Degeneracy (i.e. variances tending towards 0)
can be avoided by adding a small term $\nu^2$ to the
diagonal of the MLE:
\begin{equation*}
    \vec{\Sigma}^{(t)}_j = \frac{
    \sum_{i=1}^n{
    \gamma_j^{(t)}(\vec{x}_i) (\vec{x}_i - \vec{\mu}_j^{(t)}) (\vec{x}_i - \vec{\mu}_j^{(t)})^T
    }
    }{
        \sum_{i=1}^n{\gamma_j^{(t)}(\vec{x}_i)}
    }
    + \underbrace{\nu^2 \vec{I}}_\text{additional term}
\end{equation*}

From a Bayesian standpoint,
this is equivalent to placing a (conjugate) Wishart prior
on the covariance matrix,
and computing the MAP instead of MLE.

$\nu$ can be chosen using cross-validation.


\subsection{Use Cases}
Mixture models are useful because
they can encode assumptions about shape
(e.g. fit ellipses instead of points),
can be part of more complex models
(e.g. classifiers),
can output the likelihood of $P(\vec{x})$
and are naturally useful for semi-supervised
learning.

\subsubsection{Clustering}
GMMs are naturally useful for clustering.
Either one or multiple Gaussians can be
used to model clusters.

If only one Gaussian is used per cluster,
a normal GMM can be fitted.
Points are then assigned such that
$P(z \mid \vec{x}, \theta)$ is maximised.

However, assuming that each cluster has
a Gaussian shape is a strong assumption.
Thus, we can also model each class as
a collection of clusters,
i.e. each class is described as a
mixture of Gaussians.

TODO: How exactly should this work?

\subsubsection{Gaussian-Mixture Bayes Classifiers}
GMMs can be used in GBCs to estimate
$P(\vec{x} \mid y)$.

Thus, we first estimate the class prior
$P(y)$ as usual.
Then, for each class $y$,
instead of estimating
$P(\vec{x} \mid y)$ as a single Gaussian,
it is estimated as a GMM:
\begin{equation*}
    P(\vec{x} \mid y) = \sum_{j = 1}^{k_y}{
        w_j^{(y)}
        \mathcal{N}(\vec{x}; \vec{\mu}_j^{(y)}, \vec{\Sigma}_j^{(y)})
    }
\end{equation*}
Estimation is only done on the
$\vec{x}_i$ with $y_i = y$.

Prediction is then done the same way
as it is done with normal GBCs:
\begin{equation*}
    P(y \mid \vec{x}) = \frac{1}{Z}
    p(y) \sum_{j = 1}^{k_y}{
        w_j^{(y)}
        \mathcal{N}(\vec{x}; \vec{\mu}_j^{(y)}, \vec{\Sigma}_j^{(y)})
    }
\end{equation*}

\subsubsection{Density Estimation}
GMMs can also be used for density estimation.
$P(\vec{x})$ is modelled as a Gaussian mixture,
$P(y \mid \vec{x})$ as some other model
(e.g. logistic regression, ANN).

Then, the joint density is given as
\begin{equation*}
    P(\vec{x}, y) = P(\vec{x}) \cdot P(y \mid \vec{x})
\end{equation*}

This combines the robustness of
discriminative models with the
ability to detect outliers.

\subsubsection{Anomaly Detection}
Outlier detection is done by setting some
\emph{threshold} $\tau \in (0, 1)$.
Then, $\vec{x}$ is an outlier if
and only if
$P(\vec{x}) < \tau$.

Setting the decision threshold if no
examples of outliers are available
is challenging.
For example,
a possible goal is to have a constant
fraction of the probability mass
be represented as anomaly, e.g. 1\%.
We can think of anomalies as a positive class.
It is thus often called \emph{one-class classification}.

If examples are available,
cross-validation can be used
to select $\tau$.
Varying $\tau$ trades false-positives
against false-negatives.
Thus, precision-recall or ROC curves
can be used as an evaluation criterion.
F1 score and related metrics can
be used to select the number of clusters
and other hyperparameters.

\subsubsection{Semi-Supervised Learning}
Semi-supervised learning can be interpreted
as a missing-data problem where
some but not all labels are missing.
Semi-supervised learning is learning
from a large amount of unlabelled data and
a small amount of labelled data.

GMMs can easily be extended to
semi-supervised learning.

For instances $\vec{x}_i$ with known
label $y_i$, it must hold that
$\gamma_j(\vec{x}_i) = [j = y_i]$.
Thus, the EM algorithm can be modified.
In the E-step, we differentiate between
labelled and unlabelled samples.
For unlabelled samples,
we calculate the original responsibility
$\gamma_j^{(t)}(\vec{x}_i) = P(Z = j \mid \vec{x}_i, \vec{\mu}^{(t - 1)}, \vec{\Sigma}^{(t - 1)}, \vec{w}^{(t - 1)})$.
For labelled samples, we set $\gamma_j(\vec{x}_i) = [j = y_i]$.
The M-step does not change.


\subsection{Theory behind the EM Algorithm}
\subsubsection{Equivalent formulation}
The \emph{complete data log-likelihood} under $\theta$ is
\begin{equation*}
    \log{P(\vec{x}_{1:n} \mid \theta)}
\end{equation*}
or alternatively, including latent variables
\begin{equation*}
    \log{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)}
\end{equation*}
where $z_i$ describes the responsibilities
(as a random variable)
of data point $\vec{x}_i$.

The \emph{expected complete data log-likelihood}
as a function of $\theta$,
written as $Q(\theta; \Tilde{\theta})$, is
\begin{align*}
    Q(\theta; \Tilde{\theta}) &=
    \Exp_{z_{1:n}}[
        \log{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)} \mid \vec{x}_{1:n}, \Tilde{\theta}
    ] \\
    &= \sum_{z_{1:n}}{
        P(z_{1:n} \mid \vec{x}_{1:n}, \Tilde{\theta}) \log{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)}
    }
\end{align*}
$Q(\theta; \Tilde{\theta})$ is the EM objective.

We can show that the EM algorithm does the following:
In the E-step, the expected data log-likelihood
(as a function of $\theta$) $Q(\theta; \theta^{(t-1)})$
is calculated.
In the M-step, it is maximised:
\begin{equation*}
    \theta^{(t)} = \arg\max_\theta{Q(\theta; \theta^{(t-1)})}
\end{equation*}

$Q(\theta; \theta^{(t-1)})$ can be simplified:
\begin{align*}
        Q(\theta; \theta^{(t-1)}) &=
    \Exp_{z_{1:n}}\left[
        \log{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)} \mid \vec{x}_{1:n}, \theta^{(t-1)}
    \right] \\
    &\overset{iid}{=} \Exp_{z_{1:n}}\left[
        \sum_{i=1}^n{
        \log{P(\vec{x}_i, z_i \mid \theta)} \mid \vec{x}_{1:n}, \theta^{(t-1)}
        }
    \right] \\
    &= \sum_{i=1}^n{ \Exp_{z_{1:n}}\left[
        \log{P(\vec{x}_i, z_i \mid \theta)} \mid \vec{x}_{1:n}, \theta^{(t-1)}
    \right]
    } \\
    &= \sum_{i=1}^n{ \Exp_{z_i}\left[
        \log{P(\vec{x}_i, z_i \mid \theta)} \mid \vec{x}_i, \theta^{(t-1)}
    \right]
    } \\
    &= \sum_{i=1}^n{
        \sum_{j=1}^k{
            \underbrace{P(z_i = j \mid \vec{x}_i, \theta^{(t-1)})}_{\gamma_j(\vec{x}_i)}
            \cdot
            \log{\underbrace{P(\vec{x}_i, z_i = j \mid \theta)}_{w_j \mathcal{N}(\vec{x}_i; \vec{\mu}_j, \vec{\Sigma}_j)}}
        }
    }
\end{align*}

Thus, the EM objective function is equivalent to
\begin{equation*}
    Q(\theta; \theta^{(t-1)})
    = \sum_{i=1}^n{\sum_{z_i=1}^k{
        \gamma_{z_i}(\vec{x}_i)
        \log{P(\vec{x}_i, z_i \mid \theta)}
    }}
\end{equation*}

Thus, the E-step is equivalent to computing
the \emph{expected sufficient statistics}
$\gamma_z(\vec{x}_i) = P(z \mid \vec{x}_i, \theta^{(t-1)})$.

In the M-step, we compute
\begin{equation*}
    \theta^{(t)} = \arg\max_\theta{
        Q(\theta; \theta^{(t-1)})
    }
    = \arg\max_\theta{
        \sum_{i=1}^n{
        \sum_{z_i=1}^k{
            \gamma_{z_i}(\vec{x}_i)
            \log{P(\vec{x}_i, z_i \mid \theta)}
        }
        }
    }
\end{equation*}
This is equivalent to training a GBC with
weighted data.
This has a closed-form solution
(which is given in the EM algorithm pseudocode).

\subsubsection{Convergence}
Using the alternative formulation,
we can prove that the EM algorithm
monotonically increases the likelihood:
\begin{equation*}
    \log{P(\vec{x}_{1:n} \mid \theta^{(t)})}
    \geq
    \log{P(\vec{x}_{1:n} \mid \theta^{(t-1)})}
\end{equation*}

First, we rewrite
\begin{equation*}
    P(\vec{x}_{1:n} \mid \theta)
    = P(\vec{x}_{1:n} \mid \theta) \cdot \frac{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)}{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)}
    = \frac{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)}{P(z_{1:n} \mid \vec{x}_{1:n}, \theta)}
\end{equation*}

The logarithm and expectation are both
monotonous functions and therefore
do not change monotonicity.
Thus, applying both functions to
$P(\vec{x}_{1:n} \mid \theta)$ gives
\begin{align*}
    & \Exp_{z_{1:n}}\left[
        \log{P(\vec{x}_{1:n} \mid \theta)}
        \mid \vec{x}_{1:n}, \theta^{(t-1)}
    \right] \\
    &= \Exp_{z_{1:n}}\left[
        \log{\frac{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)}{P(z_{1:n} \mid \vec{x}_{1:n}, \theta)}}
        \mid \vec{x}_{1:n}, \theta^{(t-1)}
    \right] \\
    &= \Exp_{z_{1:n}}\left[
        \log{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)} - \log{P(z_{1:n} \mid \vec{x}_{1:n}, \theta)}
        \mid \vec{x}_{1:n}, \theta^{(t-1)}
    \right] \\
    &= \underbrace{
    \Exp_{z_{1:n}}\left[
        \log{P(\vec{x}_{1:n}, z_{1:n} \mid \theta)}
        \mid \vec{x}_{1:n}, \theta^{(t-1)}
    \right]
    }_{Q(\theta; \theta^{(t-1)})}
    -
    \underbrace{
    \Exp_{z_{1:n}}\left[
        \log{P(z_{1:n} \mid \vec{x}_{1:n}, \theta)}
        \mid \vec{x}_{1:n}, \theta^{(t-1)}
    \right]
    }_{f(\theta)}
\end{align*}

Therefore, for monotonicity
it suffices to show
\begin{align*}
    Q(\theta^{(t)}; \theta^{(t-1)}) &\geq Q(\theta^{(t - 1)}; \theta^{(t-1)}) \\
    f(\theta^{(t)}) &\leq f(\theta^{(t-1)})
\end{align*}

The first inequality follows directly from
the fact that in the M-step,
we take $\theta^{(t)} = \arg\max_\theta{Q(\theta; \theta^{(t-1)})}$.

It remains to show that $f(\theta^{(t)}) \leq f(\theta^{(t-1)})$,
i.e.
\begin{align*}
    & \Exp_{z_{1:n}}\left[
        \log{P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t)})}
        \mid \vec{x}_{1:n}, \theta^{(t-1)}
    \right]
    \leq
    \Exp_{z_{1:n}}\left[
        \log{P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t - 1)})}
        \mid \vec{x}_{1:n}, \theta^{(t-1)}
    \right] \\
    &\Leftrightarrow
    \Exp_{z_{1:n}}\left[
        \log{P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t - 1)})}
        -
        \log{P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t)})}
        \mid \vec{x}_{1:n}, \theta^{(t-1)}
    \right]
    \geq 0 \\
    &\Leftrightarrow
    \Exp_{z_{1:n}}\left[
        \log{
        \frac{P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t - 1)})}{P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t)})}
        }
        \mid \vec{x}_{1:n}, \theta^{(t-1)}
    \right]
    \geq 0 \\
    &\Leftrightarrow
    \sum_{z_{1:n}}{
        P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t-1)})
        \log{
        \frac{P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t - 1)})}{P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t)})}
        }
    }
    \geq 0 \\
    &\Leftrightarrow
    KL(P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t - 1)}) || P(z_{1:n} \mid \vec{x}_{1:n}, \theta^{(t)}))
    \geq 0
\end{align*}
This holds because the KL-divergence is non-negative.

The \emph{Kullback-Leibler divergence} $KL(p || q)$
for two distributions $p, q$ is defined as
\begin{equation*}
    KL(p || q) = \Exp_{\vec{x} \sim p}\left[
        \log{\frac{p(\vec{x})}{q(\vec{x})}}
    \right]
    = \begin{cases}
        \sum_{\vec{x}}{p(\vec{x}) \log{\frac{p(\vec{x})}{q(\vec{x})}}}
        & \text{discrete} \\
        \int{p(\vec{x}) \log{\frac{p(\vec{x})}{q(\vec{x})}} d\vec{x}}
        & \text{continuous}
    \end{cases}
\end{equation*}

The KL-divergence is non-negative
and in general not symmetric.

\subsubsection{Hard-EM Convergence}
TODO: Hard-EM convergence

\subsubsection{EM Algorithm more generally}
The EM algorithm is much more widely applicable.
It can be used whenever the E and M steps are
tractable,
i.e. it is possible to
\emph{compute} and \emph{maximise}
the complete data likelihood.

The EM algorithm can even be used in
the case of missing features,
not only missing labels.


\subsection{Bernoulli Mixture Models}
TODO: Bernoulli mixture models, tutorial 14


\subsection{Useful Concepts}
\subsubsection{Entropy}
For a discrete probability distribution $p$,
the \emph{entropy} is defined as
\begin{equation*}
    \mathcal{H}(p) = \sum_x{-p(x) \log{p(x)}}
\end{equation*}
The more "spread out" a distribution is,
the higher its entropy,
and the more concentrated the lower its entropy.
A uniform distribution has the highest entropy.

\subsubsection{Jensenâ€™s Inequality}
If $f$ is a convex function, \emph{Jensen's inequality} is
\begin{equation*}
    f(\Exp[X]) \leq \Exp[f(x)]
\end{equation*}
If $X$ is constant, this is equal.
If $f$ is a concave function,
the inequality is reversed.

\subsubsection{KL Divergence}
The \emph{KL-divergence} measures difference between
two distributions.
Let $p$ and $q$ be discrete probability distributions.
Then, the KL-divergence is defined as
\begin{equation*}
    KL(p || q) = \sum_x{p(x) \log{\frac{p(x)}{q(x)}}}
\end{equation*}

The KL-divergence is only defined if
for each $x$ $q(x) = 0 \Rightarrow p(x) = 0$,
and in that case is zero at that point.

It is also generally not symmetric
and becomes zero if $p = q$.

It can be shown that the KL-divergence is
always positive using Jensen's inequality:
\begin{equation*}
    KL(p || q) = -\sum_x{p(x) \log{\frac{p(x)}{q(x)}}} \geq - \log{
        \sum_x{p(x) \frac{p(x)}{q(x)}}
    } = 0
\end{equation*}