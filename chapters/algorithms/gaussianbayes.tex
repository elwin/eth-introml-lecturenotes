\section{Gaussian Bayes Classifiers}
Gaussian Bayes Classifiers (GBC) are
generalisations of GNB classifiers.

The class labels are modelled as generated
from a categorical variable:
\begin{equation*}
    P(Y = y) = p_y
\end{equation*}
with $y \in \mathcal{Y} = \{1, \dotsc, c\}$.

The features $\vec{x}$ are modelled as
generated by multivariate Gaussians:
\begin{equation*}
    P(\vec{x} \mid y)
    = \mathcal{N}(\vec{x}; \vec{\mu}_y, \vec{\Sigma}_y)
\end{equation*}
with $\vec{\mu}_y$ being the mean and
$\vec{\Sigma}_y$ the covariance matrix.


\subsection{Estimation}
Let $D = \{(\vec{x}_1, y_1), \dotsc, (\vec{x}_n, y_n)\}$ be the data set.

The MLE for the class label distribution
$\hat{P}(Y = y) = \hat{p}_y$ is given as
\begin{equation*}
    \hat{p}_y = \frac{Count(Y = y)}{n}
\end{equation*}

The MLE for the feature distribution
$\hat{P}(\vec{x} \mid y) = \mathcal{N}(\vec{x}; \vec{\mu}_y, \vec{\Sigma}_y)$
is given as
\begin{align*}
    \hat{\vec{\mu}}_y
    &= \frac{1}{Count(Y = y)}
    \sum_{i : y_i = y}{\vec{x}_i} \\
    \hat{\vec{\Sigma}}_y
    &= \frac{1}{Count(Y = y)}
    \sum_{i : y_i = y}{
        (\vec{x}_i - \hat{\vec{\mu}}_y)
        (\vec{x}_i - \hat{\vec{\mu}}_y)^T
    }
\end{align*}
i.e. the empirical (class) mean and the
empirical (class) covariance.

Note that now, for the (co)variance,
$d^2$ instead of $d$ parameters need
to be estimated.


\subsection{Discriminant Function}
Given $P(Y = +1) = p$ and
$P(\vec{x} \mid y) = \mathcal{N}(\vec{x}; \vec{\mu}_y, \vec{\Sigma}_y)$.

We want
\begin{equation*}
    f(\vec{x}) = \frac{
        P(Y = +1 \mid \vec{x})
    }{
        P(Y = -1 \mid \vec{x})
    }
\end{equation*}

For GBC, this is given by
\begin{equation*}
    f(\vec{x}) =
    \log{\frac{p}{1-p}} +
    \frac{1}{2} \left[
        \log{\frac{|\hat{\vec{\Sigma}}_-|}{|\hat{\vec{\Sigma}}_+|}}
        +
        \left(
        (\vec{x}-\hat{\vec{\mu}}_-)^T
        \hat{\vec{\Sigma}}_-^{-1}
        (\vec{x}-\hat{\vec{\mu}}_-)
        \right)
        -
        \left(
        (\vec{x}-\hat{\vec{\mu}}_+)^T
        \hat{\vec{\Sigma}}_+^{-1}
        (\vec{x}-\hat{\vec{\mu}}_+)
        \right)
    \right]
\end{equation*}
with $|\cdot|$ denoting the (matrix)
discriminant.


\subsection{Variations}
If $\vec{\Sigma}$ is a diagonal matrix,
this corresponds to a
\emph{Gaussian Naive Bayes} classifier.
The diagonal elements are the
$\sigma^2_{y, i}$.

For $c = 2$, $P(Y = +1) = P(Y = -1) = 0.5$,
$\vec{\Sigma}$ a diagonal matrix (naive),
and equal covariances (not depending on class)
$\hat{\vec{\Sigma}}_- = \hat{\vec{\Sigma}}_+ = \hat{\vec{\Sigma}}$,
this is of the same form as \emph{Logistic Regression}.
If the modelling assumptions are met,
the predictions are the same.

For $c = 2$, $P(Y = +1) = P(Y = -1) = 0.5$
and equal covariances (not depending on class)
$\hat{\vec{\Sigma}}_- = \hat{\vec{\Sigma}}_+ = \hat{\vec{\Sigma}}$,
this corresponds to
\emph{Fisher's Linear Discriminant Analysis}.

TODO: More here?

TODO: Big Picture, slides 22, p. 22

\subsubsection{Comparison to GNB Classifiers}
The conditional independence assumption of
GNB models can lead to overconfidence
where general GBC capture correlations
among features and thus avoids overconfidence.
However, GNB predictions can still be useful.

Also, GNB models are cheaper to fit:
Fitting a naive model has parameters
in $\mathcal{O}(c \cdot d)$
where a full GBC has parameters
in $\mathcal{O}(c \cdot d^2)$.
