\section{Generative Modelling}
\emph{Discriminative models} aim to estimate
the conditional distribution
\begin{equation*}
    P(y \mid \vec{x})
\end{equation*}
\emph{Generative models} aim to estimate
the joint distribution
\begin{equation*}
    P(y, \vec{x})
\end{equation*}

Because discriminative models do not attempt
to model $P(\vec{x})$,
they are unable to detect outliers
and can thus become overconfident.

A conditional distribution can always be
derived from a joint one, but not vice versa:
\begin{equation*}
    P(y \mid \vec{x})
    = \frac{P(\vec{x}, y)}{P(\vec{x})}
    = \frac{P(\vec{x}, y)}{
        \int{P(\vec{x}, y) dy}
    }
\end{equation*}

The typical approach for generative
modelling is as follows:
\begin{enumerate}
    \item Estimate prior on labels: $P(y)$
    \item Estimate conditional distribution
    $P(\vec{x} \mid y)$ for each class $y$
    \item Obtain posterior (predictive) distribution
    using Bayes' rule:
    \begin{equation*}
        P(y \mid \vec{x}) =
        \frac{1}{Z} 
        \underbrace{P(y) P(\vec{x} \mid y)}_{P(\vec{x}, y)}
    \end{equation*}
\end{enumerate}
where $Z = P(\vec{x})$ is a normalisation
constant,
i.e. $Z = \sum_y{P(y) P(\vec{x} \mid y)}$.
If only $P(y \mid \vec{x})$ is desired,
the normalisation constant can be omitted.

Thus, generative modelling attempts to
infer the process according to which
examples are generated.

\subsection{Comparison}
In generative modelling, we have a
probabilistic model of each class.
The decision boundary is where one
model becomes more likely.
It allows natural usage of unlabelled data.
Discriminative modelling focuses
on the decision boundary.
It is more powerful (in terms of prediction)
if a lot of examples are available.
It also cannot use unlabelled data.

Generative models are always better if
the model is well-specified.
Otherwise, generative modelling is
better in case of small amount of data,
discriminative modelling in
case of large amount of data.


\subsection{Conjugate Distributions}
A pair of prior distribution and likelihood function
is called \emph{conjugate} if
the posterior distribution remains in
the same family as the prior.

Conjugate priors thus allow for regularisation
with almost no additional computational cost.

\subsubsection{List of Conjugate Distributions}
TODO: List of conjugate priors, slides 22 p. 36 and tutorial.

\subsubsection{Dirichlet Prior, Multinomial Likelihood}
The \emph{Dirichlet} prior is conjugate with
respect to the \emph{Multinomial} likelihood.

Let $\vec{X} = (X_1, \dotsc, X_n)$ and
$X_i \sim Categorical(\vec{\theta})$ iid.
$\vec{\theta} = (\theta_1, \dotsc, \theta_m)$
is the vector of the probabilities for
individual values.
Furthermore, let $N_j$ denote the
the number of times $j$ occurs in $\vec{X}$
and $\vec{N} = (N_1, \dotsc, N_m)$.

Then, $\vec{N} \sim Multi(\vec{\theta}, n)$
is from a \emph{multinomial} distribution.

The probability mass function is
\begin{equation*}
    P(\vec{N} \mid n, \vec{\theta}) =
    \underbrace{\frac{n!}{\prod_{j=1}^m{N_j!}}}_\text{normalisation constant}
    \prod_{j=1}^m{\theta_j^{N_j}}
\end{equation*}

For $Multi(\vec{\theta}, 1)$ is a categorical
distribution,
$Multi((\theta, 1 - \theta), n)$ equivalent to a binomial distribution.
The multinomial distribution is thus a
generalisation of the binomial distribution.

The \emph{Dirichlet distribution} has probability density function
\begin{equation*}
    P(\vec{\theta} \mid \vec{\alpha}) =
    \underbrace{\frac{\Gamma(\sum_{j=1}^m{\alpha_j})}{\prod_{j=1}^m{\Gamma(\alpha_j)}}}_\text{normalisation constant}
    \prod_{j=1}^m{\theta_j^{\alpha_j - 1}}
\end{equation*}
with $\alpha_j > 0$.
We write $\vec{\theta} \sim Dir(\vec{\alpha})$.

If we have a Dirichlet prior $P(\vec{\theta})$
and a multinomial likelihood $P(\vec{X} \mid \vec{\theta})$,
the resulting posterior $P(\vec{\theta} \mid \vec{X}) \propto P(\vec{X} \mid \vec{\theta}) P(\vec{\theta})$
is
\begin{equation*}
    P(\vec{\theta} \mid \vec{X}) = Dir(\vec{N} + \vec{\alpha})
\end{equation*}
Thus, $\vec{\alpha}$ act as pseudo-counts,
similar to the beta prior and binomial likelihood.

The Dirichlet prior is also conjugate with respect
to the Categorical likelihood.

The MLE of the multinomial likelihood is
\begin{equation*}
    \vec{\theta}_j^* = \frac{N_j}{n}
\end{equation*}
and the MAP estimate with a Dirichlet prior is
\begin{equation*}
    \vec{\theta}_j^* = \frac{N_j + \alpha_j - 1}{n + \sum_{j' = 1}^m{(\alpha_{j'} - 1)}}
\end{equation*}
