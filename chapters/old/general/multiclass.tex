\section{Multi-Class Classification}
In this section, let $y_i \in \mathcal{Y} = \{1, \dotsc, c\}$
and $D$ be the data set.
In a multi-class classification scenario,
we want a function $f : \mathcal{X} \to \mathcal{Y}$.

In one-vs-all (OVA) / one-vs-rest (OVR) classification,
$c$ binary classifiers $f^{(i)}$ are trained,
one for each class.
The positive examples are those from class $c$,
negative examples those from all other classes.

The prediction is then
\begin{equation*}
    \hat{y} = \arg\max_i f^{(i)}(\vec{x})
\end{equation*}
i.e. take the classifier with the highest confidence.

The confidence of a classification is given as
$|f^{(i)}(\vec{x})|$.
For any $\alpha > 0$, multiplying it to the function output
does not change the classification result, but the confidence.
One solution for a consistent confidence measure would thus be
to normalise the weights,
i.e. $\vec{w} \gets \frac{\vec{w}}{\norm{\vec{w}}_2}$.
However, in practice, if regularisation is used,
the magnitude $\norm{\vec{w}}_2$ is generally under control.

If unit magnitude for all weights is assumed,
we can consider the euclidian distance from a point
to each decision boundary.
Thus, in all overlapped areas, the decision boundaries
are separated by equal angles.

One-vs-all classification only works if classifiers produce
confidences of similar magnitude.
Furthermore, individual classifiers almost always see
a very imbalanced data set.
Also, some class might not be linearly separable from the
others, leading to failure.

In one-vs-one (OVO) classification,
$\frac{c (c - 1)}{2}$ classifiers are trained;
one for each distinct pair of classes $(i, j)$.
The positive samples are those from class $i$,
negative ones from class $j$.

The prediction is done using a voting scheme.
The class with the highest number of positive predictions wins.
However, ties may happen and need to be broken somehow.

The advantage of OVA is that it is faster as fewer classifiers
need to be trained. However, it requires confidence and leads
to class imbalance.
OVO does not have confidence or imbalance issues,
but is slower to train.

The alternatives are using other encodings or
multi-class models.
