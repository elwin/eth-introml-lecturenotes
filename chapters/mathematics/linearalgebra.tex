\section{Linear Algebra}

Let $V$ be a $n$-dimensional vector space
with an inner product $\langle\cdot,\cdot\rangle : V \times V \to \mathbb{R}$
and an ordered basis $B = (b_1, \dotsc, b_n)$ of $V$.
Then, $\langle x, y \rangle = \hat{x}^T A \hat{y}$ where
$A_{ij} := \langle b_i, b_j \rangle$ and $\hat{x}, \hat{y}$ are the coordinates
of $x$ and $y$ with respect to $B$.
Therefore, the inner product is uniquely determined through $A$.

A symmetric matrix $A \in \mathbb{R}^{n \times n}$ which satisfies
\begin{equation*}
    \forall x \in V \setminus \{0\} : x^T A x > 0
\end{equation*}
is called a symmetric, positive definite matrix.
If only $\geq$ holds, it is called symmetric, positive semi definite.
If $A$ is symmetric, positive definite, it defines an inner product $\langle x, y \rangle = \hat{x}^T A \hat{y}$
with respect to some ordered basis $B$.

The following are equivalent conditions.
Let $A$ be a real symmetric matrix.
\begin{itemize}
    \item $A$ is p.s.d $\Longleftrightarrow$ $x^T Ax \geq 0$ for all $x$.
    \item $A$ is p.d. $\Longleftrightarrow$ $x^T Ax > 0$ for all $x \neq 0$.
    \item $A$ can be decomposed as $A = L L^T$.
\end{itemize}

Thus, $\langle\cdot,\cdot\rangle : V \times V \to \mathbb{R}$ is an inner product if and only if
there exists a symmetric, positive definite matrix $A \in \mathbb{R}^{n \times n}$ so that
$\langle x, y \rangle = \hat{x}^T A \hat{y}$.

The induced norm ($\norm{x} = \langle x, x \rangle$) satisfies the Cauchy-Schwarz inequality:
$| \langle x, y \rangle | \leq \norm{x} \norm{y}$.

In an inner product space, the angle $\omega$ between two vectors is defined as
$\cos{\omega} = \frac{| \langle x, y \rangle |}{\norm{x} \norm{y}}$.

A square matrix $A$ is orthogonal if and only if its columns are orthonormal, i.e.
$A A^T = I = A^T A$ (thus $A^{-1} = A^T$).
Transformations by orthogonal matrices preserve distances and angles.

TODO: Orthogonal complements.

An inner product between two functions $u, v : \mathbb{R} \to \mathbb{R}$ can be defined as the integral
$\langle u, v \rangle := \int_a^b{u(x) v(x) dx}$ for lower and upper limits $a, b < \infty$ respectively.
Inner products on functions may diverge, i.e. have infinite value.

Let $V$ be a vector space and $U \subseteq V$ a subspace.
A linear mapping $\pi : U \to V$ is a projection if $\pi^2 = \pi \circ \pi = \pi$.
The corresponding matrix $P_\pi$ is called a projection matrix and $P_\pi^2 = P_\pi$.

$x$ is projected orthogonal onto a line defined by vector $b$ as
$\pi_U = \lambda b = \frac{\langle x, b \rangle}{\langle b, b \rangle} b = \frac{\langle x, b \rangle}{\norm{b}^2} b$
and $\norm{\pi_U} = \norm{\lambda b} = |\lambda| \norm{b}$.
The projection matrix is given as
$P_\pi = \frac{b b^T}{\norm{b}^2}$.

In the multi-dimensional case, we have $\pi_U(x) = B \lambda$ ($\lambda$ a vector, $B$ a basis)
from which the normal equation follows:
$B^T B \lambda = B^T x$.
The resulting $\lambda$ is then $\lambda = (B^T B)^{-1} B^T x$.
Ultimately, this results in the projection
$\pi_U(x) = B (B^T B)^{-1}B^T x$ and projection matrix
$P_\pi = B (B^T B)^{-1} B^T$.

A projection into an affine subspace $L = x_0 + U$ is done as
$\pi_L(x) = x_0 + \pi_U(x - x_0)$.
This corresponds to shifting the subspace and vector, linearly projecting, and shifting back.

The trace of a diagonal matrix $A$ is defined as $tr(A) = \sum_{i=1}^n{A_{ii}}$.
The trace is invariant to cyclic permutations, i.e. $tr(AKL) = tr(KLA)$.

The characteristic polynomial is defined as
\begin{equation*}
    p_A(\lambda) = det(A - \lambda I) = c_0 + c_1 \lambda + \dotsb + c_{n-1} \lambda^{n-1} + (-1)^n \lambda^n
\end{equation*}
with $c_0 = det(A)$ and $c_{n-1} = (-1)^{n-1} tr(A)$.

All vectors collinear to an eigenvector $x$ are also eigenvectors.
TODO: Eigenspaces.

Let $A$ be a square matrix and $\lambda_i$ be an eigenvalue of $A$.
The algebraic multiplicity of $\lambda_i$ is the number of occurrences of $\lambda_i$ as a root in
the characteristic polynomial $det(A - \lambda I)$.
The geometric multiplicity of $\lambda_i$ is the dimensionality of the eigenspace $E_{\lambda_i}$.
The geometric multiplicity must always be at least one and can not exceed the algebraic multiplicity.

Let $A \in \mathbb{R}^{m \times n}$.
Then $S = A^T A$ is always symmetric, positive semi-definite. If $rk(A) = n$ then $S$ is positive definite.

The determinant of a matrix is the product of its eigenvalues, and the trace is the sum of its eigenvalues.

The Cholesky Decomposition of a symmetrical, positive definite matrix $A$ is given as
$A = L L^T$ where $L$ is a lower triangular matrix with positive diagonal elements.
$L$ is the (unique) Cholesky factor of $A$.
We have $det(A) = det(L) det(L^T) = det(L)^2$.
Because $L$ is a lower triangular matrix, this simplifies the calculation of $det(A)$.
The diagonals of $L$ are calculated as
$l_{ii} = \sqrt{a_{ii} - \sum_{j = 1}^{i - 1}{l_{ij}^2}}$.
The elements below the diagonal are also calculated in a recursive way.

Eigendecomposition:
A square matrix $A \in \mathbb{R}^{n \times n}$ can be factored into
$A = P D P^{-1}$
if and only if the eigenvectors of $A$ form a basis of $\mathbb{R}^n$.
Then, $D$ is a diagonal matrix whose diagonal entries
are the eigenvalues of $A$ and the columns of $P$
are the corresponding eigenvectors.

A symmetric matrix $S$ can always be diagonalised and all eigenvalues are real.
Furthermore, there exists an ONB of eigenvectors of $\mathbb{R}^n$, resulting in the diagnoalisation $D = P^T A P$.

Let $A \in \mathbb{R}^{m \times n}$ be a matrix of rank $r \in [0, \min\{m, n\}]$.
The SVD of $A$ is a decomposition of the form
\begin{equation*}
    A = U \Sigma V^T
\end{equation*}
with $U \in \mathbb{R}^{m \times m}$, $V \in \mathbb{R}^{n \times n}$ and $\Sigma \in \mathbb{R}^{m \times n}$.
Furthermore, $\Sigma_{ii} \geq 0$ and $\Sigma_{ij} = 0, i \neq j$.
The diagonal entries of $\Sigma$, $\sigma_i, i = 1, \dotsc, r$, are called the singular values.
By convention, the singular values are ordered in descending order.

The SVD can be interpreted as a mapping $\Phi : \mathbb{R}^n \to \mathbb{R}^m$.
First, $V^T$ performs a basis change by rotation in the source domain.
Then, $\Sigma$ scales coordinates in the new basis by the singular values and adds/deletes dimensions.
Finally, $U$ performs another basis change by rotation in the codomain.

For symmetric, positive definite matrices $S = S^T = P D P^T$, its SVD is equal to its eigendecomposition.

The singular value equation is given as $A v_i = \sigma_i u_i, i = 1, \dotsc, r$.

The SVD of a matrix $A$ is constructed as follows:

\begin{enumerate}
    \item Calculate the eigendecomposition $A^T A = P D P^T$ which always exists.
    $P$ forms an ONB of eigenvectors.
    \item $V^T = P^T$.
    \item The eigenvalues of $A^T A$ are the squared singular values of $A$,
    $\lambda_i = \sigma_i^2$, thus $\sigma_i = \sqrt{\lambda_i}$.
    All other values are set to $0$.
    \item Calculate $u_i = \frac{1}{\sigma_i} A v_i, i = 1, \dotsc, r$
    and collect them into the matrix $U$.
    \item If $m > r$ then $U$ has to be extended to an ONB with arbitrary vectors.
\end{enumerate}

Let $\vec{x} \in \mathbb{R}^n$ be a vector and $p \in (1, \infty)$.
The $p$-norm of $\vec{x}$ is
\begin{equation*}
    \norm{\vec{x}}_p = (\sum_{i = 1}^n{|x_i|^p})^\frac{1}{p}
\end{equation*}.

The spectral norm of a matrix $A$ is defined as
\begin{equation*}
    \norm{A}_2 := \max_{x \neq 0}{\frac{\norm{Ax}_2}{\norm{x}_2}} = \max_{\norm{x}_2 = 1}{\norm{Ax}_2}
\end{equation*}
and the general operator norm as
\begin{equation*}
    \norm{A}_{(p,q)} :=
    \sup_{x \neq 0}{\frac{\norm{Ax}_q}{\norm{x}_p}} = \sup_{\norm{x}_p = 1}{\norm{Ax}_q}
\end{equation*}
The spectral norm of a matrix $A$ is equal to its largest singular value $\sigma_1$.

A rank $r$ matrix $A$ with SVD $A = U \Sigma V^T$ can be written as a sum of rank 1 matrices
\begin{equation*}
    A = \sum_{i=1}^r{\sigma_i u_i v_i^T} = \sum_{i=1}^r{\sigma_i A_i}
\end{equation*}
If we only sum up to $k \leq r$, this is called the rank-$k$-approximation of $A$
\begin{equation*}
    \hat{A}(k) = \sum_{i=1}^k{\sigma_i u_i v_i^T} = \sum_{i=1}^k{\sigma_i A_i}
\end{equation*}
with $rk(\hat{A}) = k$.

Let $A \in \mathbb{R}^{m \times n}$ of rank $r$ and
$B \in \mathbb{R}^{m \times n}$ of rank $k$.
Then, for any $k \leq r$,
$\hat{A}(k) = \arg\min_{rk(B) = k}\norm{A - B}_2$ and
$\norm{A - \hat{A}}_2 = \sigma_{k + 1}$.
Thus, the SVD is the projection which minimises the error with respect to the spectral norm
between $A$ and any rank-$k$ approximation.
