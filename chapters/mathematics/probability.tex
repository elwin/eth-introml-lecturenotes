\section{Probability}

\subsection{General}
$\Omega$ is the sample space,
$\mathcal{A} \subseteq 2^{\Omega}$ the event space and
$P(A), A \in \mathcal{A}$ a probability.
$(\Omega, \mathcal{A}, P)$ is a probability space.

$\mathcal{T}$ is the target space and the elements are states.
$X : \Omega \to \mathcal{T}$ is a random variable.
For any subset $S \subseteq \mathcal{T}$,
$P_X(S) \in [0, 1]$ is the probability of a specific event corresponding
to the random variable $X$.
Let $X^{-1}(S) = \{ \omega \in \Omega \mid X(\omega) \in S \}$.
Then, $P_X(S) = P(X \in S) = P(X^{-1}(S))$.
$P_X$ (equivalent to $P \circ X^{-1}$) is the distribution of
random variable $X$.

For a discrete random variable, the probability mass function is
$P(X = x)$.
For a continuous random variable, usually $P(a \leq X \leq b)$ is used.
$P(X \leq x)$ is the cumulative distribution function.

A univariate distribution consists of a single random variable,
multivariate distributions of more than one.
Multivariate distributions usually consider a vector of random variables
with states $\vec{x}$.

For discrete target spaces, the joint probability is
$P(X = x_i, Y = y_j) = P(X = x_i \cap Y = y_j)$.
It is usually denoted as $p(x, y)$.
The marginal probability is written as $p(x)$.
The distribution of $X$ is denoted as $X \sim p(x)$.
The conditional distribution for $Y = y$ is written as $p(y | x)$.
Discrete probability distributions are used to model
categorical variables, i.e. variables that take a finite set of values.
Discrete distributions are also used to construct probabilistic models
that combine a finite number of continuous distributions.
In general, the mass function is denoted as $P(X = x)$ and the density
functions as $p(x)$.

For continuous target spaces,
random variables in $\R^D$ are considered to be
real-valued random vectors.

A probability density function $f : \R^D \to \R$ holds
\begin{itemize}
    \item $\forall \vec{x} \in \R^D : f(\vec{x}) \geq 0$
    \item $\int_{\R^D}{f(\vec{x}) d\vec{x}} = 1$
\end{itemize}
A random variable $X$ is associated as
$P(a \leq X \leq b) = \int_a^b{f(x) dx}$.
States $\vec{x} \in \R^D$ are considered analogously by
considering a vector of $x \in \R$.

A cumulative distribution function of a multivariate real-valued
random variable $X$ with states $\vec{x} \in \R^D$ is
given by
$F_X(\vec{x}) = P(X_1 \leq x_1, \dotsc, X_D \leq x_D)$,
where $X = [X_1, \dotsc, X_D]^T$ and $\vec{x} = [x_1, \dotsc, x_D]^T$.
The cdf can be expressed as an integral over the pdf:
$F_X(\vec{x}) = \int_{-\infty}^{x_1}{\dots \int_{-\infty}^{x_D}{f(z_1, \dotsc, z_D) dz_1 \dots dz_D}}$.

The nomenclature is summarised as follows:
Let $x \in \mathcal{T}$. $p(x)$ denotes the probability of outcome $x$.
For discrete variables $X$, this is written as $P(X = x)$ and
considered the distribution.
For continuous variables, $p(x)$ is called the probability density function.
The distribution is considered the cumulative density function $P(X \leq x)$ (but also sometimes the pdf).

The characteristic function for a random vector $X \in \R^d$ is defined as
\begin{equation*}
    \varphi_X(\vec{t}) = \Exp[\exp{(i \vec{t}^T X)}] \text{ for all $\vec{t} \in \R^d$}
\end{equation*}
and uniquely identifies a distribution.

The joint density can also be defined as
\begin{equation*}
    f_{X, Y}(x, y) =
        \frac{\partial^2 F}{\partial x \partial y}(x, y)
\end{equation*}.

Let $\mathcal{Y}$ be the target space of a random variable $Y$.
The sum rule/marginalisation property states that
\begin{equation*}
    p(\vec{x}) =
    \begin{cases}
        \sum_{\vec{y} \in \mathcal{Y}}{p(\vec{x}, \vec{y})} & \text{if $\vec{y}$ is discrete} \\
        \int_\mathcal{Y}{p(\vec{x}, \vec{y}) d\vec{y}} & \text{if $\vec{y}$ is continuous}
    \end{cases}
\end{equation*}
This generalises to multiple random variables/vectors by integrating
away all non-needed random variables.

The product rule states that
\begin{equation*}
    p(\vec{x}, \vec{y})
    = p(\vec{y} | \vec{x}) p(\vec{x})
    = p(\vec{x} | \vec{y}) p(\vec{y})
\end{equation*}.
This is expressed in terms of the pmf (discrete) and pdf (continuous).

Let $A_1, \dotsc, A_n$ be arbitrary events.
The chain rule specifies that
\begin{equation*}
    p(A_1, \dotsc, A_n)
    = \prod_{i=1}^n{p(A_i | A_1, A_2, \dotsc, A_{i - 1})}
\end{equation*}.

Let $X, Y$ be random variables with joint density $f(x, y)$.
For $\epsilon > 0$ and $y \in \R, A \subseteq \R$, we have
\begin{equation*}
    P(X \in A | Y = y) = \int{\int_A{\frac{f(x, y)}{f_Y(y)}dx}dy}
\end{equation*}
and the conditional distribution for $x \in \R$
\begin{equation*}
    f_{X | Y}(x | y) = \frac{f(x, y)}{f_Y(y)}
\end{equation*}.


\subsection{Bayesian Statistics}
A latent variable is a random variable which is not observed.

Assume we have \emph{prior} knowledge $p(\vec{x})$
over some latent variable $\vec{x}$,
and some observed relationship $p(\vec{y} | \vec{x})$ between
$\vec{x}$ and some other random variable $\vec{y}$.
If we observe $\vec{y}$ we can apply Bayes' theorem
\begin{equation*}
    \underbrace{p(\vec{x} | \vec{y})}_\text{posterior} = \frac{
        \overbrace{p(\vec{y} | \vec{x})}^\text{likelihood}
        \overbrace{p(\vec{x})}^\text{prior}
    }{
        \underbrace{p(\vec{y})}_\text{evidence}
    }
\end{equation*}

One can say
\emph{posterior is proportional to likelihood times prior} or
mathematically $p(\vec{x} | \vec{y}) \propto p(\vec{y} | \vec{x}) p(\vec{x})$.

The prior captures subjective prior knowledge about the latent
variable $\vec{x}$ before seeing any data.
The likelihood is the probability of data $\vec{y}$ given
$\vec{x}$. It is a distribution in $\vec{y}$.
The posterior expresses what we know about $\vec{x}$ after
having observed $\vec{y}$.

The evidence/marginal likelihood is given by
\begin{equation*}
    p(\vec{y})
    = \int{p(\vec{y} | \vec{x}) p(\vec{x}) d\vec{x}}
    = \Exp_X[p(\vec{y} | \vec{x})]
\end{equation*}
and it normalises the posterior.


\subsection{Summary Statistics and Independence}
The expected value of a function $g: \R \to \R$
of a univariate continuous random variable $X \sim p(x)$ is
given by
\begin{equation*}
    \Exp_X[g(x)] = \int_\mathcal{X}{g(x) p(x) dx}
\end{equation*}
and in the discrete case by
\begin{equation*}
    \Exp_X[g(x)] = \sum_{x \in \mathcal{X}}{g(x) p(x)}
\end{equation*}
where $\mathcal{X}$ is the target space of $X$.

For multivariate random variables, the expected value is defined
element-wise. For $X = [X_1, \dotsc, X_D]$, it is
\begin{equation*}
    \Exp_X[g(\vec{x})] =
    \begin{bmatrix}
        \Exp_{X_1}[g(X_1)] \\
        \vdots \\
        \Exp_{X_D}[g(X_D)]
    \end{bmatrix}
    \in \R^D
\end{equation*}.
The expected value is a linear operator.

TODO: Weak and strong law of large numbers.

TODO: Central limit theorem.

The mean of a random variable with states $\vec{x}$ is defined as
$\Exp_X[\vec{x}] \in \R^D$,
i.e. the expected value where $g(x)$ is the identity function.

In the one dimensional case,
the median is the value in the middle of the sorted values in the
discrete case, and the value with cdf $0.5$ in the continuous case.

The mode is the most frequently occurring value.
In the discrete case, it is the $\arg\max$ of the pmf.
A discrete distribution can have more than one node if
multiple elements occur equally (maximally) often.
In the continuous case, it is a peak in the density $p(\vec{x})$,
i.e. a $\vec{x}$ for which $p(\vec{x})$ is a local maximum.
Continuous distributions may have more than one mode.

If the associated random variables are clear from the
context, the subscripts in the expectation,
covariance and variance are usually dropped.

The covariance between two univariate random variables $X, Y$ is
\begin{equation*}
    \Cov_{X, Y}[x, y] :=
        \Exp_{X, Y}[(x - \Exp_X[x]) (y - \Exp_Y[y])]
\end{equation*}
or transformed
\begin{equation*}
    \Cov[x, y] = \Exp[xy] - \Exp[x]\Exp[y]
\end{equation*}.

For multivariate random variables
$X \in \R^D$ and $Y \in \R^E$
with states $\vec{x}$ and $\vec{y}$ respectively,
the covariance is defined as
\begin{equation*}
    \Cov[\vec{x}, \vec{y}]
    = \Exp[\vec{x}\vec{y}^T] - \Exp[\vec{x}]\Exp[\vec{y}]^T
    = \Cov[\vec{y}, \vec{x}]^T
    \in \R^{D \times E}
\end{equation*}.

The variance of a random variable $X$ in the univariate case is
\begin{equation*}
    \Var_X[x] := \Cov[x, x] = \Exp_X[(x - \mu)^2]
    = \Exp_X[x^2] - (\Exp_X[x])^2
\end{equation*}
where $\mu = \Exp_X[x]$,
and in the multivariate case
\begin{equation*}
    \Var_X[\vec{x}] = \Cov_X[\vec{x}, \vec{x}]
    = \Exp_X[(\vec{x} - \vec{\mu })(\vec{x} - \vec{\mu })^T]
    = \Exp_X[\vec{x}\vec{x}^T] - \Exp_X[\vec{x}]\Exp_X[\vec{x}]^T
\end{equation*}
where $\vec{x} \in \R^D$ and $\vec{\mu} \in \R^D$
is the mean vector.

In the multivariate case, $\Var[\vec{x}]$ is a $D \times D$ matrix,
the so-called covariance matrix $S$ with
$S_{ij} = \Cov[x_i, x_j]$.
It is symmetric and positive (semi)definite.
The diagonal contains the variances of the marginals.
The off-diagonal entries are called cross-covariance terms.

The correlation between two random variables $X, Y$ is the normalised
covariance, defined as
\begin{equation*}
    corr[x, y] = \frac{\Cov[x, y]}{\sqrt{\Var[x]\Var[y]}}
    \in [-1, 1].
\end{equation*}

The standard deviation is defined as
$\sigma(x) = \sqrt{\Var[x]}$.

These definitions are also called population mean and population
covariance because they refer to the true statistics for the population.

Let $N$ be the sample size, $X_1, \dotsc, X_N$ i.i.d random variables
and $x_1, \dotsc, x_N$ realisations.

The empirical mean is defined as
\begin{equation*}
    \bar{\vec{x}} := \frac{1}{N}\sum_{n = 1}^N{\vec{x}_n}
    \in \R^D
\end{equation*}
and the empirical covariance matrix as
\begin{equation*}
    \vec{\Sigma} :=
    \frac{1}{N} \sum_{n = 1}^N{
        (\vec{x}_n - \bar{\vec{x}})(\vec{x}_n - \bar{\vec{x}})^T
    }
    \in \R^{D \times D}
\end{equation*}.
Empirical covariance matrices are symmetric, positive semi-definite.

Let $X, Y$ be two random variables with states
$\vec{x}, \vec{y} \in \R^D$.
Then:
\begin{align*}
    \Exp[\vec{x} + \vec{y}] &= \Exp[\vec{x}] + \Exp[\vec{y}] \\
    \Exp[\vec{x} - \vec{y}] &= \Exp[\vec{x}] - \Exp[\vec{y}] \\
    \Var[\vec{x} + \vec{y}] &= \Var[\vec{x}] + \Var[\vec{y}] + \Cov[\vec{x}, \vec{y}] + \Cov[\vec{y}, \vec{x}] \\
    \Var[\vec{x} - \vec{y}] &= \Var[\vec{x}] + \Var[\vec{y}] - \Cov[\vec{x}, \vec{y}] - \Cov[\vec{y}, \vec{x}]
\end{align*}

Let $X$ be a random variable with mean $\vec{\mu}$ and
covariance matrix $\vec{\Sigma}$ and a deterministic affine
transform $\vec{y} = \vec{A} \vec{x} + \vec{b}$ of $\vec{x}$.
Then $\vec{y}$ is also a random variable with
\begin{align*}
    \Exp_Y[\vec{Y}] &=
        \Exp_X[\vec{A} \vec{x} + \vec{b}] =
        \vec{A} \Exp_X[\vec{x}] + \vec{b} =
        \vec{A} \vec{\mu} + \vec{b} \\
    \Var_Y[\vec{Y}] &=
        \Var_X[\vec{A} \vec{x} + \vec{b}] =
        \Var_X[\vec{A} \vec{x}] =
        \vec{A} \Var_X[\vec{x}] \vec{A}^T =
        \vec{A} \vec{\Sigma} \vec{A}^T
\end{align*}
and
\begin{align*}
    \Cov[\vec{x}, \vec{y}] &=
        \Exp[\vec{x}(\vec{A} \vec{x} + \vec{b})^T]
        - \Exp[\vec{x}]\Exp[\vec{A} \vec{x} + \vec{b}]^T \\
    &= \Exp[\vec{x}]\vec{b}^T + \Exp[\vec{x}\vec{x}^T]\vec{A}^T - \vec{\mu}\vec{b}^T - \vec{\mu}\vec{\mu}^T\vec{A}^T \\
    &= \vec{\mu}\vec{b}^T - \vec{\mu}\vec{b}^T + (\Exp[\vec{x}\vec{x}^T] - \vec{\mu}\vec{\mu}^T)\vec{A}^T \\
    &= \vec{\Sigma}\vec{A}^T
\end{align*}

Two random variables are statistically independent,
$X \independent Y$ (or $X \independent Y | \emptyset$),
if and only if
$p(\vec{x}, \vec{y}) = p(\vec{x}) p(\vec{y})$.

Two random variables $X, Y$ are conditionally independent given $Z$,
$X \independent Y | Z$,
if and only if
\begin{equation*}
    p(\vec{x}, \vec{y} | \vec{z}) = p(\vec{x} | \vec{z}) p(\vec{y} | \vec{z}) \text{ for all $z \in \mathcal{Z}$}
\end{equation*}.
By the product rule, it holds that
\begin{equation*}
    p(\vec{x}, \vec{y} | \vec{z})
    = p(\vec{x} | \vec{y}, \vec{z}) p(\vec{y} | \vec{z})
\end{equation*}
and thus conditional independence also holds if
\begin{equation*}
    p(\vec{x} | \vec{y}, \vec{z}) = p(\vec{x} | \vec{z})
\end{equation*}.

TODO: Inner products of random variables
For uncorrelated random variables with zero mean,
the covariance serves as inner product.


\subsection{Gaussian Distribution}
The density of a univariate normal distribution is given by
\begin{equation*}
    p(x | \mu, \sigma^2)
    = \frac{1}{\sqrt{2 \pi \sigma^2}}
    \exp\left(-\frac{(x - \mu)^2}{2 \sigma^2}\right)
\end{equation*}.

The density of a multivariate normal distribution is given by
\begin{equation*}
    p(\vec{x} | \vec{\mu}, \vec{\Sigma})
    = (2 \pi)^{-\frac{D}{2}} |\vec{\Sigma}|^{-\frac{1}{2}}
    \exp\left( -\frac{1}{2} (\vec{x} - \vec{\mu})^T \vec{\Sigma}^{-1} (\vec{x} - \vec{\mu}) \right)
\end{equation*}
where $|\Sigma|$ denotes the determinant of $\Sigma$.
We write $p(\vec{x}) = \mathcal{N}(\vec{x} | \vec{\mu}, \vec{\Sigma})$
or $X \sim \mathcal{N}(\vec{\mu}, \vec{\Sigma})$.

TODO: Marginals and conditionals of Gaussians are Gaussians

The product of two Gaussians
$\mathcal{N}(\vec{x} | \vec{a}, \vec{A})$
$\mathcal{N}(\vec{x} | \vec{b}, \vec{B})$ is a Gaussian
$c \mathcal{N}(\vec{x} | \vec{c}, \vec{C})$ with
\begin{align*}
    \vec{C} &= (\vec{A}^{-1} + \vec{B}^{-1})^{-1} \\
    \vec{c} &= \vec{C} (\vec{A}^{-1}\vec{a} + \vec{B}^{-1}\vec{b}) \\
    c &= (2 \pi)^{-\frac{D}{2}} |\vec{A} + \vec{B}|^{-\frac{1}{2}}
    \exp\left( -\frac{1}{2} (\vec{a} - \vec{b})^T (\vec{A} + \vec{B})^{-1} (\vec{a} - \vec{b}) \right)
\end{align*}.
$c$ can be written as
$c = \mathcal{N}(\vec{a} | \vec{b}, \vec{A} + \vec{B}) = \mathcal{N}(\vec{b} | \vec{a}, \vec{A} + \vec{B})$
(here, $a$ and $b$ are not random variables, thus the notation
is considered the functional form of a Gaussian density).

If $X, Y$ are independent Gaussian variables, then $\vec{x} + \vec{y}$
is also Gaussian distributed, given by
\begin{equation*}
    p(\vec{x} + \vec{y})
    = \mathcal{N}(\vec{\mu}_x + \vec{\mu}_y, \vec{\Sigma}_x + \vec{\Sigma}_y)
\end{equation*}
and
\begin{equation*}
    p(a\vec{x} + b\vec{y})
    = \mathcal{N}(a\vec{\mu}_x + b\vec{\mu}_y, a^2\vec{\Sigma}_x + b^2\vec{\Sigma}_y)
\end{equation*}.

Let $p(x) = \alpha p_1(x) + (1 - \alpha)p_2(x)$ be a weighted
mixture of univariate Gaussian densities $p_1(x), p_2(x)$.
Then
\begin{align*}
    \Exp[x] &= \alpha \mu_1 + (1 - \alpha) \mu_2 \\
    \Var[x] &= [\alpha \sigma_1^2 + (1 - \alpha) \sigma_2^2] +
    ([\alpha \mu_1^2 + (1 - \alpha) \mu_2^2] - [\alpha \mu_1 + (1 - \alpha) \mu_2]^2)
\end{align*}.
The mean of a mixture is the weighted sum of the means of each component.
The variance of a mixture is the mean of the conditional variance and
the variance of the conditional mean.

Let $X \sim \mathcal{N}(\vec{\mu}, \vec{\Sigma})$, $\vec{A}$ a matrix
and $\vec{y} = \vec{A}\vec{x} + \vec{b}$.
Then
\begin{align*}
    \Exp[\vec{y}] &= \vec{A} \vec{\mu} \\
    \Var[\vec{y}] &= \vec{A} \Var[\vec{x}] \vec{A}^T = \vec{A} \vec{\Sigma} \vec{A} \\
    p(\vec{y}) &= \mathcal{N}(\vec{y} | \vec{A}\vec{\mu}, \vec{A} \vec{\Sigma} \vec{A}^T)
\end{align*}.

TODO: Reverse transformation.

Assume we can sample from a $\mathcal{N}(\vec{0}, \vec{I})$ distribution.
To sample from a multivariate normal $\mathcal{N}(\vec{\mu}, \vec{\Sigma})$,
we can perform a linear transformation:
Use the Cholesky decomposition to get $\vec{\Sigma} = \vec{A}\vec{A^T}$
and then use $\vec{y} = \vec{A}\vec{x} + \vec{\mu}$.


\subsection{Conjugacy and the Exponential Family}
The Bernoulli distribution is for a single binary random variable,
parameterised by $\mu \in [0, 1]$.
\begin{align*}
    p(x | \mu) &= \mu^x (1 - \mu)^{1 - x}, x \in \{0, 1\} \\
    \Exp[x] &= \mu \\
    \Var[x] &= \mu (1 - \mu)
\end{align*}

The Binomial distribution generalises the Bernoulli distribution over
integers. It is written as $Bin(N, \mu)$ where
$\mu \in [0, 1]$ and $N \in \N$.
\begin{align*}
    p(m | N, \mu) &= \binom{N}{m} \mu^m (1 - \mu)^{N - m} \\
    \Exp[m] &= N \mu \\
    \Var[m] &= N \mu (1 - \mu)
\end{align*}

The Beta distribution is a distribution over a continuous random variable
$\mu \in [0, 1]$, which is often used to represent the probability for
some binary event, e.g. the parameter of a Bernoulli distribution.
It is written $Beta(\alpha, \beta)$ with $\alpha > 0$, $\beta > 0$.
\begin{align*}
    p(\mu | \alpha, \beta) &=
        \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}
        \mu^{\alpha - 1} (1 - \mu)^{\beta - 1} \\
    \Exp[\mu] &= \frac{\alpha}{\alpha + \beta} \\
    \Var[\mu] &= \frac{\alpha \beta}{
        (\alpha + \beta)^2 (\alpha + \beta + 1)
    }
\end{align*}
The fraction of Gamma functions normalises the pdf.
The Gamma function is given as
\begin{align*}
    \Gamma(t) &:= \int_0^\infty{x^{t-1} \exp(-x) dx}, t > 0 \\
    \Gamma(t + 1) &= t \Gamma(t)
\end{align*}
and can be seen as a generalisation of the factorial.
$\alpha$ moves mass towards $1$ and $\beta$ towards $0$.
Some special cases are
\begin{itemize}
    \item $\alpha = 1, \beta = 1$, then we obtain the uniform distribution
        $\mathcal{U}[0, 1]$.
    \item $\alpha, \beta < 1$, then we get a bimodal distribution with
        peaks at $0$ and $1$.
    \item $\alpha, \beta > 1$, then the distribution is unimodal.
    \item $\alpha, \beta > 1$ and $\alpha = \beta$, then the distribution
        is unimodal, symmetric, and cetred in $[0, 1]$, i.e. has
        mode/mean at $\frac{1}{2}$.
\end{itemize}

TODO: Can variable transformations be used to change the interval?

Selecting a prior is often difficult because it is hard to express
prior knowledge about the problem and computing the posterior distribution
might not be possible analytically.
A prior is conjugate for the likelihood function if the posterior is
of the same form/type as the prior.

The Beta prior is conjugate for the Binomial likelihood function:
\begin{align*}
    p(\mu | x = h, N, \alpha, \beta) &\propto p(x | N, \mu) p(\mu | \alpha, \beta) \\
    &\propto \mu^h (1 - \mu)^{N - h} \mu^{\alpha - 1} (1-\mu)^{\beta - 1} \\
    &= \mu^{h + \alpha - 1} (1 - \mu)^{N - h + \beta - 1} \\
    &\propto Beta(h + \alpha, N - h + \beta)
\end{align*}

The Beta prior is conjugate for the Bernoulli distribution:
\begin{align*}
    p(\theta | x, \alpha, \beta) &\propto p(x | \theta) p(\theta | \alpha, \beta) \\
    &\propto \theta^x (1 - \theta)^{1-x} \theta^{\alpha-1}(1-\theta)^{\beta - 1} \\
    &= \theta^{\alpha + x - 1} (1 - \theta)^{\beta + (1 - x) - 1} \\
    &\propto Beta(\alpha + x, \beta + (1 - x))
\end{align*}

TODO: Inverse Wishart distribution

TODO: Inverse Gamma distribution

TODO: Multinomial distribution

TODO: Dirichlet distribution

\begin{table}[]
\begin{tabular}{@{}lll@{}}
\toprule
Likelihood  & Conjugate prior          & Posterior                \\ \midrule
Bernoulli   & Beta                     & Beta                     \\
Bernoulli   & Beta                     & Beta                     \\
Gaussian (univariate)      & Gaussian, inverse Gamma   & Gaussian, inverse Gamma   \\
Gaussian (multivariate)    & Gaussian, inverse Wishart & Gaussian, inverse Wishart \\
Multinomial & Dirichlet                & Dirichlet                \\ \bottomrule
\end{tabular}
\caption {Conjugate priors}
\end{table}
For the Gaussian distribution, the priors are listed for mean and variance.

TODO: Sufficient statistics and correspondence to the exponential family.

There are three levels of abstraction when considering distributions:
A fixed distribution with fixed parameters,
a fixed distribution but there parameters are inferred (usually done)
and only a family of distributions.

An exponential family is a family of probability distributions
parameterised by $\vec{\theta} \in \R^D$ of the form
\begin{equation*}
    p(\vec{x} | \vec{\theta}) = h(\vec{x})
        \exp(\langle \vec{\theta}, \vec{\phi}(\vec{x}) \rangle - A(\vec{\theta}))
\end{equation*}
where $\vec{\phi}(\vec{x})$ is the vector of sufficient statistics.
Any inner product can be used, but usually it is the dot product.
$h(\vec{x})$ can be absorbed by adding $\log h(\vec{x})$ to the
sufficient statistics and constraining the corresponding parameter
to $\theta_0 = 1$.
The term $A(\vec{x})$ is just a normalisation.
Thus, the form can be rewritten as
\begin{equation*}
    p(\vec{x} | \vec{\theta}) \propto \exp(\vec{\theta}^T \vec{\phi}(\vec{x}))
\end{equation*}
and $\vec{\theta}$ is then called the natural parameters.

TODO: Example bernoulli as Exponential Family

Assume the random variable $X$ is a member of the exponential family:
\begin{equation*}
    p(\vec{x} | \vec{\theta]}) =
    h(\vec{x})
        \exp(\langle \vec{\theta}, \vec{\phi}(\vec{x}) \rangle - A(\vec{\theta}))
\end{equation*}
Every exponential family has a conjugate prior
\begin{equation*}
    p(\vec{\theta} | \vec{\gamma}) =
    h_c(\vec{\theta}) \exp\left(
        \left\langle 
            \begin{bmatrix}
                \vec{\gamma}_1 \\
                \gamma_2
            \end{bmatrix},
            \begin{bmatrix}
                \vec{\theta} \\
                -A(\vec{\theta})
            \end{bmatrix}
        \right\rangle
        - A_c(\vec{\gamma})
    \right)
\end{equation*}
where $\vec{\gamma} = 
\begin{bmatrix}
    \vec{\gamma}_1 \\
    \gamma_2
\end{bmatrix}$ has dimension $\dim(\vec{\theta}) + 1$
and the sufficient statistics of the conjugate prior are
$\begin{bmatrix}
    \vec{\theta} \\
    -A(\vec{\theta})
\end{bmatrix}$.

Exponential families have finite-dimensional sufficient statistics
and conjugate distributions are easy to write down and also come
from an exponential family.


\subsection{Variable Transformation}
Let $X$ be a discrete variable with pmf $P(X = x)$ and
$U(x)$ an invertable function.
For the transformed random variable $Y := U(X)$ the pmf is given by
$P(Y = y) = P(X = U^{-1}(y))$.

Let $X$ now be a continuous random variable with cdf $F_X(x)$,
$U$ a function and $Y := U(X)$ the transformed variable.
The distribution function technique is as follows:
\begin{enumerate}
    \item Find the cdf $F_Y(y) = P(Y \leq y)$.
    \item Differentiate the cdf $F_Y(y)$ to get the pdf $f_Y(y)$.
\end{enumerate}
Keep in mind that the domain of the random variable might have changed.
In general, the function $U(x)$ requires to have an inverse
$x = U^{-1}(y)$.

Let $X$ be a continuous random variable with a strictly monotonic cdf
$F_X(x)$. Then, the random variable $Y := F_X(x)$ has a uniform distribution.
This is called the probability integral transform.
It can be used to sample from a distribution by first sampling from
a uniform distribution and then transforming it with the inverse cdf.

The change-of-variable technique for an invertible function $U(x)$
and $Y := U(X)$ is
\begin{equation*}
    f(y) = f_x(U^{-1}(y)) \cdot \left| \frac{d}{dy}U^{-1}(y) \right|
\end{equation*}
and in the general case
\begin{equation*}
    f(\vec{y}) = f_{\vec{x}} (U^{-1}(\vec{y})) \times
    \left| \det\left( \frac{\partial}{\partial \vec{y}}
    U^{-1}(\vec{y}) \right) \right|
\end{equation*}
where $\frac{\partial}{\partial \vec{y}}U^{-1}(\vec{y})$
is the Jacobian of $U^{-1}$.
