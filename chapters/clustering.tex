\chapter{Clustering}
\todo{Fix this chapter}

\section{k-means Clustering}
Given a set of data points,
they should be grouped into clusters of similar points.
Points are typically represented either in high-dimensional
Euclidean space or with distances specified by a metric/kernel.

\emph{k-means} clustering is a model-based approach,
i.e. it maintains cluster models and infers membership.

Each cluster is represented by a single
\emph{cluster centroid} $\vec{\mu}_i \in \R^d$.
A point is assigned to its closest cluster centroid.
This induces a Voronoi partition of the input space.
Therefore, decision boundaries are linear and the problem
can be interpreted as linear unsupervised classification.

\subsection{k-means Problem}
Points are represented in Euclidean space:
$\vec{x}_i \in \R^d$.
Clusters are represented by their centres:
$\vec{\mu}_j \in \R^d$.
Each point is assigned to its closest centre.

The goal is to pick centres to minimise the average squared
distance. Thus, the empirical risk is given as
\begin{equation*}
\hat{R}(\vec{\mu}) =
\hat{R}(\vec{\mu}_1, \dotsc, \vec{\mu}_k) =
\sum_{i=1}^n{
	\min_{
		j \in \{1, \dotsc, k\}
	}{\norm{\vec{x}_i - \vec{\mu}_j}_2^2}
}
\end{equation*}
and the resulting optimisation problem is
\begin{equation*}
\hat{\vec{\mu}} = \arg\min_{\vec{\mu}}{\hat{R}(\vec{\mu})}
\end{equation*}

This is a non-convex objective.
Furthermore, the problem is NP-hard,
thus cannot be solved optimally in general.


\subsection{k-means Algorithm / Lloyd's Heuristic}
The k-means problem could be solved by SGD.
However, there exists a heuristic for solving it
without gradient-based methods.
\emph{Lloyd's heuristic} is an algorithm to solve the
k-means problem approximately.

\begin{enumerate}
	\item Initialise cluster centres
	\begin{equation*}
	\vec{\mu}^{(0)} = [\vec{\mu}^{(0)}_1, \dotsc, \vec{\mu}^{(0)}_k]
	\end{equation*}
	
	\item While not converged
	\begin{enumerate}
		\item Assign each point $\vec{x}_i$ to closest centre:
		\begin{equation*}
		z_i^{(t)} = \arg\min_{j \in \{1, \dotsc, k\}}{
			\norm{\vec{x}_i - \vec{\mu}_j^{(t - 1)}}_2^2
		}
		\end{equation*}
		\item Update centres as the mean of assigned data points:
		\begin{equation*}
		\vec{\mu}_j^{(t)} = \frac{1}{n_j}
		\sum_{i : z_i^{(t)} = j}{\vec{x}_i}
		\end{equation*}
		where $n_j = |\{i \mid z_i^{(t)} = j\}|$.
	\end{enumerate}
\end{enumerate}

This algorithm is guaranteed to monotonically decrease
the average squared distance in each iteration, i.e.
\begin{equation*}
\hat{R}(\vec{\mu}^{(t)}) \leq \hat{R}(\vec{\mu}^{(t-1)})
\end{equation*}
It has runtime complexity of $\mathcal{O}(n \cdot k \cdot d)$
per iteration.


\subsection{k-means++}
k-means generally converges to a local optimum and its
performance thus strongly depends on initialisation.

One way to solve this is to use \emph{multiple random restarts}
and taking the best solution.
However, this prefers large clusters with many data points.
Also, one could use a \emph{farthest point heuristic}.
However, that is prone to outliers.

\emph{k-means++} is an initialisation scheme which tries
to combat both problems.

\begin{enumerate}
	\item Start with random data point as centre:
	\begin{align*}
	i_1 &\sim Uniform(\{1, \dotsc, n\}) \\
	\vec{\mu}_1^{(0)} &= x_{i_1}
	\end{align*}
	
	\item For $j = 2 : k$
	\begin{enumerate}
		\item Pick a data point as a centre randomly
		with probability proportional to the closest
		selected centre, i.e. pick $i_j$ with probability
		\begin{equation*}
		\frac{1}{z} \cdot \left(
		\min_{l \in \{1, \dotsc, j - 1\}}{
			\norm{\vec{x}_{i_j} - \vec{\mu}_l^{(0)}}_2^2
		}
		\right)
		\end{equation*}
		\item Set the data point as centre:
		\begin{equation*}
		\vec{\mu}_j^{(0)} = \vec{x}_{i_j}
		\end{equation*}
	\end{enumerate}
\end{enumerate}

The expected cost is $\mathcal{O}(\log k)$ times that
of the optimal solution, i.e.
\begin{equation*}
\Exp[\hat{R}(\vec{\mu}^{(0)})]
\leq \mathcal{O}(\log k) \cdot \min_{\vec{\mu}}{\hat{R}(\vec{\mu})}
\end{equation*}


\subsection{Model Selection}
Model selecting (i.e. determining $k$)
is very difficult in general.
The problem is that it is not possible to use a validation
set to measure generalisation error because
the cost on the validation set typically tends to decrease
too with increasing $k$.
If no \emph{specific domain knowledge} is available,
there some other approaches.

A \emph{heuristic approach} is using an \emph{elbow plot}.
One can plot the cost as a function of $k$ and then
look for an "elbow",
i.e. a point where increasing $k$ only leads to
diminishing returns in terms of cost.
The elbow function is monotonically decreasing and convex.

A \emph{regularisation approach} is obtained by adding
a regularisation term to the cost function:
\begin{equation*}
\vec{\mu}^* = \min_{k, \vec{\mu}}{
	\hat{R}(\vec{\mu}) + \lambda k
}
\end{equation*}
This is equivalent to the elbow method for
$\epsilon = \lambda$,
i.e. using a $k$ so that the decrease in cost is
less than $\lambda$.


\subsection{Challenges}
The general challenges with k-means are
\begin{itemize}
	\item Generally converges to local optimum
	\item Number of required iterations can be exponential
	\item Cannot model clusters of arbitrary shape.
	On Euclidean space, clusters are assumed to be spheres.
	\item Determining $k$ is difficult
\end{itemize}

The first two challenges are usually not a problem in practice.
The third challenge can be fixed by using kernels.
However, selecting $k$ is a major unsolved problem to date.
