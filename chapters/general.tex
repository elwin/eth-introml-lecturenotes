\chapter{General}

\section{Terminology}
The \emph{loss} $\ell(f(\vec{x}_i), \vec{y}_i)$ is defined
on a single data point,
the \emph{cost} on the whole data set.
The cost might also include penalty terms.

The \emph{objective} refers to any function
optimised during training.

\emph{Regression} has the goal to predict real-valued
labels/vectors, in general a mapping
$f : \mathbb{R}^d \to \mathbb{R}$.

In \emph{classification}, the target variable
$Y$ is discrete, i.e. categorical.

In \emph{supervised learning}, data is available
as input-output pairs.

A \emph{decision rule}/\emph{hypothesis} is a rule
to assign a class to a sample.

The \emph{hypothesis / hypothesis class} is the
function / class of functions used to fit data.

A \emph{surrogate loss} is used whenever we want
to optimise a loss which is intractable.
The surrogate loss approximates the desired
loss during optimisation.
The target loss is used during evaluation.

\emph{Parametric} models have a constant number of parameters.
Nonparametric models grow in complexity
with the size of data.
Nonparametric models are potentially much more expressive
but also more computationally complex.


TODO: Difference between derivative and gradient,
especially column/row vector.


\input{chapters/general/gradientdescent.tex}
\input{chapters/general/modelselection.tex}
\input{chapters/general/standardisation.tex}
\input{chapters/general/featureselection.tex}
\input{chapters/general/kernels.tex}
\input{chapters/general/classificationmetricsimbalance.tex}
\input{chapters/general/multiclass.tex}
\input{chapters/general/dimred.tex}
\input{chapters/general/probmodelling.tex}
\input{chapters/general/bayesiandecisiontheory.tex}
\input{chapters/general/generativemodelling.tex}
