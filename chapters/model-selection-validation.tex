\chapter{Model Selection and Validation}
\todo{Fix this chapter}

\section{Model Selection}
Underfitting happens if the model is too
simple, overfitting if the model is too complex.

As the model complexity increases,
the training error usually decreases.
However, the prediction error usually
decreases up to a point, and then starts
increasing again.


\subsection{Risks}
We usually assume that the data set is generated
independently and identically distributed (i.i.d.)
from some unknown distribution $P$,
$(\vec{x}_i, y_i) \sim P(\vec{X}, Y)$.
The i.i.d. assumption can be invalid, e.g. if
\begin{itemize}
	\item Time series data
	\item Spatially correlated data
	\item Correlated noise
\end{itemize}
The goal is then to minimise the expected error
(true risk) under $P$.
The empirical risk is used to estimate the true risk.

In the following, $\ell(y, \hat{y})$ is a loss function
and $\hat{y} = f(\vec{x}; \theta)$ is a prediction.

The population risk/true risk/expected error under $P$ is
\begin{equation*}
R(\vec{w})
= \int{P(\vec{x}, y)\ell(y, \hat{y}) d\vec{x} dy}
= \Exp_{\vec{x}, y}[\ell(y, \hat{y})]
\end{equation*}

The true risk on a sample data set $D$ is
\begin{equation*}
\hat{R}_D(\vec{w}) = \frac{1}{|D|}
\sum_{(\vec{x}, y) \in D}{\ell(y, \hat{y})}
\end{equation*}

Under the i.i.d. assumption,
according to the law of large numbers,
$\hat{R}_D(\vec{w}) \to R(\vec{w})$
for any fixed $\vec{w}$ as $|D| \to \infty$.

Empirical risk minimisation on training data $D$ is
finding
$\hat{\vec{w}}_D = \arg\min_{\vec{w}}{\hat{R}_D(\vec{w})}$.
Ideally, we want to solve
$\vec{w}^* = \arg\min_{\vec{w}}{R(\vec{w})}$.
Generalisation refers to a model which extracts
all relevant information from the training data
but also performs well on unseen data.
TODO: What is the generalisation error? Is it $R(\hat{\vec{w}}_D)$?

For learning via empirical risk minimisation,
uniform convergence is needed:
\begin{equation*}
\sup_{\vec{w}}{|R(\vec{w}) - \hat{R}(\vec{w})|} \to 0
\text{ as $|D| \to \infty$}
\end{equation*}
This is not implied by the law of large numbers
and depends on the model class.
For example, if "bad" points converge slower than
"good" points, the generalisation error might increase
with increasing amounts of data.

In general, it holds that
\begin{equation*}
\Exp_D[\hat{R}_D(\hat{\vec{w}}_D)] \leq
\Exp_D[R(\hat{\vec{w}}_D)]
\end{equation*}
Thus, when evaluating performance on the training data,
an overly optimistic estimate is achieved.

The general idea is thus to use two independent data sets
$D_{train} \sim P$ and $D_{test} \sim P$.
Optimisation happens on the training set, i.e.
\begin{equation*}
\hat{\vec{w}} = \arg\min_{\vec{w}}{\hat{R}_{train}(\vec{w})}
\end{equation*}
and evaluation on the test set
\begin{equation*}
\hat{R}_{test}(\hat{\vec{w}}) = \frac{1}{|D_{test}|}
\sum_{(\vec{x}, y) \in D_{test}}{
	(y - \hat{\vec{w}}^T \vec{x})^2
}
\end{equation*}
Then,
\begin{equation*}
\Exp_{D_{train}, D_{test}}[\hat{R}_{test}(\hat{\vec{w}}_{D_{train}})] =
\Exp_{D_{train}}[R(\hat{\vec{w}}_{D_{train}})]
\end{equation*}

The test error itself is random.
The variance increases for more complex models.
Thus, using a single test set creates bias.


\subsection{Cross-Validation}
Using multiple test sets wastes data.
Thus, for each candidate model $m$
and $i = 1, \dotsc, k$:
\begin{enumerate}
	\item Split $D = D_{train}^{(i)} \uplus D_{val}^{(i)}$
	\item Train model:
	$\hat{\vec{w}}_{i,m} = \arg\min_{\vec{w}}{\hat{R}_{train}^{(i)}(\vec{w})}$
	\item Estimate error:
	$\hat{R}_{m}^{(i)} = \hat{R}_{val}^{(i)}(\hat{\vec{w}}_{i, m})$
\end{enumerate}
The model can then be selected as
\begin{equation*}
\hat{m} = \arg\min_m{\frac{1}{k}
	\sum_{i=1}^k{\hat{R}_m^{(i)}}
}
\end{equation*}

Monte Carlo cross-validation picks a training set of
given size uniformly at random and validates on the
remaining points.
The prediction error is then estimated over multiple
random trials.

$k$-fold cross-validation partitions the training set
into $k$ folds.
$k - 1$ are used for training, $1$ for validation.
The prediction error is then estimated as the mean
validation error while varying the validation folds.
$k = 1$ is called leave-one-out cross-validation (LOOCV).

For large enough $k$, the cross-validation error estimate
is very nearly unbiased.

If $k$ is picked too small, there is little data for
training and a risk to underfit the training set and
overfit the test set.
If $k$ is picked too large, performance is usually better
but the computational complexity increases.
In practice, $k = 5$ or $k = 10$ often works well.

However, the error on the validation set will usually
also be underestimated.
If a model is selected, it is usually retrained with
the full data set.


\section{Feature Selection}
There are various reasons to
not use all available features, e.g. because of
\begin{itemize}
	\item Interpretability
	\item Generalisation
	\item Storage / computation / cost
\end{itemize}

A naÃ¯ve approach would be to try out all subsets of
features.
However, by using greedy feature selection,
computational cost is massively reduced.

In the following,
let $V = \{1, \dotsc, d\}$ be the set of all features
and $\hat{L}(S)$ be the cross-validation error
using features in $S \subseteq V$ only.


\subsection{Greedy Forward Selection}
Start with $S = \emptyset$ and $E_0 = \infty$.

Then, for $i = 1, \dotsc, d$,
find the best element to add
\begin{equation*}
s_i = \arg\min_{j \in V \setminus S}{
	\hat{L}(S \cup \{j\})
}
\end{equation*}
and compute the new error
\begin{equation*}
E_i = \hat{L}(S \cup \{s_i\})
\end{equation*}
If the new error increases, $E_i > E_{i-1}$,
break, else update $S \gets S \cup \{s_i\}$.

The problem with greedy forward selection is if there is
some combination of features which improve the model together
but do not improve it (or even make it worse) if used alone,
i.e. dependent features.


\subsection{Greedy Backward Selection}
Start with $S = V$ and $E_{d + 1} = \infty$.

Then, for $i = d, \dotsc, 1$,
find the best element to remove
\begin{equation*}
s_i = \arg\min_{j \in S}{
	\hat{L}(S \setminus \{j\})
}
\end{equation*}
and compute the new error
\begin{equation*}
E_i = \hat{L}(S \setminus \{s_i\})
\end{equation*}
If the new error increases, $E_i > E_{i-1}$,
break, else update $S \gets S \setminus \{s_i\}$.


\subsection{Sparsity Trick in Linear Models}
Explicitly selecting $k$ features is equivalent to constraining
$\vec{w}$ to have at most $k$ non-zero entries in linear models,
i.e. to be sparse.

The $\norm{\vec{w}}_0$ is defined as the number of non-zero
entries in $\vec{w}$.
Then, we want to optimise the objective function so that
$\norm{\vec{w}}_0 \leq k$.
Alternatively, a penalty term $\lambda \norm{\vec{w}}_0$
can be added to the objective.
However, this is a hard combinatorial optimisation problem.

The $L_1$ loss can act as a surrogate for the $L_0$ loss.
This is called the sparsity trick.

\emph{Fisher consistency} is defined as follows:
Let $\psi : \mathcal{Y} \times \mathcal{S} \to \R$
be a surrogate loss and
$\ell : \mathcal{Y} \times \mathcal{S} \to \R$
be a loss.
$\psi$ is consistent with respect to $\ell$
if every minimiser $f$ of the surrogate risk
$R_\psi(f)$ is also a minimiser of the
risk function $R_\ell(f)$.

The $L_1$ norm is convex and thus,
combined with convex losses,
results in a convex optimisation problem.

While SGD can be used in principle,
convergence is usually slow and coefficients
are rarely exactly zero.

During cross-validation, a good approach is to start with
a large $\lambda$ (s.t. pretty much all weights are zero)
and then gradually decrease it.


\subsection{Comparison}
For greedy methods,
forward selection is usually faster if there are few relevant
features, but it can miss dependencies.
Backward selection can handle dependent features but may be
very slow.
Both methods are only heuristics and can result in non-optimal
selections.
Also, both methods can result in high computational cost.

$L_1$-regularisation is faster because training and feature
selection happen jointly,
but it only works for linear models.
