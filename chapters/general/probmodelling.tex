\section{Probabilistic Modelling}
The goal of supervised learning is:
Given training data $D = \{(\vec{x}_1, y_1), \dotsc, (\vec{x}_n, y_n)\}$,
we want to identify a hypothesis $h : \mathcal{X} \to \mathcal{Y}$
and minimise the prediction error (risk).
The prediction error / true risk is
defined according to some loss function
$\ell$ and given as
\begin{equation*}
    R(h) = \int{P(\vec{x}, y) \ell(y; h(\vec{x})) d\vec{x} dy}
        = \Exp_{\vec{X}, Y}[\ell(Y; h(\vec{X})]
\end{equation*}

The fundamental assumption is that
the data set is generated i.i.d.,
i.e.
\begin{equation*}
    (\vec{x}_i, y_i) \overset{i.i.d.}{\sim} P(\vec{X}, Y)
\end{equation*}

In least squares regression,
the risk is
$R(h) = \Exp_{\vec{X}, Y}[(Y - h(\vec{X}))^2]$.
Assuming the true distribution $P(\vec{X}, Y)$ is known,
the $h$ minimising the risk is given as
\begin{align*}
    \min_h{R(h)}
    &= \min_h{\Exp_{\vec{X}, Y}[(Y - h(\vec{X}))^2]} \\
    &= \min_h{\Exp_{\vec{X}}[\Exp_Y[(Y - h(\vec{x}))^2 \mid \vec{X} = \vec{x}]]} \\
    &= \Exp_{\vec{X}}[
        \min_{h(\vec{x})}{\Exp_Y[(Y-h(\vec{x})^2 \mid \vec{X} = \vec{x}]}
    ]
\end{align*}
since $h$ can be considered independently for each $\vec{x}$.

Now, for a fixed $\vec{x}$, the optimal prediction is
\begin{equation*}
    y^*(\vec{x}) \in \arg\min_{\hat{y}}{
        \underbrace{\Exp_Y[(\hat{y} - y)^2]}_{= \ell(\hat{y})}
    }
\end{equation*}

This leads to
\begin{align*}
    \frac{d}{d \hat{y}} \ell(\hat{y}) &= \int{2 (\hat{y} - y) p(y \mid \vec{x}) dy} \overset{!}{=} 0 \\
    &\Rightarrow \underbrace{\int{\hat{y} \cdot p(y \mid \vec{x}) dy}}_{=\hat{y}} = \underbrace{\int{y \cdot p(y \mid \vec{x}) dy}}_{= \Exp[Y \mid \vec{X} = \vec{x}]} \\
    &\Leftrightarrow \hat{y} = \Exp[Y | \vec{X} = \vec{x}]
\end{align*}

Therefore, in least squares regression,
the hypothesis minimising the true risk
is given by the conditional mean
\begin{equation*}
    h^*(\vec{x}) = \Exp[Y \mid \vec{X} = \vec{x}]
\end{equation*}

This hypothesis is called the
\emph{Bayes' optimal predictor}.

In practice, with finite data, one strategy
is to estimate the conditional distribution
\begin{equation*}
    \hat{P}(Y \mid \vec{X})
\end{equation*}
and for a test point $\vec{x}$ to
predict the label
\begin{equation*}
    \hat{y} = \hat{\Exp}[Y \mid \vec{X} = \vec{x}]
    = \int{y \cdot \hat{P}(y \mid \vec{X} = \vec{x}) dy}
\end{equation*}


\subsection{Conditional Maximum Likelihood Estimation}
A common approach for conditional distribution estimation
is to choose a particular \emph{parametric form}
$\hat{P}(Y \mid \vec{X}, \theta)$
and optimise the parameters using
\emph{maximum conditional likelihood estimation}:
\begin{equation*}
    \theta^* = \arg\max_\theta{
        \hat{P}(y_1, \dotsc, y_n \mid \vec{x}_1, \dotsc, \vec{x}_n, \theta)
    }
\end{equation*}
By factoring the density (as the data is i.i.d.),
by applying the logarithm and switching the sign,
this is equivalent to minimising the
\emph{negative log likelihood}
\begin{equation*}
    \theta^* = \arg\min_\theta{
        -\sum_{i=1}^n{\log{\hat{P}(y_i \mid \vec{x}_i, \theta)}}
    }
\end{equation*}

We denote the negative log likelihood by $L(\theta)$.

MLE has several nice statistical properties:
\begin{description}
    \item[Consistency]: Parameter estimate
    converges to true parameters in probability.
    \item[Asymptotic efficiency]: Smallest
    variance among all well-behaved estimators
    for large $n$.
    \item[Asymptotic normality]
\end{description}


\subsubsection{Example: Regression with Gaussian Noise}
Assume $Y = h(\vec{X}) + \epsilon$,
$\epsilon \sim \mathcal{N}(0, \sigma^2)$ 
(with known $\sigma^2$) and
$h(\vec{x}) = \vec{w}^T \vec{x}$.
I.e., the model is linear and noise is Gaussian
with known variance.

Then,
$\hat{p}(y \mid \vec{X} = \vec{x}) = \mathcal{N}(y; \vec{w}^T \vec{x}, \sigma^2)$
and
\begin{equation*}
    \hat{\vec{w}} = \arg\min_{\vec{w}}{L(\vec{w})}
    = \arg\min_{\vec{w}}{
        -\sum_{i=1}^n{\log{\hat{p}(y_i \mid \vec{x}_i, \vec{w}, \sigma^2)}}
    }
\end{equation*}

Furthermore,
\begin{align*}
    - \log{\hat{p}(y_i \mid \vec{x}_i, \vec{w}, \sigma^2)}
    &= -\log{\mathcal{N}(y; \vec{w}^T \vec{x}, \sigma^2)} \\
    &= -\log{\frac{1}{\sqrt{2 \pi \sigma^2}} \exp{
        \left( -\frac{(y - \vec{w}^T \vec{x})^2}{2 \sigma^2} \right)
    }} \\
    &= \frac{1}{2} \log{(2 \pi \sigma^2)} + \frac{1}{2 \sigma^2} (y-\vec{w}^T \vec{x})^2
\end{align*}
and therefore
\begin{align*}
    \hat{\vec{w}} &= \arg\min_{\vec{w}}{
        \sum_{i=1}^n{\frac{1}{2} \log{(2 \pi \sigma^2)} + \frac{1}{2 \sigma^2} (y_i-\vec{w}^T \vec{x}_i)^2}
    } \\
    &= \arg\min_{\vec{w}}{
        \underbrace{\frac{n}{2} \log{(2 \pi \sigma^2)}}_{= \text{const w.r.t. $\vec{w}$}}
        + \frac{1}{2 \sigma^2} \sum_{i=1}^n{(y_i - \vec{w}^T \vec{x}_i)^2}
    } \\
    &= \arg\min_{\vec{w}}{\sum_{i=1}^n{(y_i - \vec{w}^T \vec{x}_i)^2}}
\end{align*}

Thus, under the conditional linear Gaussian assumption,
maximising the likelihood is equivalent
to least squares regression.


\subsubsection{MLE for i.i.d. Gaussian noise}
The above example holds more general.

Suppose $\mathcal{H} = \{h : \mathcal{X} \to \R\}$
is a class of functions.
Further assume
\begin{equation*}
    P(Y = y \mid \vec{X} = \vec{x}) =
    \mathcal{N}(y \mid h^*(\vec{x}), \sigma^2)
\end{equation*}
for some $h^* \in \mathcal{H}$ and some $\sigma^2 > 0$.

The MLE for data $D = \{(\vec{x}_1, y_1), \dotsc, (\vec{x}_n, y_n)\}$
is given by
\begin{equation*}
    \hat{h} = \arg\min_{h \in \mathcal{H}}{
        \sum_{i=1}^n{(y_i - h(\vec{x}_i))^2}
    }
\end{equation*}

Thus, the MLE is given by the least squares
solution, assuming noise is iid Gaussian
with constant variance.


\subsection{Bias-Variance Tradeoff}
While MLEs have nice statistical properties,
those only hold for $n \to \infty$.
For finite $n$, overfitting must be avoided.

The bias variance tradeoff states
for the sum of squared errors that
\begin{equation*}
    \text{Prediction error} = \text{Bias}^2 + \text{Variance} + \text{Noise}
\end{equation*}
where
\begin{description}
    \item[Bias] is the excess risk of the best
    \emph{considered} model,
    compared to minimal achievable risk
    knowing $P(\vec{X}, Y)$
    (i.e. given infinite data).
    Going away from the Bayes' optimal
    prediction, i.e. restricting function
    class, increases bias.
    \item[Variance] is the risk incurred
    due to estimating using finite data
    \item[Noise] is the risk incurred by
    optimal model (i.e. irreducible error)
\end{description}

Usually, bias and variance have to be traded
via model selection and regularisation.
High bias corresponds to underfitting
while high variance corresponds to overfitting.

The MLE solution depends on the training data $D$,
i.e. $\hat{h} = \hat{h}_D$.

We want to chose $\mathcal{H}$ to have small bias,
i.e. have small squared error on average:
\begin{equation*}
    \Exp_{\vec{X}}[
        \underbrace{\Exp_D[\hat{h}_D(\vec{X})] - h^*(\vec{X})}_{\text{Bias}}
    ]^2
\end{equation*}

The estimator itself is random and
thus has some variance:
\begin{equation*}
    \Exp_{\vec{X}}[\Var_D[\hat{h}_D(\vec{X})]^2]
    = \Exp_{\vec{X}}[
        \Exp_D[(
            \hat{h}_D(\vec{X}) - \Exp_{D'}[\hat{h}_{D'}(\vec{X})]
        )^2]
    ]
\end{equation*}
TODO: No idea if the variance formula is correct...

Even if the Bayes' optimal hypothesis $h^*$ is
known, we would still incur some error du to noise:
\begin{equation*}
    \Exp_{\vec{X}, Y}[
        (Y - h^*(\vec{X}))^2
    ]
\end{equation*}

Ultimately, for least squares regression it holds that
\begin{align*}
    \underbrace{\Exp_D[\Exp_{\vec{X}, Y}[
        (Y - \hat{h}_D(\vec{X}))^2
    ]]}_{\text{Expected risk}}
\end{align*}
TODO: Week 18, slide 20

The MLE for linear regression is unbiased
if $h^* \in \mathcal{H}$.
Furthermore, it is the minimum variance
estimator among all unbiased estimators.
However, it may still overfit.
Thus, we can trade a small amount of bias
for a potentially large reduction in variance
by using regularisation.


\subsection{Maximum a Posteriori Estimate}
Bias can be introduced by expressing assumptions / a belief
on the parameters.
This is done using a \emph{Bayesian prior}.

Assume the parameters $\theta$ are from a known
\emph{prior distribution} which is fixed before any data
is observed.
Particularly, $\theta$ and the $\vec{X}_i$ are independent.

Then, the \emph{posterior distribution} of the parameter
$\theta$ is given using Bayes' rule
and $P(\theta \mid \vec{x}_{1:n}) = P(\theta)$ as
\begin{equation*}
    \underbrace{
        P(\theta \mid y_{1:n}, \vec{x}_{1:n})}_\text{posterior}
    = \frac{
        \overbrace{P(\theta)}^\text{prior}
        \overbrace{
        P(y_{1:n} \mid \theta, \vec{x}_{1:n})}^\text{likelihood}
    }{
        P(y_{1:n} \mid \vec{x}_{1:n})
    }
\end{equation*}

Finally, the \emph{maximum a posteriori (MAP) estimate}
is obtained by maximising $P(\theta \mid y_{1:n}, \vec{x}_{1:n})$
(or equivalently minimising the negative log term).
Note that $P(y_{1:n} \mid \vec{x}_{1:n})$ is constant w.r.t.
$\theta$ and can thus be ignored during optimisation.

MLE is a special case of MAP estimation.
Choosing a uniform parameter prior during
MAP estimation results in MLE.


\subsubsection{Example: Ridge Regression}
Assume $\theta = \vec{w} \sim \mathcal{N}(0, \beta^2 \vec{I})$,
i.e. the $w_i \sim \mathcal{N}(0, \beta^2)$ independently.

Then,
\begin{equation*}
    \arg\max_{\vec{w}}{
        P(\vec{w} \mid \vec{x}_{1:n}, y_{1:n})
    }
    = \arg\min_{\vec{w}}{
        -\log{P(\vec{w})}
        -\log{P(y_{1:n} \mid \vec{x}_{1:n}, \vec{w})}
        + \underbrace{\log{P(y_{1:n} \mid \vec{x}_{1:n})}}_\text{constant w.r.t. $\vec{w}$}
    }
\end{equation*}

For the negative prior log likelihood, we have
\begin{align*}
    -\log{P(\vec{w})} &=
        -\log{\prod_{i=1}^d{P(w_i)}} \\
    &= - \sum_{i=1}^d{\log{\mathcal{N}(w_i ; 0, \beta^2)}} \\
    &= - \sum_{i=1}^d{\log{
        \frac{1}{\sqrt{2 \pi \beta^2}}
        \exp{\left( -\frac{w_i^2}{2 \beta^2} \right)}
    }} \\
    &= \frac{d}{2} \log{2 \pi \beta^2}
    + \frac{1}{2 \beta^2} \sum_{i=1}^d{w_i^2} \\
    &= \text{const} + \frac{1}{2 \beta^2} \norm{\vec{w}}_2^2
\end{align*}

Combining the above form with the already known negative log
likelihood of a linear model with Gaussian noise, we get
\begin{align*}
    & \arg\min_{\vec{w}}{
        -\log{P(\vec{w})}
        -\log{P(y_{1:n} \mid \vec{x}_{1:n}, \vec{w})}
    } \\
    &= \arg\min_{\vec{w}}{
        \frac{1}{2\beta^2} \norm{\vec{w}}_2^2
        + \frac{1}{2\sigma^2} \sum_{i=1}^n{(y_i - \vec{w}^T \vec{x}_i)^2}
    } \\
    &= \arg\min_{\vec{w}}{
        \underbrace{\frac{\sigma^2}{\beta^2}}_\lambda \norm{\vec{w}}_2^2
        + \sum_{i=1}^n{(
        y_i - \vec{w}^T \vec{x}_i
        )^2}
    }
\end{align*}

Therefore, ridge regression is MAP estimation
for a linear regression problem, assuming that
the noise $P(y \mid \vec{x}, \vec{w})$ is
i.i.d. Gaussian and the prior $P(\vec{w})$
is Gaussian.

$\lambda$ is the ratio between the noise variance
and the prior variance.
Thus, a large $\lambda$ means that we believe to
have a lot of noise or the weights to be close to zero.


\subsubsection{Example: Lasso Regression}
Lasso regression / $L_1$-regularised linear regression
corresponds to a linear regression problem with Gaussian
noise and a Laplace parameter prior.

The density of a (univariate) Laplace distribution is given as
\begin{equation*}
    p(x; \mu, b) = \frac{1}{2b} \exp{\left(
        -\frac{|x - \mu|}{b}
    \right)}
\end{equation*}

The Laplace distribution has a spike around $\mu$.
With $\mu = 0$, this should bias the weights to be exactly zero.
However, the Laplace prior might not be optimal
for sparse solutions as we get non-zero weights
with probability 1 (it is continuous).

TODO: Slide 30.
The probability for a Gaussian distribution to have values far off from the mean is very low,
exponentially low.
The student-t distribution is heavier tailed, thus has only polynomial decay.
This can help substantially for robustness towards outliers.
Gaussian likelihood assumes that all points are very close to the prediction.
This is generally not true, especially if we have large outliers.
In those cases, we should use a different loss function,
i.e. replacing Gaussian with a more heavier-tailed distribution.

TODO: student's t-distribution slides 19, p. 9

\subsection{Regularisation via MAP inference}
Regularised estimation can often be understood as MAP inference.
The loss corresponds to the likelihood,
the regulariser to the parameter prior.
This leads to the form
\begin{equation*}
    \arg\min_{\vec{w}}{
        \sum_{i=1}^n{\ell(\vec{w}^T \vec{x}_i; \vec{x}_i, y_i)}
        + C(\vec{w})
    }
    = \arg\max_{\vec{w}}{
        \prod_{i=1}^n{P(y_i \mid \vec{x}_i, \vec{w})} P(\vec{w})
    }
    = \arg\max_{\vec{w}}{P(\vec{w} \mid D)}
\end{equation*}
where
\begin{align*}
    C(\vec{w}) &= - \log{P(\vec{w})} \\
    \ell(\vec{w}^T \vec{x}_i; \vec{x}_i, y_i)
        &= -\log{P(y_i \mid \vec{x}_i, \vec{w})}
\end{align*}

This allows to exchange priors (regularisers)
and likelihoods (loss functions).


\subsection{Classification}
In classification, the risk is
\begin{equation*}
    R(h) = \Exp_{\vec{X}, Y}[
        \underbrace{[Y \neq h(\vec{X})]}_\text{$1$ if $Y \neq H(\vec{X})$, $0$ otherwise}
    ]
\end{equation*}

Assuming $P(\vec{X}, Y)$ is known,
and $(\vec{x}_i, y_i) \sim P(\vec{X}, Y)$ iid,
we get
\begin{align*}
    h^*(\vec{x}) &= \arg\min_{\hat{y}}{
        \Exp_Y[[Y \neq \hat{y}] \mid \vec{X} = \vec{x}]
    } \\
    &= \arg\min_{\hat{y}}{
        \sum_{y=1}^c{P(Y = y \mid \vec{X} = \vec{x}) \cdot [y \neq \hat{y}] }
    } \\
    &= \arg\min_{\hat{y}}{
        \sum_{y : y \neq \hat{y}}{P(Y = y \mid \vec{X} = \vec{x})}    
    } \\
    &= \arg\min_{\hat{y}}{1 - P(Y = \hat{y} \mid \vec{X} = \vec{x})} \\
    &= \arg\max_{\hat{y}}{P(Y = \hat{y} \mid \vec{X} = \vec{x})}
\end{align*}

Thus, the \emph{Bayes' optimal predictor} (classifier)
is given by the most probable class:
\begin{equation*}
    h^*(\vec{x}) = \arg\max_y{P(Y = y \mid \vec{X} = \vec{x})}
\end{equation*}
A natural approach is therefore again to estimate $P(Y \mid \vec{X})$.


\subsection{MAP Learning Summary}
To summaries, learning through MAP inference
works as follows:
\begin{enumerate}
    \item Start with i.i.d. assumption
    on data points (can be relaxed).
    \item Choose likelihood function
    (results in loss).
    \item Choose prior distribution
    (results in regulariser).
    \item Optimise for MAP parameters.
    \item Choose hyperparameters
    via cross-validation.
    \item Make predictions via Bayesian
    decision theory.
\end{enumerate}
