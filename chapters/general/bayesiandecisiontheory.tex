\section{Bayesian Decision Theory}
Let $P(y \mid \vec{x})$ be a conditional
distribution over labels,
$\mathcal{A}$ a \emph{set of actions},
and $C : \mathcal{Y} \times \mathcal{A} \to \R$
be a \emph{cost function}.

\emph{Bayesian decision theory} (BDT)
recommends to
pick the action which minimises the
\emph{expected cost}:
\begin{equation*}
    a^* =
    \arg\min_{a \in \mathcal{A}}{
        \Exp_y[C(y, a) \mid \vec{x}]
    }
\end{equation*}

If the true distribution $P(y \mid \vec{x})$
was known, this decision implements the
\emph{Bayes optimal decision}.
However, in practice, the distribution
can only be estimated.
If the action set is discrete,
calculations are often simplified
by calculating the expected cost
for each action individually
and then comparing them.


\subsection{Example: Logistic Regression}
In logistic regression,
the estimated conditional distribution is
\begin{equation*}
    \hat{P}(y \mid \vec{x}) =
    Ber(y ; \sigma(\hat{\vec{w}}^T \vec{x}))
\end{equation*}
the action set is
$\mathcal{A} = \{+1, -1\}$ and
the cost function
$C(y, a) = [y \neq a]$.

Then, the action which minimises the
expected cost is the most likely class:
\begin{align*}
    a^* &=
    \arg\min_{a \in \mathcal{A}}{
        \Exp_y[C(y, a) \mid \vec{x}]
    } \\
    &= \arg\max_y{\hat{P}(y \mid \vec{x})} \\
    &= sign(\vec{w}^T \vec{x})
\end{align*}


\subsection{Asymmetric Costs}
Logistic regression is used again,
however, with asymmetric cost.
The estimated conditional distribution is
\begin{equation*}
    \hat{P}(y \mid \vec{x}) =
    Ber(y ; \sigma(\hat{\vec{w}}^T \vec{x}))
\end{equation*}
the action set is
$\mathcal{A} = \{+1, -1\}$ and
the cost function is
\begin{equation*}
    C(y, a) =
    \begin{cases}
        c_{FP} & \text{if $y = -1$ and $a = +1$} \\
        c_{FN} & \text{if $y = 1$ and $a = -1$} \\
        0 & \text{otherwise}
    \end{cases}
\end{equation*}

Let $p = P(Y = +1 \mid \vec{x})$ be the
probability of a positive classification
for the input $\vec{x}$.
We can now defined the costs of positive
predictions as
\begin{equation*}
    c_+ = \Exp_y[C(y, +1) \mid \vec{x}]
    = (1 - p) \cdot c_{FP} + p \cdot 0
\end{equation*}
and for negative predictions as
\begin{equation*}
    c_- = \Exp_y[C(y, -1) \mid \vec{x}]
    = (1 - p) \cdot 0 + p \cdot c_{FN}
\end{equation*}

The optimal prediction for the estimated
distribution 
(using $p = P(y = +1 \mid \vec{x})$)
is then given as
\begin{align*}
    \text{predict} \hat{y} = +1
    &\Leftrightarrow c_+ < c_- \\
    &\Leftrightarrow (1-p) \cdot c_{FP} < p \cdot c_{FN} \\
    &\Leftrightarrow p > \frac{c_{FP}}{c_{FP} + c_{FN}}
\end{align*}


\subsection{Uncertainty}
Logistic regression is used again,
however, with an additional doubt action.
The estimated conditional distribution is
\begin{equation*}
    \hat{P}(y \mid \vec{x}) =
    Ber(y ; \sigma(\hat{\vec{w}}^T \vec{x}))
\end{equation*}
The action set is now
$\mathcal{A} = \{+1, -1, D\}$
where $D$ stands for \emph{doubt}.
For some doubt penalty $c$, the
new cost function is
\begin{equation*}
    C(y, a) =
    \begin{cases}
        [a \neq y] & \text{if $a \in \{+1, -1\}$} \\
        c & \text{if $a = D$}
    \end{cases}
\end{equation*}

Then, the action minimising the expected
cost is given by
\begin{equation*}
    a^* = \begin{cases}
        y & \text{if $\hat{P}(y \mid \vec{x}) \geq 1 - c$} \\
        D & \text{otherwise}
    \end{cases}
\end{equation*}


\subsection{Example: Least-Squares Regression}
In least-squares regression,
the estimated conditional distribution is
\begin{equation*}
    \hat{P}(y \mid \vec{x}) =
    \mathcal{N}(y ; \hat{\vec{w}}^T \vec{x}, \sigma^2)
\end{equation*}
the action set is
$\mathcal{A} = \R$ and
the cost function
$C(y, a) = (y - a)^2$.

Then, the action which minimises the
expected cost is the conditional mean:
\begin{align*}
    a^* &=
    \arg\min_{a \in \mathcal{A}}{
        \Exp_y[C(y, a) \mid \vec{x}]
    } \\
    &= \Exp[y \mid \vec{x}] \\
    &= \int{y \cdot \hat{P}(y \mid \vec{x}) dy} \\
    &= \hat{\vec{w}}^T \vec{x}
\end{align*}

If we chose an asymmetric cost
\begin{equation*}
    C(y, a) =
    \underbrace{c_1 \max{(y - a, 0)}}_\text{underestimation}
    +
    \underbrace{c_2 \max{(a - y, 0)}}_\text{overestimation}
\end{equation*}
then the action which minimises the
expected cost is
\begin{equation*}
    a^* = \hat{\vec{w}}^T \vec{x}
    + \sigma \cdot \Phi^{-1}\left(
    \frac{c_1}{c_1 + c_2}
    \right)
\end{equation*}
This shifts the prediction according
to the costs.


\subsection{Active Learning}
The goal of active learning is to minimise
the number of required labels.
The general idea is to pick examples
which the estimated conditional distribution
is most uncertain about.
This requires a uncertainty score $u_j$.
A simple one is
$u_j = -|\hat{P}(y_j = +1 \mid \vec{x}_j) - 0.5|$.
The most uncertain example is then
$j^* = \arg\max_j{u_j}$.

Given a pool of unlabelled examples
$D_X = \{\vec{x}_1, \dotsc, \vec{x}_n\}$
and an initially empty set of labelled
samples $D$.
For $t = 1, 2, \dotsc$, first
estimate $\hat{P}(Y \mid \vec{x})$
given the current data set $D$.
Then, pick the unlabelled example which
the current estimate is most uncertain
about:
\begin{equation*}
    i_t \in \arg\min_i{
        |0.5 - \hat{P}(Y_i \mid \vec{x}_i)|
    }
\end{equation*}
Finally, query the label $y_{i_t}$
and update the data set
$D \gets D \cup \{(\vec{x}_{i_t}, y_{i_t})\}$.

Active learning violates the i.i.d.
assumption!
It can thus get stuck with bad models.
There are also more advance selection
criteria,
e.g. querying the point which reduces
uncertainty of other points as much
as possible.
