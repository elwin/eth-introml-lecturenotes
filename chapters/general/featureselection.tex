\section{Feature Selection}
There are various reasons to
not use all available features, e.g. because of
\begin{itemize}
    \item Interpretability
    \item Generalisation
    \item Storage / computation / cost
\end{itemize}

A naÃ¯ve approach would be to try out all subsets of
features.
However, by using greedy feature selection,
computational cost is massively reduced.

In the following,
let $V = \{1, \dotsc, d\}$ be the set of all features
and $\hat{L}(S)$ be the cross-validation error
using features in $S \subseteq V$ only.


\subsection{Greedy Forward Selection}
Start with $S = \emptyset$ and $E_0 = \infty$.

Then, for $i = 1, \dotsc, d$,
find the best element to add
\begin{equation*}
    s_i = \arg\min_{j \in V \setminus S}{
        \hat{L}(S \cup \{j\})
    }
\end{equation*}
and compute the new error
\begin{equation*}
    E_i = \hat{L}(S \cup \{s_i\})
\end{equation*}
If the new error increases, $E_i > E_{i-1}$,
break, else update $S \gets S \cup \{s_i\}$.

The problem with greedy forward selection is if there is
some combination of features which improve the model together
but do not improve it (or even make it worse) if used alone,
i.e. dependent features.


\subsection{Greedy Backward Selection}
Start with $S = V$ and $E_{d + 1} = \infty$.

Then, for $i = d, \dotsc, 1$,
find the best element to remove
\begin{equation*}
    s_i = \arg\min_{j \in S}{
        \hat{L}(S \setminus \{j\})
    }
\end{equation*}
and compute the new error
\begin{equation*}
    E_i = \hat{L}(S \setminus \{s_i\})
\end{equation*}
If the new error increases, $E_i > E_{i-1}$,
break, else update $S \gets S \setminus \{s_i\}$.


\subsection{Sparsity Trick in Linear Models}
Explicitly selecting $k$ features is equivalent to constraining
$\vec{w}$ to have at most $k$ non-zero entries in linear models,
i.e. to be sparse.

The $\norm{\vec{w}}_0$ is defined as the number of non-zero
entries in $\vec{w}$.
Then, we want to optimise the objective function so that
$\norm{\vec{w}}_0 \leq k$.
Alternatively, a penalty term $\lambda \norm{\vec{w}}_0$
can be added to the objective.
However, this is a hard combinatorial optimisation problem.

The $L_1$ loss can act as a surrogate for the $L_0$ loss.
This is called the sparsity trick.

\emph{Fisher consistency} is defined as follows:
Let $\psi : \mathcal{Y} \times \mathcal{S} \to \mathbb{R}$
be a surrogate loss and
$\ell : \mathcal{Y} \times \mathcal{S} \to \mathbb{R}$
be a loss.
$\psi$ is consistent with respect to $\ell$
if every minimiser $f$ of the surrogate risk
$R_\psi(f)$ is also a minimiser of the
risk function $R_\ell(f)$.

The $L_1$ norm is convex and thus,
combined with convex losses,
results in a convex optimisation problem.

While SGD can be used in principle,
convergence is usually slow and coefficients
are rarely exactly zero.

During cross-validation, a good approach is to start with
a large $\lambda$ (s.t. pretty much all weights are zero)
and then gradually decrease it.


\subsection{Comparison}
For greedy methods,
forward selection is usually faster if there are few relevant
features, but it can miss dependencies.
Backward selection can handle dependent features but may be
very slow.
Both methods are only heuristics and can result in non-optimal
selections.
Also, both methods can result in high computational cost.

$L_1$-regularisation is faster because training and feature
selection happen jointly,
but it only works for linear models.
